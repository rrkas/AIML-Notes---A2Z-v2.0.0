\chapter{N-gram Language Modelling}

SEE: \fullref{Language Models}


\section{N-gram/ shingles \cite{wiki-n-gram}}

\begin{enumerate}
    \item An n-gram is a sequence of n adjacent symbols in particular order. The symbols may be n adjacent letters (including punctuation marks and blanks), syllables, or rarely whole words found in a language dataset; or adjacent phonemes extracted from a speech-recording dataset, or adjacent base pairs extracted from a genome. They are collected from a text corpus or speech corpus.
    
    \item If Latin numerical prefixes are used, then n-gram of size 1 is called a "unigram", size 2 a "bigram" (or, less commonly, a "digram") etc.

    \item If, instead of the Latin ones, the English cardinal numbers are furtherly used, then they are called "four-gram", "five-gram", etc.

    \item Similarly, using Greek numerical prefixes such as "monomer", "dimer", "trimer", "tetramer", "pentamer", etc., or English cardinal numbers, "one-mer", "two-mer", "three-mer", etc. are used in computational biology, for polymers or oligomers of a known size, called k-mers. When the items are words, n-grams may also be called shingles.

    \item[] $C(\text{\textbf{text}}) : \text{Count of \textbf{text} in corpus}$

    \item[] $
        P(X_1...X_n) = P(X_1)P(X_2|X_1)P(X_3|X_{1:2})...P(X_n|X_{1:n-1}) = \dprod_{k=1}^{n} P(X_k|X_{1:k-1}) 
    $\\
    Or,\\
    $
        P(w_{1:n}) = P(w_1)P(w_2|w_1)P(w_3|w_{1:2})...P(w_n|w_{1:n-1}) = \dprod_{k=1}^{n} P(w_k|w_{1:k-1}) 
    $

\end{enumerate}


\subsection{Bigram \cite{nlp-1}}
bigram: \(w_{n-1}w_n\)
\[
    \text{count of the bigram} = C(w_{n-1}w_n)
\]
\[
    P(w_n|w_{n-1}) = \dfrac{C(w_{n-1}w_n)}{\sum_w C(w_{n-1}w)} = \dfrac{C(w_{n-1}w_n)}{C(w_{n-1})} 
\]

\subsection{General N-gram \cite{nlp-1}}

\[
    P(w_n|w_{n-N+1:n-1}) = \dfrac{C(w_{n-N+1:n-1} w_n)}{C(w_{n-N+1:n-1})}
\]

where,
\[
    n \text{ is the position of the current word in the text.}
\]
\[
    N \text{ is the order of the N-gram model.}
\]


\subsection{Markov Models and $n$-grams \cite{dnn-1,wiki-n-gram}}

\begin{enumerate}[itemsep=0.2cm]
    \item N-gram approximation (using \textbf{Markov assumption}):
    $
        P(w_n|w_{1:n-1}) \approx P(w_n|w_{n-N+1:n-1})
    $

    \item A distribution over sequences satisfies the Markov property of first order if $P(x_{t+1} \mid x_t, \ldots, x_1) = P(x_{t+1} \mid x_t)$ \cite{dnn-1}

    \item $
        \begin{aligned}
        P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4),\\
        P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2) P(x_4  \mid  x_3),\\
        P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1, x_2) P(x_4  \mid  x_2, x_3).
        \end{aligned}
    $ \hfill \cite{dnn-1}

\end{enumerate}


\subsection*{Note}

\begin{enumerate}
    \item Unfortunately, models like this get unwieldy rather quickly for the following reasons
    \begin{enumerate}
        \item many $n$-grams occur very rarely, making Laplace smoothing rather unsuitable for language modeling

        \item we need to store all counts

        \item this entirely ignores the meaning of the words. For instance, “cat” and “feline” should occur in related contexts.\\
        It is quite difficult to adjust such models to additional contexts.

        \item long word sequences are almost certain to be novel, hence a model that simply counts the frequency of previously seen word sequences is bound to perform poorly there.
    \end{enumerate}
\end{enumerate}




\section{Perplexity (PP/ PPL) \cite{nlp-1,dnn-1}}

\begin{enumerate}
    \item The perplexity (sometimes abbreviated as PP or PPL) of a language model on a test set is the inverse probability of the test set (one over the probability of the test set), normalized by the number of words. For this reason it’s sometimes called the \textbf{per-word perplexity}\indexlabel{per-word perplexity}.

    \item We might measure the quality of the model by computing the likelihood of the sequence. Unfortunately this is a number that is hard to understand and difficult to compare. After all, shorter sequences are much more likely to occur than the longer ones. \cite{dnn-1}

    \item Let word: \(W = w_1w_2 ...w_N\)
    \begin{enumerate}
        \item $perplexity(W) = P(w_1w_2 ...w_N)^{-\dfrac{1}{N}} = \sqrt[N]{\dfrac{1}{P(w_1w_2 ...w_N)}} = \sqrt[N]{\prod_{i=1}^{N} \dfrac{1}{P(w_i|w_1w_2 ...w_{i-1})}}$

        \item For Unigram:
        \(
           \displaystyle perplexity(W) = \sqrt[N]{\prod_{i=1}^{N} \dfrac{1}{P(w_i)}}
        \)
        
        \item For Bigram:
        \(
           \displaystyle perplexity(W) = \sqrt[N]{\prod_{i=1}^{N} \dfrac{1}{P(w_i|w_{i-1})}}
        \)
    \end{enumerate}

    \item For historical reasons, scientists in natural language processing prefer to use a quantity called perplexity. \cite{dnn-1}
    \[
        \exp\left(-\dfrac{1}{n} \dsum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)
    \]

    \item Perplexity can be best understood as the reciprocal of the geometric mean of the number of real choices that we have when deciding which token to pick next. Let’s look at a number of cases:
    \begin{enumerate}
        \item In the \textbf{best case scenario}, the model always perfectly estimates the probability of the target token as 1. In this case the perplexity of the model is $1$.

        \item In the \textbf{worst case scenario}, the model always predicts the probability of the target token as 0. In this situation, the perplexity is \textbf{positive infinity}.

        \item At the \textbf{baseline}, the model predicts a uniform distribution over all the available tokens of the vocabulary. In this case, the perplexity equals the \textbf{number of unique tokens of the vocabulary}.\\
        In fact, if we were to store the sequence without any compression, this would be the best we could do for encoding it. Hence, this provides a nontrivial upper bound that any useful model must beat.
    \end{enumerate}

\end{enumerate}




\subsection{Perplexity’s Relation to Entropy}

SEE:
\begin{enumerate}
    \item \fullref{Information Theory: Entropy}
    \item \fullref{Log-Likelihood Loss/ cross-entropy loss}
\end{enumerate}

The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity.



\subsubsection{Entropy for sequence of words}\label{Entropy for sequence of words}
sequence of words: $W = {w_1,w_2,...,w_n}$\\
language: $L$

\[
    H(w_1,w_2,...,w_n) = H(w_{1:n}) = -\dsum_{w_{1:n}\in L} p(w_{1:n})\log(p(w_{1:n}))
\]

\subsubsection{Entropy Rate/ Per-word Entropy}
We could define the entropy rate (we could also think of this as the per-word entropy) as the entropy of this sequence divided by the number of words:

\[
    \text{Entropy Rate} = \dfrac{1}{n}H(w_{1:n}) = -\dfrac{1}{n}\sum_{w_{1:n}\in L} p(w_{1:n})\log(p(w_{1:n}))
\]

But to measure the true entropy of a language, we need to consider sequences of infinite length. If we think of a language as a stochastic process $L$ that produces a sequence of words, and allow $W$ to represent the sequence of words $w_1,...,w_n$, then $L$’s entropy rate $H(L)$ is defined as:

\[
    H(L) = \lim_{n \rightarrow \infty} \dfrac{1}{n}H(w_{1:n}) = -\lim_{n \rightarrow \infty} \dfrac{1}{n}\sum_{W \in L} p(w_{1:n})\log(p(w_{1:n}))
\]

The \textbf{Shannon-McMillan-Breiman theorem}\indexlabel{Shannon-McMillan-Breiman theorem} states that if the language is regular in certain ways (to be exact, if it is both \textbf{stationary} and \textbf{ergodic}). That is, we can take a single sequence that is long enough instead of summing over all possible sequences. The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.\\By making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability

\begin{enumerate}
    \item A \textbf{stochastic process} is said to be \textbf{stationary} if the probabilities it assigns to a sequence are \textbf{invariant} with respect to shifts in the time index.\\ In other words, the probability distribution for words at time $t$ is the same as the probability distribution at time $t + 1$. Markov models, and hence n-grams, are stationary.\\ For example, in a bigram, $P_i$ is dependent only on $P_{i-1}$. So if we shift our time index by $x$, $P_{i+x}$ is still dependent on $P_{i+x-1}$.
    \item \textbf{ergodic}\indexlabel{ergodic}: of or relating to a process in which every sequence or sizable sample is equally representative of the whole (as in regard to a statistical parameter)\\
    Source: \href{https://www.merriam-webster.com/dictionary/ergodic}{https://www.merriam-webster.com/dictionary/ergodic}
\end{enumerate}

\[
    H(L) = \lim_{n \rightarrow \infty} -\dfrac{1}{n} \log(p(w_{1:n}))
\]


\subsubsection*{Putting pieces together}
\[
\begin{aligned}
    Perplexity(W)       
        &= 2^{H(W)}    \\ 
        &= P(w_1w_2...w_N)^{-\dfrac{1}{N}} \\ 
        &= \sqrt[N]{\dfrac{1}{P(w_1w_2...w_N)}} \\ 
        &= \sqrt[N]{\sum_{i=1}^{N} \dfrac{1}{P(w_i|w_1w_2...w_{i-1})}} 
\end{aligned}
\]





\section{Sampling sentences from a language model \cite{nlp-1}}

\textbf{Sampling} from a distribution means to choose random points according to their likelihood. Thus sampling from a language model—which represents a distribution over sentences—means to generate some sentences, choosing each sentence according to its likelihood as defined by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability.

\begin{algorithm}
    \caption{Sampling from a Language Model}
    Initialize Tokens with the tokenized input prompt\;
    Set MaxLength to the desired maximum sequence length\;
    Set k to the desired number of top tokens to consider\;
    
    \While{Length(Tokens) < MaxLength}{
        \Comment{Get the probability distribution for the next token}
        NextTokenProbs $\leftarrow$ Model(Tokens)\; 
        
        \Comment{Select the top-k probabilities and their indices}
        TopKProbs, TopKIndices $\leftarrow$ TopK(NextTokenProbs, k)\; 
        
        \Comment{Normalize the top-k probabilities to form a probability distribution}
        TopKProbs $\leftarrow$ Normalize(TopKProbs)\; 
        
        \Comment{Sample an index from the top-k indices based on the normalized probabilities}
        SampledIndex $\leftarrow$ Sample(TopKIndices, TopKProbs)\; 
        
        \Comment{Append the sampled token index to the sequence}
        Tokens.append(SampledIndex)\; 
    }
    
    Detokenize Tokens to form the final generated sentence\;
\end{algorithm}


\section{Generalization of n-grams \cite{nlp-1}}
\begin{enumerate}
    \item $V$ = vocabulary size, or the number of unique words in the corpus
    \item $N$ = total count of word tokens ( $N \geq V$ )
\end{enumerate}

Total possible n-grams = $V^n$

\section{Zero probability n-grams (zeros) \cite{nlp-1}}

As any corpus is limited, some perfectly acceptable English word sequences are bound to be missing from it. That is, we’ll have many cases of putative “zero probability n-grams” that should really have some non-zero probability.

\begin{enumerate}
    \item Their presence means we are underestimating the probability of all sorts of words that might occur, which will hurt the performance of any application we want to run on this data.
    \item If the probability of any word in the test set is 0, the entire probability of the test set is 0. By definition, perplexity is based on the inverse probability of the test set. Thus if some words have zero probability, we can’t compute perplexity at all, since we can’t divide by 0!
\end{enumerate}


\section{Closed vocabulary \cite{nlp-1}}

Stipulating that we already know all the words that can occur. In such a \textbf{closed vocabulary} system the test set can only contain words from closed vocabulary this known lexicon, and there will be \textbf{no unknown words}.


\section{Unknown Words/ Out Of Vocabulary (OOV) \cite{nlp-1}}

If our language model is using words instead of tokens, however, we have to deal with unknown words, or out of vocabulary (OOV) words: words we haven’t seen before. The percentage of OOV words that appear in the test set is called the OOV rate.


\section{Smoothing or Discounting \cite{nlp-1}}

To keep a language model from assigning zero probability to these unseen events, we’ll have to shave off a bit of probability mass from some more frequent events and give it to the events we’ve never seen. This modification is called \textbf{smoothing} or \textbf{discounting}.

\begin{customTableWrapper}{1.3}
\begin{table}[H]
    \begin{tabular}{l l}
        $w_i$ & word token \\
        $c_i$ & original count of word token $w_i$ \\
        $N$ & total number of word tokens \\
        $P(w_i)$ & original (unigram) probability of $w_i$ \\
        $c_i^*$ & adjusted/ discounted count of $w_i$ \\
        $P_i^*$ & adjusted/ discounted probability of $w_i$ \\
    \end{tabular}
\end{table}
\end{customTableWrapper}


\subsection{Dicounting \cite{nlp-1}}
A related way to view smoothing is as discounting (lowering) some non-zero counts in order to get the probability mass that will be assigned to the zero counts.


\[
    d_c = \dfrac{c^*}{c}  \hfill \text{(relative discount)}
\]

\subsection{Absolute Discounting}
Absolute discounting means subtracting a fixed (absolute) discount d from each count. The intuition is that since we have good estimates already for the very high counts, a small discount $d$ won’t affect them much. It will mainly modify the smaller counts, for which we don’t necessarily trust the estimate anyway. 
\[
    P_{AbsoluteDiscounting}(w_i|w_{i-1}) = \dfrac{C(w_{i-1}w_i)-d}{\sum_v C(w_{i-1} v)} + \lambda (w_{i-1})P(w_i) \hfill \text{(for bigrams)}
\]

\begin{enumerate}
    \item \textbf{First Term}: discounted bigram with $0 \leq d \leq 1$\\
    There are principled methods for setting $d$. Example:\\
    Set $d$ as a function of $n_1$ and $n_2$, the number of unigrams that have a count of 1 and a count of 2:\\
    \[
        d = \dfrac{n_1}{n_1 + n_2}
    \]
    \item \textbf{Second Term}: unigram with an interpolation weight $\lambda$
\end{enumerate}

\subsection{Kneser-Ney Discounting}\label{Kneser-Ney Discounting}
The Kneser-Ney intuition is to base our estimate of $P_{CONTINUATION}$ on the number of different contexts word w has appeared in, that is, the number of bigram types it completes. Every bigram type was a novel continuation the first time it was seen. We hypothesize that words that have appeared in more contexts in the
past are more likely to appear in some new context as well. The number of times a word w appears as a novel continuation can be expressed as:

\[
    P_{CONTINUATION}(w) \propto \dabs{\dCurlyBrac{v : C(vw) > 0}}
\]
\[
    P_{CONTINUATION}(w) = \dfrac{\dabs{\dCurlyBrac{v : C(vw) > 0}}}{\dabs{\dCurlyBrac{(u',w') : C(u'w') > 0}} } = \dfrac{\dabs{\dCurlyBrac{v : C(vw) > 0}}}{\dsum_{w'} \dabs{\dCurlyBrac{v : C(vw') > 0}}}
\]

\subsection{Laplace (add-one) Smoothing \cite{nlp-1,dnn-1}}
The simplest way to do smoothing is to add one to all the n-gram counts, before we normalize them into probabilities.





\noindent
For \textbf{unigrams},
\begin{enumerate}[itemsep=0.15cm]
    \item $P(w_i) = \dfrac{c_i}{N}$
    
    \item $P_{Laplace}(w_i) = \dfrac{c_i + 1}{N + V} \hfill \text{(Laplace)}$
    
    \item $c_i^* = (c_i +1)\dfrac{N}{N + V}$
        
    \item $P^*(w_i) = \dfrac{c_i^*}{N}$

    \item $\hat{P}(x) = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}$ \hfill \cite{dnn-1}
\end{enumerate}

\vspace{0.5cm}
\noindent
For \textbf{bigrams},

\begin{enumerate}[itemsep=0.15cm]
    \item $P_{Laplace}(w_n|w_{n-1}) = \dfrac{C(w_{n-1}w_n) + 1}{\sum_w (C(w_{n-1}w) + 1)} = \dfrac{C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V} $

    \item $c^*(w_{n-1}w_n) = \dfrac{[C(w_{n-1}w_n)+1] \times C(w_{n-1})}{C(w_{n-1}) + V} $

    \item $\hat{P}(x' \mid x)  = \frac{n(x, x') + \epsilon_2 \hat{P}(x')}{n(x) + \epsilon_2}$ \hfill \cite{dnn-1}
\end{enumerate}

\vspace{0.5cm}
\noindent
For \textbf{trigrams},
\begin{enumerate}
    \item $\hat{P}(x'' \mid x,x') = \frac{n(x, x',x'') + \epsilon_3 \hat{P}(x'')}{n(x, x') + \epsilon_3}$ \hfill \cite{dnn-1}

\end{enumerate}

\vspace{0.5cm}
\noindent
here, $\epsilon_1,\epsilon_2, \epsilon_3$ are hyperparameters.\\
when $\epsilon_i = 0$, no smoothing is applied; when $\epsilon_i$ approaches positive infinity, $\hat{P}$ approaches the uniform probability $1/m$





\subsection{Add-k smoothing \cite{nlp-1}}

For bigrams,
\[
    P_{Add-k}(w_n|w_{n-1}) = \dfrac{C(w_{n-1}w_n) + k}{\sum_w (C(w_{n-1}w) + k)} = \dfrac{C(w_{n-1}w_n) + k}{C(w_{n-1}) + kV} 
\]

\subsection{(Interpolated) Kneser-Ney smoothing}

For bigram:
\[
\begin{aligned}
    P_{KN}(w_i|w_{i-1})  &= \dfrac{max(C(w_{i-1}w_i)-d,0)}{C(w_{i-1})} \\ 
    &+ \lambda (w_{i-1})P_{CONTINUATION}(w_i) \\ 
\end{aligned}
\]
where, $\lambda$ is a normalizing constant that is used to distribute the probability mass we’ve discounted:
\[
    \lambda(w_{i-1}) 
    = \dfrac{d}{\dsum_v C(w_{i-1}v)} \dabs{\dCurlyBrac{w : C(w_{i-1}w) > 0}}
\]

General recursive formula:
\[
\begin{aligned}
    P_{KN}(w_i|w_{i-n+1:i-1})  &= \dfrac{max(c_{KN}(w_{i-n+1:i})-d,0)}{\sum_v c_{KN}(w_{i-n+1:i-1} v)} \\ 
    &+ \lambda (w_{i-n+1:i-1})P_{KN}(w_i|w_{i-n+2:i-1}) 
\end{aligned}
\]

At termination:
\[
    P_{KN}(w) = \dfrac{max(c_{KN}(w)-d,0)}{\sum_{w'} c_{KN}(w')} + \lambda(\epsilon)\dfrac{1}{V}
\]

\textbf{Note}:

\begin{enumerate}[itemsep=0.1cm]
    \item $\epsilon$ = empty string
    \item If we want to include an unknown word <UNK>, it’s just included as a regular vocabulary entry with count zero, and hence its probability will be a lambda-weighted uniform distribution \(\dfrac{\lambda(\epsilon)}{V}\)
    \item \(c_{KN}(\cdot) = \begin{cases}
        \rcmdXcount(\cdot) & \text{for the highest order} \\
        \rcmdXcontinuationcount(\cdot) & \text{for lower orders}
    \end{cases}\)
    \item $\rcmdXcontinuationcount(x)$ of a string $x$ is the number of unique single word contexts for that string $x$.
    \item discount: $0 \leq d \leq 1$ 
    \item \(\dfrac{d}{\sum_v C(w_{i-1}v)}\) is normalized discount
    \item \(\dabs{\dCurlyBrac{w : C(w_{i-1}w) > 0}}\) is number of word types that can follow $w_{i-1}$ or equivalently, the number of word types that we discounted; in other words, the number of times we applied the normalized discount.
\end{enumerate}

\vspace{0.2cm}

SEE: \fullref{Kneser-Ney Discounting}

\subsection{Modified Kneser-Ney smoothing}
Rather than use a single modified
Kneser-Ney fixed discount d, modified Kneser-Ney uses three different discounts d1, d2, and d3+ for n-grams with counts of 1, 2 and three or more, respectively.

\section{Backoff \& Interpolation for \textit{zeros} \cite{nlp-1}}
If we are trying to compute $P(w_n|w_{n-2}w_{n-1})$ but we have no examples of a particular trigram $w_{n-2}w_{n-1}w_n$, we can instead estimate its probability by using the bigram probability $P(w_n|w_{n-1})$. Similarly, if we don’t have counts to compute $P(w_n|w_{n-1})$, we can look to the unigram $P(w_n)$.

In other words, sometimes using less context is a good thing, helping to generalize more for contexts that the model hasn’t learned much about. There are two ways to use this n-gram “hierarchy”.

\subsection{Backoff}
In backoff, we use the trigram if the evidence is sufficient, otherwise we use the bigram, otherwise the unigram. In other words, we only “back off” to a lower-order n-gram if we have zero evidence for a higher-order n-gram. 

In a backoff n-gram model, if the n-gram we need has zero counts, we approximate it by backing off to the (n-1)-gram. We continue backing off until we reach a history that has some counts.

\subsubsection{Katz backoff}

In order for a backoff model to give a correct probability distribution, we have discount to discount the higher-order n-grams to save some probability mass for the lower order n-grams. Just as with add-one smoothing, if the higher-order n-grams aren’t discounted and we just used the undiscounted MLE probability, then as soon as we replaced an n-gram which has zero probability with a lower-order n-gram, we would be adding probability mass, and the total probability assigned to all possible strings by the language model would be \textbf{greater than 1}! In addition to this explicit discount factor, we’ll need a function $\alpha$ to distribute this probability mass to the lower order n-grams.

In Katz backoff we rely on a discounted probability $P^*$
if we’ve seen this n-gram before (i.e., if we have non-zero counts). Otherwise, we recursively back off to the Katz probability for the shorter-history (n-1)-gram. The probability for a backoff n-gram $P_{BO}$ is thus computed as follows:

\[
    P_{BO}(w_n|w_{n-N+1:n-1}) =
\begin{cases} 
    P^*(w_n|w_{n-N+1:n-1}), & \text{if } C(w_{n-N+1:n}) > 0 \\
    \alpha(w_{n-N+1:n-1}) P_{BO}(w_n|w_{n-N+2:n-1}), & \text{otherwise}.
\end{cases}
\]

\subsubsection{Good-Turing backoff}
Katz backoff is often combined with a smoothing method called Good-Turing. The combined Good-Turing backoff algorithm involves quite detailed computation for estimating the Good-Turing smoothing and the $P^*$ and $\alpha$ values.

\subsubsection{Stupid backoff}
Stupid backoff gives up the idea of trying to make the language model a true probability distribution. There is no discounting of the higher-order probabilities. If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed (context-independent) weight. This algorithm does not produce a probability distribution.

\[
    S(w_i|w_{i-N+1:i-1}) = \begin{cases}
        \dfrac{count(w_{i-N+1:i})}{count(w_{i-N+1:i-1})} & \text{ if $count(w_{i-N+1:i}) > 0$}\\[0.2cm]
        \lambda S(w_i|w_{i-N+2:i-1}) & \text{ otherwise}
    \end{cases}
\]
where the termination is:
\[
    S(w) = \dfrac{C(w)}{N} \hfill \text{(for unigram)}
\]

\subsection{Interpolation}
In interpolation, we always mix the probability estimates from all the n-gram estimators, weighting and combining the trigram, bigram, and unigram counts.

\subsubsection{Simple Interpolation}
In simple linear interpolation, we combine different order n-grams by linearly interpolating them. Thus, we estimate the trigram probability $P(w_n|w_{n-2}w_{n-1})$ by mixing together the unigram, bigram, and trigram probabilities, each weighted by a $\lambda$:

\[
\begin{aligned}
    \hat{P}(w_n|w_{n-2}w_{n-1}) &= \lambda_1 \cdot P(w_n)  \\ 
    &+ \lambda_2 \cdot P(w_n|w_{n-1}) \\ 
    &+ \lambda_3 \cdot P(w_n|w_{n-2}w_{n-1}) 
\end{aligned}
\]


if \( \left( \sum_\lambda = 1 \right)\), then its equivalent to a weighted average.

\subsubsection{Conditional Interpolation}
With context-conditioned weights:
\[
\begin{aligned}
    \hat{P}(w_n|w_{n-2}w_{n-1}) &= \lambda_1(w_{n-2:n-1})P(w_n) \\ 
    &+ \lambda_2(w_{n-2:n-1})P(w_n|w_{n-1}) \\ 
    &+ \lambda_3(w_{n-2:n-1})P(w_n|w_{n-2}w_{n-1}) \\ 
\end{aligned}
\]

if \( \left( \sum_\lambda = 1 \right)\), then its equivalent to a weighted average.

Both the simple interpolation and conditional interpolation $\lambda$s are learned from a held-out corpus. A held-out corpus is an additional training corpus, so-called because we hold it out from the training data, that we use to set hyperparameters like these $\lambda$ values. We do so by choosing the $\lambda$ values that maximize the likelihood of the held-out corpus.

SEE: Expectation–Maximization (EM) algorithm (TODO)


\section{Huge Language Model}
Efficiency considerations are important when building language models that use such large sets of n-grams. 
\begin{enumerate}
    \item Rather than store each word as a string, it is generally represented in memory as a 64-bit hash number, with the words themselves stored on disk. Probabilities are generally quantized using only 4-8 bits (instead of 8-byte floats), and n-grams are stored in reverse tries.
    \item An n-gram language model can also be shrunk by pruning, for example only storing n-grams with counts greater than some threshold (such as the count threshold of 40 used for the Google n-gram release) or using entropy to prune less-important n-grams.
    \item Another option is to build approximate language models using techniques like \indexlabel{Bloom filters}.
    \item efficient language model toolkits like \href{https://github.com/kpu/kenlm}{https://github.com/kpu/kenlm (\textbf{Kenlm})}\indexlabel{KenLM} use sorted arrays, efficiently combine probabilities and backoffs in a single value, and use merge sorts to efficiently build the probability tables in a minimal number of passes through a large corpus.
\end{enumerate}












































