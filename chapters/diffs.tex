\chapter{Differences}

\section{Supremum VS Maximum \cite{chatgpt}}\label{Supremum VS Maximum}

\begin{longtable}{|p{3cm}|p{6cm}|p{6cm}|}
    \hline
    \textbf{Aspect} & \textbf{Supremum} & \textbf{Maximum}\\ \hline
    \endfirsthead

    \hline
    \textbf{Aspect} & \textbf{Supremum} & \textbf{Maximum}\\ \hline
    \endhead

    \hline\endfoot
    \hline\endlastfoot

    \textbf{Inclusion in the Set} & The supremum of a set may or may not be an element of the set. & The maximum of a set must be an element of the set. \\
    \hline
    
    \textbf{Existence} & Every non-empty set bounded above in a complete ordered set (like the real numbers) has a supremum & A maximum exists only if there is at least one element that is greater than or equal to all other elements in the set\\
    \hline

    \textbf{Relationship} & If the maximum exists for a set S, it is equal to the supremum. In other words, if $max(S)$ exists, then $sup(S) = max(S)$ & If the maximum does not exist, the supremum is still defined as the least upper bound\\
    \hline

    \textbf{Example} & 
    \begin{minipage}{5cm}
        \vspace{0.1cm}
        $S = \dCurlyBrac{x \in R | x < 2}$ \\
        $sup(S) = 2$\\
        $max(S)$ doesnâ€™t exist
        \vspace{0.1cm}
    \end{minipage} &
    \begin{minipage}{5cm}
        \vspace{0.1cm}
        $S = \dCurlyBrac{x \in R | x \leq 2}$ \\
        $sup(S) = 2$\\
        $max(S) = 2$
        \vspace{0.1cm}
    \end{minipage} \\
    \hline
\end{longtable}


\section{Infimum VS Minimum}\label{Infimum VS Minimum}
Similar to \fullref{Supremum VS Maximum}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Covariate Shift (Data Shift) VS Concept Shift VS Prior Probability Shift (Label Shift) \cite{chatgpt}} \label{Covariate Shift (Data Shift) VS Concept Shift VS Prior Probability Shift (Label Shift)}

\begin{longtable}{|m{2cm}|m{4.5cm}|m{4.5cm}|m{4.5cm}|}
    \hline
    
    \textbf{Aspect} & \textbf{Covariate Shift (Data Shift)} & \textbf{Concept Shift} & \textbf{Prior Probability Shift (Label Shift)} \\ \hline
    \endfirsthead

    \hline
    \textbf{Aspect} & \textbf{Covariate Shift (Data Shift)} & \textbf{Concept Shift} & \textbf{Prior Probability Shift (Label Shift)} \\ \hline
    \endhead
    
    \hline\endfoot
    
    \hline\endlastfoot
    
    \textbf{Definition} & Change in the distribution of the input data (features) between training and test sets. & Change in the conditional distribution of the output given the input. & Change in the distribution of the output labels between training and test sets. \\ \hline
    
    \textbf{Cause} & Different data sources, sampling methods, or temporal changes affecting features. & Changes in the underlying relationship between features and labels, often due to evolving contexts or environments. & Variations in the frequency or proportion of different labels in the dataset over time. \\ \hline

    \textbf{Effect} & Model performance degradation due to misaligned feature distributions. & Model predictions become inaccurate because the learned relationships are no longer valid. & Bias in predicted label distribution if not accounted for, leading to misclassification. \\ \hline

    \textbf{Detection} & Comparing feature distributions (e.g., using statistical tests like KS test, histograms). & Analyzing the performance metrics over time or checking for changes in decision boundaries. & Comparing label distributions (e.g., using chi-square tests) or through domain knowledge. \\ \hline

    \textbf{Adjustment Techniques} & Reweighting samples, domain adaptation methods, or using robust models. & Regular retraining with updated data, transfer learning, or domain adaptation techniques. & Adjusting model outputs using methods like importance weighting, resampling, or calibration. \\ \hline

    \textbf{Example} & A retail model trained on summer sales data might perform poorly in winter due to changes in purchasing behavior. & A spam detection model might become less effective if spammers change tactics and the nature of spam emails evolves. & A medical diagnosis model might require adjustment if the prevalence of certain diseases changes in the population. \\ \hline

    \textbf{Relevance in Real-world Applications} & High when dealing with non-stationary environments or merging datasets from different sources. & High in dynamic environments where the relationship between input features and output labels can evolve. & High in situations with shifting class distributions, like seasonal changes or demographic shifts. \\ \hline

\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Supervised Learning VS Unsupervised Learning VS Reinforcement Learning \cite{chatgpt,medium-numsmt2-rl-ch1-part-1}}\label{Supervised Learning VS Unsupervised Learning VS Reinforcement Learning}

\begin{longtable}{|p{1.6cm}|p{4.5cm}|p{4.5cm}|p{4.5cm}|}
    \hline
    \textbf{Aspect} & \textbf{Supervised Learning} & \textbf{Unsupervised Learning} & \textbf{Reinforcement Learning} \\
    \hline
    \endfirsthead

    \hline
    \textbf{Aspect} & \textbf{Supervised Learning} & \textbf{Unsupervised Learning} & \textbf{Reinforcement Learning} \\
    \hline
    \endhead
    
    \hline\endfoot
    \hline\endlastfoot
    
    \textbf{Training data} & Labeled data: Input-output pairs are provided for training. & Unlabeled data: Only input data is provided without corresponding output labels. & Trained from examples \& interaction with environment. \\
    \hline
    \textbf{Goal} & To learn a mapping from input data to output labels based on examples provided during training. & To find underlying patterns or structures in input data without explicit output labels. & To learn a policy that maximizes the cumulative reward by interacting with the environment. \\
    \hline
    \textbf{Feedback} & Direct feedback (error signal) is given during training based on the comparison between predicted output and actual output labels. & No explicit feedback based on labeled data; learning typically involves finding patterns or representations within the data. & Feedback is in the form of rewards or penalties from the environment. \\
    \hline
    \textbf{Examples} & Classification, regression tasks. & Clustering, association, dimensionality reduction. & Game playing, robotics, resource management. \\
    \hline
\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Automatic Differentiation (AutoDiff) VS Backpropagation VS Chain Rule \cite{chatgpt}}\label{Automatic Differentiation (AutoDiff) VS Backpropagation VS Chain Rule}

\begin{longtable}{|p{2cm}|p{5cm}|p{3.5cm}|p{3.5cm}|}
    \hline
    \textbf{Aspect} & \textbf{Automatic Differentiation (AutoDiff)} & \textbf{Backpropagation} & \textbf{Chain Rule} \\
    \hline
    \endfirsthead
    
    \hline
    \textbf{Aspect} & \textbf{Automatic Differentiation (AutoDiff)} & \textbf{Backpropagation} & \textbf{Chain Rule} \\
    \hline\endhead
    
    \hline \endfoot
    
    \hline\endlastfoot
    
    \textbf{Definition} & Techniques for numerically evaluating derivatives of functions & Specific application of reverse mode AutoDiff for training neural networks & Calculus principle for computing derivatives of composed functions \\
    \hline

    \textbf{Modes} & Forward Mode, Reverse Mode & Reverse Mode & Not applicable \\
    \hline
    
    \textbf{Process} & Breaks down computation into elementary operations & Forward Pass, Loss Computation, Backward Pass & \(\dfrac{dy}{dx} = \dfrac{dy}{du} \cdot \dfrac{du}{dx}\) \\
    \hline
    
    \textbf{Used For} & Computing derivatives in general computational graphs & Training neural networks by computing gradients of the loss function & Computing derivatives of composed functions \\
    \hline
    
    \textbf{Efficiency} & Forward Mode: Efficient for few inputs, many outputs & Efficient for training neural networks due to large number of parameters & Applies to both AutoDiff and Backpropagation \\
    & Reverse Mode: Efficient for many inputs, few outputs & & \\
    \hline
    
    \textbf{Application} & Any function specified by a computer program & Neural network training (gradient-based optimization) & Fundamental concept in calculus \\
    \hline
    
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Batch Gradient Descent VS Mini-Batch Gradient Descent VS Gradient Descent (SGD) \cite{chatgpt}} \label{Batch Gradient Descent VS Mini-Batch Gradient Descent VS Gradient Descent (SGD)}

\begin{longtable}{|p{2.5cm}|p{4cm}|p{3.5cm}|p{3.5cm}|}
    \hline
    \textbf{Aspect} & \textbf{Batch Gradient Descent} & \textbf{Mini-Batch Gradient Descent} & \textbf{Gradient Descent (SGD)} \\
    \hline
    \endfirsthead
    
    \hline
    \textbf{Aspect} & \textbf{Batch Gradient Descent} & \textbf{Mini-Batch Gradient Descent} & \textbf{Gradient Descent (SGD)} \\
    \hline
    \endhead
    
    \hline\endfoot
    \hline\endlastfoot
    
    \textbf{Definition} & Updates parameters after computing the gradient on the entire dataset & Updates parameters after computing the gradient on a small batch of data & Updates parameters for each training example \\
    \hline

    \textbf{Update Frequency} & Low (after the entire dataset) & Medium (after each mini-batch) & High (after each example) \\
    \hline
    
    \textbf{Computation Cost/ Update} & High & Medium & Low \\
    \hline
    
    \textbf{Memory Usage} & High & Medium & Low \\
    \hline
    
    \textbf{Convergence Speed} & Slow & Fast & Fast \\
    \hline
    
    \textbf{Stability of Updates} & Stable (low variance) & Moderate (reduced variance) & Noisy (high variance) \\
    \hline
    
    \textbf{Convergence} & Guaranteed to decrease cost function & More stable convergence than SGD & May not converge to the global minimum \\
    \hline
    
    \textbf{Practical Use} & Suitable for smaller datasets & Suitable for large datasets & Suitable for very large datasets \\
    \hline
    
    \textbf{Hardware Optimization} & Limited & Allows for parallel processing & Limited \\
    \hline
    
    \textbf{Escape Local Minima} & Less likely & Yes (benefits of both SGD and Batch) & Yes (due to noisy updates) \\
    \hline
    
    \textbf{Example Size} & Entire dataset & Mini-batch (subset of dataset) & Single example \\
    \hline
    
    \textbf{Efficiency} & Low for large datasets & High, balancing efficiency and performance & High for very large datasets \\
    \hline

\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation Metrics VS Criterion}\label{Evaluation Metrics VS Criterion}

\begin{longtable}{|p{2cm}|p{6cm}|p{6cm}|}
    \hline
    \textbf{Aspect} & \textbf{Evaluation Metrics} & \textbf{Loss Function (Criterion)} \\
    \hline
    \endfirsthead

    \hline
    \textbf{Aspect} & \textbf{Evaluation Metrics} & \textbf{Loss Function (Criterion)} \\
    \hline\endhead
    
    \hline\endfoot
    \hline\endlastfoot
    
    \textbf{Purpose} & Measure model performance on test or validation data after training. & Quantify error between predicted and actual values to guide optimization during training. \\
    \hline
    
    \textbf{Usage} & Assess model generalization and overall performance. & Internally used by optimizer to update model parameters based on prediction errors. \\
    \hline
    
    \textbf{Focus} & Reflects how well the model generalizes to new, unseen data. & Minimizing error on training data to improve model accuracy. \\
    \hline

    \textbf{Examples} & Accuracy, precision, recall, F1-score for classification tasks; RMSE, MAE for regression tasks. & MSE (Mean Squared Error), cross-entropy loss for classification tasks, etc. \\
    \hline
\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Perceptron vs Artificial Neuron \cite{chatgpt}}\label{Perceptron vs Artificial Neuron}

\begin{longtable}{| m{3cm} | m{6cm} | m{6cm} |}
    
    \hline
    \textbf{Feature} & \textbf{Perceptron} & \textbf{Neuron in a Neural Network} \\
    \hline
    \endfirsthead
    
    \hline
    \textbf{Feature} & \textbf{Perceptron} & \textbf{Neuron in a Neural Network} \\
    \hline
    \endhead
    
    \hline
    \endfoot
    
    \hline
    \endlastfoot
    
    \textbf{Definition} & A type of artificial neuron used in binary classifiers. & A general computational unit in a neural network. \\
    \hline
    
    \textbf{Activation Function} & Uses a step function (binary output: 0 or 1). & Uses various activation functions (e.g., ReLU, sigmoid, tanh). \\
    \hline
    
    \textbf{Complexity} & Simple and linear. & Can be complex and non-linear. \\
    \hline
    
    \textbf{Learning Rule} & Uses the perceptron learning rule. & Uses more advanced learning rules (e.g., backpropagation). \\
    \hline
    
    \textbf{Output} & Binary output (0 or 1). & Continuous output, depending on the activation function. \\
    \hline
    
    \textbf{Applications} & Suitable for linear classification tasks. & Suitable for both linear and non-linear tasks. \\
    \hline
    
    \textbf{Historical Context} & One of the earliest models of an artificial neuron, introduced by Frank Rosenblatt in 1958. & Evolved from the perceptron, used in modern deep learning. \\
    \hline
    
    \textbf{Layers} & Typically used in single-layer models. & Used in multiple layers (deep networks). \\
    \hline

    \textbf{Expressive Power} & Limited to linearly separable problems. & Can model complex relationships and patterns. \\
    \hline

\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Connectionism vs ANN \cite{chatgpt,arxiv-2405.04048}}\label{connectionism vs ann}


\begin{longtable}{|>{\raggedright\arraybackslash}p{3cm}|>{\raggedright\arraybackslash}p{6cm}|>{\raggedright\arraybackslash}p{6cm}|}
    
    \hline
    \textbf{Aspect} & \textbf{Connectionism} & \textbf{Artificial Neural Networks (ANNs)} \\
    \hline
    \endfirsthead
    
    \hline
    \textbf{Aspect} & \textbf{Connectionism} & \textbf{Artificial Neural Networks (ANNs)} \\
    \hline\endhead
    
    \hline\endfoot
    
    \hline\endlastfoot
    
    \textbf{Definition} & Theoretical framework for understanding the mind using interconnected networks of simple units. & Computational models inspired by brain's neural networks, used in machine learning. \\
    \hline
    
    \textbf{Scope} & Broad approach to cognition and brain function. & Specific computational implementations within connectionism. \\
    \hline
    
    \textbf{Focus} & Understanding cognitive processes and brain function. & Practical machine learning applications. \\
    \hline
    
    \textbf{Origins} & Cognitive science and psychology. & Efforts to mimic brain function for computational purposes. \\
    \hline
    
    \textbf{Key Concepts} & Neural networks, learning, parallel distributed processing, emergent properties. & Architecture (layers), activation functions, training (e.g., backpropagation), types (e.g., CNNs, RNNs). \\
    \hline
    
    \textbf{Learning} & Adjusting connection strengths based on experience. & Training involves adjusting weights to minimize prediction errors. \\
    \hline
    
    \textbf{Information Processing} & Parallel and distributed. & Parallel and distributed, with specific architectures (e.g., feedforward, convolutional). \\
    \hline
    
    \textbf{Applications} & Understanding perception, memory, language; modeling psychological phenomena. & Image and speech recognition, natural language processing, autonomous vehicles, medical diagnosis, financial forecasting. \\
    \hline
    
    \textbf{Theoretical Foundation} & Provides conceptual framework for cognition and brain modeling. & Empirical support for connectionist theories, demonstrating learning and task performance. \\
    \hline

    \textbf{Practical Implementation} & Conceptual approach with some computational models. & Implementations used in AI and machine learning. \\
    \hline
\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Multilayer Perceptron (MLP) vs Artificial Neural Network (ANN)}\label{mlp vs ann}


\begin{longtable}{|>{\raggedright}m{4cm}|>{\raggedright}m{5cm}|>{\raggedright\arraybackslash}m{5cm}|}
    \hline
    \textbf{Feature} & \textbf{MLP (Multilayer Perceptron)} & \textbf{ANN (Artificial Neural Network)} \\
    \hline
    \endfirsthead

    \hline
    \textbf{Feature} & \textbf{Multilayer Perceptron (MLP)} & \textbf{Artificial Neural Network (ANN)} \\
    \hline
    \endhead

    \hline\endfoot
    
    \hline\endlastfoot
    
    \textbf{Definition} & A type of ANN with multiple layers & A broad category of machine learning models inspired by the human brain \\
    \hline
    
    \textbf{Structure} & Consists of input, hidden, and output layers & Can have various structures, including MLP, CNN, RNN, etc. \\
    \hline
    
    \textbf{Layers} & Always has at least one hidden layer & Can have any number of layers and types \\
    \hline
    
    \textbf{Neurons} & Fully connected layers of neurons & Can include different types of neurons (e.g., convolutional, recurrent) \\
    \hline
    
    \textbf{Training} & Typically trained with backpropagation & Can be trained with various algorithms depending on the type \\
    \hline
    
    \textbf{Usage} & Commonly used for classification and regression tasks & Used for a wide range of tasks including image recognition, natural language processing, etc. \\
    \hline
    
    \textbf{Complexity} & Generally less complex compared to other ANNs like CNNs and RNNs & Can range from simple to highly complex models \\
    \hline
    
    \textbf{Flexibility} & Less flexible compared to other ANN architectures & Highly flexible with many architectures available \\
\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model Parameter VS Model Hyperparameter \cite{chatgpt}}\label{Model Parameter VS Model Hyperparameter}

\begin{longtable}{|p{3cm}|p{6cm}|p{6cm}|}
    \hline
    \textbf{Aspect} & \textbf{Model Parameter} & \textbf{Model Hyperparameter} \\
    \hline
    \endfirsthead

    \hline
    \textbf{Aspect} & \textbf{Model Parameter} & \textbf{Model Hyperparameter} \\
    \hline
    \endhead

    \hline\endfoot
    \hline\endlastfoot

    \textbf{Setting/ changing} & Learned from the training data & Set before the training process \\ 
    \hline

    \textbf{Impact/ Significance} & Directly affect the predictions of the model & Affect the training process and the structure of the model \\
    \hline
\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Underfitting VS Overfitting \cite{chatgpt}}\label{Underfitting VS Overfitting}


\begin{longtable}{|p{3cm}|p{6cm}|p{6cm}|}
    \hline
    \textbf{Aspect} & \textbf{Underfitting} & \textbf{Overfitting} \\ 
    \hline
    \endfirsthead

    \hline
    \textbf{Aspect} & \textbf{Underfitting} & \textbf{Overfitting} \\ 
    \hline
    \endhead

    \hline\endfoot
    \hline\endlastfoot

    \textbf{Modelâ€™s complexity} & Underfitting occurs when a model is too simple to capture the underlying structure of the data & Overfitting occurs when a model is too complex and learns the noise in the training data rather than the actual underlying pattern\\
    \hline

    \textbf{Modelâ€™s Params} & model has too few parameters & model has too many parameters relative to the amount of training data \\
    \hline

    \textbf{Num of training iters} & model is not trained for enough iterations & reduce number of iters\\
    \hline

    \textbf{Training data performance} & Poor & Excellent\\
    \hline

    \textbf{Test data performance} & Poor & Poor \\
    \hline

    \textbf{Bias} & High & \\
    \hline

    \textbf{Variance} & & High \\
    \hline

    \textbf{Solution(s)} & \tableenumerate{
        \item \textbf{Increase model complexity}: Use a more complex model with more parameters
        
        \item \textbf{Train longer}: Ensure the model has sufficient time to learn from the data.
        
        \item \textbf{Feature engineering}: Create more relevant features that can help the model capture the underlying patterns
        
        \item \textbf{Reduce regularization}: Decrease the strength of regularization to allow the model to fit the training data better.
    } &
    \tableenumerate{
        \item \textbf{Simplify the model}: Use a less complex model with fewer parameters
        
        \item \textbf{Increase training data}: More data can help the model learn the underlying pattern rather than noise
        
        \item \textbf{Use regularization}: Techniques like L1 or L2 regularization can penalize large coefficients and prevent the model from becoming too complex
        
        \item \textbf{Cross-validation}: Use cross-validation techniques to ensure the model generalizes well to unseen data
        
        \item \textbf{Pruning}: In decision trees, remove branches that have little importance to reduce complexity
    }\\
    \hline


    \textbf{Risk Estimate} \cite{mfml-1} & & risk estimate from the training data $R_{emp}(f, X_{train}, y_{train})$ underestimates the expected risk $R_true(f)$\\
    \hline



\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Likelihood VS Marginal likelihood (evidence) \cite{chatgpt}} \label{Likelihood VS Marginal likelihood (evidence)}

\begin{longtable}{|p{3cm}|p{6cm}|p{6cm}|}
    \hline
    \textbf{Aspect} & \textbf{Likelihood} & \textbf{Marginal Likelihood} \\
    \hline
    \endfirsthead

    \hline
    \textbf{Aspect} & \textbf{Likelihood} & \textbf{Marginal Likelihood} \\
    \hline
    \endhead

    \hline\endfoot
    \hline\endlastfoot

    \textbf{Definition} & The likelihood function measures how likely it is to observe the given data under different parameter values of a statistical model. It is a function of the model parameters, given the data. & The marginal likelihood, or evidence, is the probability of observing the data under a particular model, integrated over all possible parameter values weighted by their prior distribution. It serves as a measure of how well a model explains the observed data, considering both the likelihood and the prior distribution of the parameters. \\
    \hline

    \textbf{Formula} & $L(\theta;X) = P(X|\theta)$ & $P(X|M) = \int P(X|\theta,M) P(\theta|M) d\theta$ \\
    \hline

    \textbf{Usage} & 
    \tableenumerate{
        \item \textbf{Parameter Estimation}: In frequentist statistics, the maximum likelihood estimation (MLE) involves finding the parameter values that maximize the likelihood function.
        
        \item \textbf{Model Fitting}: It helps in fitting the model to the observed data by identifying the most probable parameters
    } &
    \tableenumerate{
        \item \textbf{Model Comparison}: In Bayesian model comparison, the marginal likelihood is used to compute the Bayes factor, which compares the relative plausibility of different models.
        
        \item \textbf{Model Selection}: It helps in selecting the model that best explains the observed data while incorporating the prior beliefs about the parameters.
    }\\
    \hline

    \textbf{Characteristics} &
    \tableenumerate{
        \item  It is not a probability distribution over the parameters but rather a measure of fit
        
        \item  It does not account for the prior distribution of parameters (in a Bayesian context)
    } &
    \tableenumerate{
        \item It accounts for both the fit of the model to the data (through the likelihood) and the complexity of the model (through the prior).
        
        \item It is a probability of the data given the model, hence it is used for comparing different models
    }\\
    \hline

    \textbf{Model Fitting} & likelihood is prone to overfitting & marginal likelihood is typically not as the model parameters have been marginalized out (i.e., we no longer have to fit the parameters) \\
    \hline

\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Linked List VS Array \cite{geeksforgeeks/linked-list-vs-array}}\label{Linked List VS Array}

\begin{longtable}{|p{3cm}|p{6cm}|p{6cm}|}
    \hline
    \textbf{Aspect} & \textbf{Array} & \textbf{Linked List} \\
    \hline
    \endfirsthead

    \hline
    \textbf{Aspect} & \textbf{Array} & \textbf{Linked List} \\
    \hline
    \endhead

    \hline\endfoot
    \hline\endlastfoot

    \textbf{Memory} & contiguous & not contiguous \\
    \hline

    \textbf{Size} & Fixed & Dynamic \\
    \hline

    \textbf{Memory allocation} & Compile time & Runtime \\
    \hline

    \textbf{Memory Usage} & Less & More \\
    \hline

    \textbf{Element Access} & Easy & Difficult\\
    \hline

    \textbf{Insertion \& Deletion operations} & Difficult & Easy \\
    \hline
\end{longtable}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Morphemes VS Lemma VS Stem \cite{chatgpt}} \label{Morphemes VS Lemma VS Stem}

\begin{longtable}{|p{2.5cm}|p{4cm}|p{4cm}|p{4cm}|}
    \hline
    \textbf{Aspect} & \textbf{Morpheme} & \textbf{Lemma} & \textbf{Stem} \\
    \hline
    \endfirsthead

    \hline
    \textbf{Aspect} & \textbf{Morpheme} & \textbf{Lemma} & \textbf{Stem} \\
    \hline
    \endhead

    \hline\endfoot
    \hline\endlastfoot

    \textbf{Definition} & The smallest grammatical units in a language carrying meaning; cannot be further divided. & The canonical or dictionary form of a word, encompassing all its inflected forms. & The form of a word to which affixes can be added, representing the core meaning. \\
    \hline

    \textbf{Function} & Constructs words and conveys grammatical relationships and meanings & Used for dictionary entries and linguistic analysis & Serves as the base for inflectional and derivational affixes \\
    \hline

    \textbf{Types} & Free Morphemes (can stand alone) and Bound Morphemes (cannot stand alone) & Single form representing all inflections of a word & Base form which can have derivational or inflectional affixes attached\\
    \hline

    \textbf{Word Analysis} & "unhappiness" = "un-" + "happy" + "-ness" & Lemma for "running", "ran", "runs" is "run" & Stem for "running", "runs" is "run" \\
    \hline
\end{longtable}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Ridge Regression (L2) VS Lasso Regression (L1)}\label{Ridge Regression VS Lasso Regression}

\begin{longtable}{|p{7cm}|p{7cm}|}
    \hline
    \textbf{Ridge Regression} & \textbf{Lasso Regression}\\
    \hline
    \endfirsthead
    
    \hline
    \textbf{Ridge Regression} & \textbf{Lasso Regression}\\
    \hline
    \endhead

    \hline\endfoot
    \hline\endlastfoot

    Shrinks the coefficients toward zero & Encourages some coefficients to be exactly zero\\
    \hline

    Adds a penalty term proportional to the sum of squared coefficients & Adds a penalty term proportional to the sum of absolute values of coefficients \\
    \hline

    Does not eliminate any features & Can eliminate some features \\
    \hline

    Suitable when all features are importantly & Suitable when some features are irrelevant or redundant\\
    \hline

    More computationally efficient & Less computationally efficient\\
    \hline

    Requires setting a hyperparameter & Requires setting a hyperparameter\\
    \hline

    Performs better when there are many small to medium-sized coefficients & Performs better when there are a few large coefficients\\
    \hline
\end{longtable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Pearsonâ€™s Rho ($\rho_P$) VS Spearmanâ€™s Rho ($\rho_S$) VS Kendallâ€™s Tau Correlation ($\tau_K$)} \label{Pearsonâ€™s Rho VS Spearmanâ€™s Rho VS Kendallâ€™s Tau Correlation}

\begin{enumerate}
    \item They all describe a different aspect of the population and the difference can be quite large depending on the family of CDFs.

    \item product-moment correlation (Pearsonâ€™s Rho) provides a measure that is particularly suitable for linear associations between variables.

    \item data is normally distributed:
    \begin{enumerate}
        \item when the data are bivariate normally distributed, or when transformations can be applied that would make the data approximately bivariate normal, apply the product-moment estimator on the (transformed) data

        \item Pearsonâ€™s estimator $r_P$ is more efficient (i.e., has a smaller asymptotic variance) than Spearmanâ€™s estimator $r_S$
    \end{enumerate}

    \item data is \textbf{NOT} normally distributed:
    \begin{enumerate}
        \item Often Pearsonâ€™s product-moment estimator is immediately disqualified, since it is affected by transformations of the data. 

        \item prefer Kendallâ€™s tau estimator over Spearman when the data is continuous, while we prefer Spearmanâ€™s rank when there are some ties
    \end{enumerate}

    \item For large datasets Kendallâ€™s tau estimator is more efficient than Spearmanâ€™s rank correlation (the variances of $r_K$ and $r_S$ in the Fisher $z$-transformation. For Kendallâ€™s tau this variance was $0.437/(n - 4)$, while it was at least $1/(n - 3)$ for Spearmanâ€™s rank correlation)

    \item when the data contain ties (i.e., values among the $x$ variable and/or values among the $y$ variable are equal), Spearmanâ€™s rank correlation has a smaller standard error than Kendallâ€™s tau estimator

\end{enumerate}

\section{PDF (Probability Density Function) VS PMF (Probability Mass Function) VS CDF (Cumulative Distribution Function)}\label{PDF (Probability Density Function) VS PMF (Probability Mass Function) VS CDF (Cumulative Distribution Function)}

\begin{table}[H]
    \centering
    \begin{tabular}{| m{2cm} | m{4cm} | m{4cm} | m{4cm} |}
        \hline
        
        \textbf{Aspect} & \textbf{PDF (Probability Density Function)} & \textbf{PMF (Probability Mass Function)} & \textbf{CDF (Cumulative Distribution Function)} \\
        \hline
        
        \textbf{Applies to} & Continuous random variables & Discrete random variables & Both continuous and discrete variables \\
        \hline
        
        \textbf{Definition} & Likelihood of the variable taking a specific value & Probability of the variable taking a specific value & Probability that the variable is less than or equal to a specific value \\
        \hline
        
        \textbf{Value Range} & Can be greater than 1 but integrates to 1 over all possible values & Between 0 and 1 & Between 0 and 1 \\
        \hline
        
        \textbf{Probability Calculation} & \(\int_{a}^{b} f_X(x) \, dx\) for interval \([a, b]\) & \(P(X = x)\) & \(P(X \leq x)\) \\
        \hline
        
        \textbf{Integral/ Sum Requirement} & Integral of the PDF over the entire range is 1 & Sum of the PMF over all possible values is 1 & \(\lim_{x \to -\infty} F_X(x) = 0\) and \(\lim_{x \to \infty} F_X(x) = 1\) \\
        \hline
        
        \textbf{Example} & \(f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}\) & \(P(X = x) = \frac{1}{6}\) for a fair die & \(F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt\) for continuous, \(F_X(x) = \sum_{x_i \leq x} P(X = x_i)\) for discrete \\
        \hline
    \end{tabular}
\end{table}


\section{Sum of Random Variables VS Mixture Distribution \cite{chatgpt}}\label{Sum of Random Variables VS Mixture Distribution}

\begin{longtable}{|p{2.5cm}|p{5cm}|p{5cm}|}
    \hline
    \textbf{Aspect} & \textbf{Sum of Random Variables} & \textbf{Mixture Distribution} \\
    \hline
    \endfirsthead
    
    \hline
    \textbf{Aspect} & \textbf{Sum of Random Variables} & \textbf{Mixture Distribution} \\
    \hline
    \endhead
    
    \hline
    \endfoot
    
    \hline
    \endlastfoot
    
    \textbf{Nature} & 
    Creates a new random variable by adding the original variables. & 
    Combines different distributions into one, considering the probabilities of each component. \\
    \hline
    
    \textbf{Definition} & 
    If \( X \) and \( Y \) are random variables, \( Z = X + Y \). The distribution of \( Z \) is derived from the distributions of \( X \) and \( Y \). & 
    A weighted sum of component distributions. For example, \( f(x) = \dsum_{i=1}^{k} \pi_i f_i(x) \), where \( \dsum_{i=1}^{k} \pi_i = 1 \). \\
    \hline
    
    \textbf{Independence} & 
    For independent random variables, the distribution of the sum is the convolution of their distributions. & 
    Does not require component distributions to be independent. \\
    \hline
    
    \textbf{Dependence} & 
    For dependent random variables, the sum's distribution depends on their joint distribution. & 
    Components can be dependent, but the mixture model itself does not model dependence between components. \\
    \hline
    
    \textbf{Central Limit Theorem} & 
    The sum of a large number of i.i.d. random variables tends to be normally distributed. & 
    Not directly related to the Central Limit Theorem. \\
    \hline
    
    \textbf{Applications} & 
    Used in contexts involving additive processes (e.g., total score, aggregated risk). & 
    Used in modeling populations with inherent heterogeneity (e.g., multi-modal distributions). \\
    \hline
    
    \textbf{Computation} & 
    Involves convolution for independent variables, which can be mathematically intensive. & 
    Involves weighted sums, and parameter estimation often uses methods like Expectation-Maximization (EM). \\
    \hline

\end{longtable}


\section{Confidence Interval (CI) VS Asymptotic Confidence Interval (Asymptotic CI) \cite{chatgpt}} \label{Confidence Interval (CI) VS Asymptotic Confidence Interval (Asymptotic CI)}

\begin{longtable}{|p{3cm}|p{6cm}|p{6cm}|}
    \hline
    \textbf{Aspect} & \textbf{Confidence Interval (CI)} & \textbf{Asymptotic Confidence Interval (Asymptotic CI)} \\
    \hline
    \endfirsthead
    
    \hline
    \textbf{Aspect} & \textbf{Confidence Interval (CI)} & \textbf{Asymptotic Confidence Interval (Asymptotic CI)} \\
    \hline
    \endhead
    
    \hline\endfoot
    
    \hline\endlastfoot
    
    \textbf{Definition} & A range of values derived from sample data that is likely to contain the true parameter with a certain probability. & A confidence interval that becomes increasingly accurate as the sample size grows, often based on asymptotic properties. \\
    \hline

    \textbf{Basis} & Often derived from exact statistical methods or models, considering the sample size and distribution. & Derived from asymptotic theory, often assuming large sample sizes and normality of the estimator distribution. \\
    \hline
    
    \textbf{Sample Size} & Can be used for both small and large sample sizes, depending on the method. & Primarily used for large sample sizes where asymptotic properties are valid. \\
    \hline
    
    \textbf{Accuracy} & More accurate for small samples when exact methods are used. & Accuracy improves with larger sample sizes; may be less accurate for small samples. \\
    \hline
    
    \textbf{Distribution} & May use exact distributions (e.g., t-distribution) appropriate to the sample size. & Assumes that the estimator follows a normal distribution as sample size increases (Central Limit Theorem). \\
    \hline
    
    \textbf{Computation} & Can be more complex and involve exact calculations or simulations. & Often simpler and relies on large-sample approximations. \\
    \hline
    
    \textbf{Examples} & Exact t-interval for small sample sizes, exact binomial confidence intervals. & Approximate normal confidence interval for means, large-sample z-intervals. \\
    \hline
\end{longtable}























