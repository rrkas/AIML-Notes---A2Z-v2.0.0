\chapter{Data For ML: Text Data}

\section{Herdan’s Law or Herdan’s Law ( $|V| = kN^{\beta}$ ) \cite{nlp-1}}

\begin{equation}
    |V| = kN^{\beta}
\end{equation}

\begin{itemize}
    \item $0 < \beta < 1$ 
    \item $k > 0$
    \item The larger the corpora we look at, the more word types we find.
    \item value of $\beta$ depends on the corpus size and the genre
\end{itemize}


\section{Co-occurrence Matrix/ term-term matrix/ word-word matrix/ term-context matrix \cite{nlp-1}}\label{Co-occurrence Matrix/ term-term matrix/ word-word matrix/ term-context matrix}

\url{https://drive.google.com/file/d/14x7oawk84MPvBV7MEU7Q6ovVudMfUHBA/view}\\
120/577








































