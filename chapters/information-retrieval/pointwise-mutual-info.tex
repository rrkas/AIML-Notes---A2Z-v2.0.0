\chapter{Positive Pointwise Mutual Information (PPMI) \cite{nlp-1}}

\section{Pointwise mutual information ( $\rcmdXPMI(w,c)$ ) \cite{nlp-1}}
It is a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent:
\[
    I(x,y) = \log_2\left(\displaystyle\frac{P(x,y)}{P(x)P(y)}\right)
\]
\[
    \displaystyle I(X,Y) = \sum_x \sum_y P(x,y) \log_2\left(\frac{P(x,y)}{P(x)P(y)}\right)
\]
\[
    \rcmdXPMI(w,c) = \log_2\left(\displaystyle\frac{P(w,c)}{P(w)P(c)}\right)
\]

\begin{itemize}
    \item The \textbf{numerator} tells us how often we observed the two words together (assuming we compute probability by using the MLE).

    \item The \textbf{denominator} tells us how often we would \textbf{expect} the two words to co-occur assuming they each occurred independently.

    \item the ratio gives us an estimate of how much more the two words co-occur than we expect by chance.

    \item PMI values range from negative to positive infinity $(-\infty, \infty)$.

    \item Negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable unless our corpora are enormous.

    
\end{itemize}


\section{Positive PMI ( $\rcmdXPPMI(w,c)$ ) \cite{nlp-1}}\label{Positive Pointwise Mutual Information (PPMI)}

\[
    \rcmdXPPMI(w,c) = \max\left(\log_2\left(\displaystyle\frac{P(w,c)}{P(w)P(c)}\right), 0\right)
\]


For given co-occurrence matrix $F$:
\begin{itemize}
    \item $W$ rows (words)
    \item $C$ columns (Contexts)
    \item $f_{ij}$ gives the number of times word $w_i$ occurs with context $c_j$
    \item \textbf{PPMI matrix}\index{PPMI matrix}\label{PPMI matrix}: 
    \begin{itemize}
        \item $\rcmdXPPMI_{ij} = \rcmdXPPMI(w_i, c_j) = \rcmdXPPMI(w = i, c = j)$ gives the PPMI value of word $w_i$ with context $c_j$
        
        \item \( \displaystyle P_{ij}=\frac{f_{ij}}{\displaystyle\sum_{i=1}^{W} \sum_{j=1}^{C} f_{ij}}  \quad\quad P_{i*}=\frac{\displaystyle\sum_{j=1}^{C} f_{ij}}{\displaystyle\sum_{i=1}^{W} \sum_{j=1}^{C} f_{ij}} \quad\quad P_{*j}=\frac{\displaystyle\sum_{i=1}^{W} f_{ij}}{\displaystyle\sum_{i=1}^{W} \sum_{j=1}^{C} f_{ij}} \)
        
        \item \( \displaystyle \rcmdXPPMI_{ij} = \max\left( \log_2\left( \frac{p_{ij}}{p_{i*}p_{*j}} \right),0\right) \)
        
    \end{itemize}
\end{itemize}

\vspace{0.2cm}
\textbf{Disadvantage}: PMI has the problem of being biased toward infrequent events; very rare words tend to have very high PMI values.

\textbf{Solutions}:
\begin{enumerate}
    \item One way to reduce this bias toward low frequency events is to slightly change the computation for $P(c)$, using a different function $P_\alpha(c)$ that raises the probability of the context word to the power of $\alpha$.
    \[
        \displaystyle \rcmdXPPMI_\alpha(w, c) = \max\left(\log_2\left(\frac{P(w, c)}{P(w)P_\alpha(c)}\right),0\right) \hfill \left[ P_\alpha(c) = \frac{\rcmdXcount(c)^\alpha}{\displaystyle\sum_c \rcmdXcount(c)^\alpha} \right]
    \]
    
    If $\alpha < 1$, this works because raising the count to $\alpha < 1$ increases the probability assigned to \textbf{rare contexts}, and hence lowers their PMI ($P_\alpha(c) > P(c)$ when $c$ is rare).

    \item \textbf{Laplace smoothing}: Before computing PMI, a small constant $k$ (values of $0.1-3$ are common) is added to each of the counts, shrinking (discounting) all the non-zero values. The larger the $k$, the more the non-zero counts are discounted.
    
\end{enumerate}


















\vspace{4cm}
\url{https://drive.google.com/file/d/14x7oawk84MPvBV7MEU7Q6ovVudMfUHBA/view}\\
129/577




























































