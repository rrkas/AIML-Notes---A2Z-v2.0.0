\chapter{Term Frequency-Inverse Document Frequency (TF-IDF) \cite{nlp-1, ir-1}} \label{Term Frequancy-Inverse Document Frequency (TF-IDF)}


\section{Vectors and documents \cite{nlp-1}}\label{tf-idf: Vectors and documents}
Uses \fullref{Term-document matrix}


\section{Words as vectors: document dimensions \cite{nlp-1}}\label{tf-idf: Words as vectors: document dimensions}

\begin{enumerate}
    \item Vector semantics can also be used to represent the meaning of words. 

    \item We do this row vector by associating each word with a word vector - a \textbf{row vector} rather than a column vector, hence with different dimensions.

    \item similar words have similar vectors because they tend to occur in similar documents. The \fullref{Term-document matrix} thus lets us represent the meaning of a word by the documents it tends to occur in.
\end{enumerate}


\section{Words as vectors: word dimensions \cite{nlp-1}} \label{tf-idf: Words as vectors: word dimensions}

Uses \fullref{Co-occurrence matrix/ term-term matrix/ word-word matrix/ term-context matrix}



\section{Cosine Similarity ( $\rcmdXcosine(\mathbf{v,w})$ / $\cos(\mathbf{v,w})$ ) \cite{nlp-1}}\label{Cosine Similarity}

\textbf{Raw Dot Product}: \( \displaystyle\mathbf{v \cdot w} = \sum_{i=1}^{N} v_i w_i \)

\begin{enumerate}
    \item The \textbf{dot product} acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. 
    
    \item Vectors that have zeros in different dimensions - orthogonal vectors - will have a dot product of 0, representing their strong dissimilarity.

    \item The dot product is higher if a vector is longer, with higher values in each dimension.
    
    \item More frequent words have longer vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them. The raw dot product thus will be higher for frequent words.

    \item \textbf{Disadvantage}: This raw dot product, however, has a problem as a similarity metric: it favors long vectors.

    
\end{enumerate}


\textbf{Normalized Dot Product}: \( \displaystyle \rcmdXcosine(\mathbf{v,w}) = \dfrac{\mathbf{v\cdot w}}{|\mathbf{v}|\cdot|\mathbf{w}|} = \dfrac{\displaystyle\sum_{i=1}^{N} v_i w_i}{\displaystyle \sqrt{\sum_{i=1}^{N} v_i^2}\sqrt{\sum_{i=1}^{N} w_i^2}} \)


\section{TF-IDF: Weighing terms in the vector \cite{nlp-1}}\label{TF-IDF: concept}

\begin{enumerate}
    \item raw frequency is \textbf{NOT} the best measure of association between words.\\
    Raw frequency is very skewed and \textbf{NOT} very discriminative.

    \item \textbf{Paradox}: Words that occur nearby frequently are more important than words that only appear once or twice. Yet words that are too frequent-ubiquitous, like \textit{the} or \textit{good}- are unimportant.

    \item The \textbf{tf-idf weighting} is the product of two terms, each term capturing one of these two intuitions.
\end{enumerate}


\noindent \textbf{SEE}:
\begin{enumerate}
    \item \fullref{Document Frequency}
    \item \fullref{Collection frequency}
    \item \fullref{inverse document frequency/ idf term weight}
    \item \fullref{tf-idf weighted value: formula}
\end{enumerate}


\section{Document similarity using tf-idf}
\begin{enumerate}
    \item The tf-idf model of meaning is often used for document functions like deciding if two documents are similar.

    \item We represent a document by taking the vectors of all the words in the document, and computing the \textbf{centroid} of all those vectors.
    
    \item The centroid is the \textit{multidimensional version of the mean}; the centroid of a set of vectors is a single vector that has the minimum sum of squared distances to each of the vectors in the set.

    \item Given $k$ word vectors $w_1,w_2,...,w_k$, the centroid document vector $d$ is:
    \[
        \displaystyle d = \dfrac{1}{k}\displaystyle\sum_{i=1}^{k} w_i
    \]

    

\end{enumerate}









































































































































