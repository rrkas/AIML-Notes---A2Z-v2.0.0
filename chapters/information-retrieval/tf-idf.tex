\chapter{Term Frequency-Inverse Document Frequency (TF-IDF) \cite{nlp-1}} \label{Term Frequancy-Inverse Document Frequency (TF-IDF)}


\section{Terminologies \cite{nlp-1, chatgpt}}
\begin{table}[h!]
    \centering
    \begin{tabular}{| m{2   cm} | m{6cm} | m{6cm} |}
        \hline
        \textbf{Term} & \textbf{Definition} & \textbf{Example} \\
        \hline
        Corpus & A large and structured set of texts, often used for linguistic research, text mining, or NLP. & The British National Corpus (BNC), which contains 100 million words of text samples from a wide range of sources. \\
        \hline
        Collections & A group of documents or texts gathered together, often around a specific theme, topic, or source. Collections can be subsets within a larger corpus. & A collection of scientific articles on climate change, or a collection of user reviews from a specific website. \\
        \hline
        Documents & Individual pieces of text within a corpus or a collection. They are the smallest unit of analysis and can vary in length and format. & A single news article, a research paper, a blog post, or a book chapter. \\
        \hline
    \end{tabular}
\end{table}


\section{Vectors and documents \cite{nlp-1}}\label{tf-idf: Vectors and documents}

\begin{itemize}
    \item In a \textbf{term-document matrix}\index{term-document matrix} \label{term-document matrix}, each row represents a word in the vocabulary and each column represents a document from some collection of documents.
    
    \item The term-document matrix was first defined as part of the \textbf{vector space model}\index{vector space model}\label{vector space model} of information retrieval. In this model, a document is represented as a count vector.

    \item in term-document matrices, the vectors representing each document would have dimensionality $|V|$ (vocabulary size).

    \item Two documents that are similar will tend to have similar words, and if two documents have similar words their column vectors will tend to be similar.

    \item documents can be represented as vectors in a vector space. A \textbf{vector space} (SEE: \fullref{Vector Spaces}) is a collection of vectors, characterized by their dimension.
\end{itemize}

\section{Words as vectors: document dimensions \cite{nlp-1}}\label{tf-idf: Words as vectors: document dimensions}

\begin{itemize}
    \item Vector semantics can also be used to represent the meaning of words. 

    \item We do this row vector by associating each word with a word vector - a \textbf{row vector} rather than a column vector, hence with different dimensions.

    \item similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the documents it tends to occur in.
\end{itemize}


\section{Words as vectors: word dimensions \cite{nlp-1}} \label{tf-idf: Words as vectors: word dimensions}

\begin{itemize}
    \item \textbf{Co-occurrence matrix/ term-term matrix/ word-word matrix/ term-context matrix}\index{Co-occurrence matrix/ term-term matrix/ word-word matrix/ term-context matrix}\label{Co-occurrence matrix/ term-term matrix/ word-word matrix/ term-context matrix}, in which the columns are labeled by words rather than documents. 
    
    \item Dimensionality $|V|\times|V|$ and each cell records the number of times the row (target) word and the column (context) word co-occur in some context in some training corpus.

    \item The context could be the document, in which case the cell represents the number of times the two words appear in the same document.

    \item It is most common, however, to use smaller contexts, generally a window around the word, for example of $4$ words to the left and $4$ words to the right, in which case the cell represents the number of times (in some training corpus) the column word occurs in such a $\pm 4$ word window around the row word.

    \item Since most of the entries of the matrix are zero these are \textbf{sparse vector representations}.
\end{itemize}



\section{Cosine Similarity ( $\rcmdXcosine(\mathbf{v,w})$ / $\cos(\mathbf{v,w})$ ) \cite{nlp-1}}\label{Cosine Similarity}

\textbf{Raw Dot Product}: \( \displaystyle\mathbf{v \cdot w} = \sum_{i=1}^{N} v_i w_i \)

\begin{itemize}
    \item The \textbf{dot product} acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. 
    
    \item Vectors that have zeros in different dimensions - orthogonal vectors - will have a dot product of 0, representing their strong dissimilarity.

    \item The dot product is higher if a vector is longer, with higher values in each dimension.
    
    \item More frequent words have longer vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them. The raw dot product thus will be higher for frequent words.

    \item \textbf{Disadvantage}: This raw dot product, however, has a problem as a similarity metric: it favors long vectors.

    
\end{itemize}


\textbf{Normalized Dot Product}: \( \displaystyle \rcmdXcosine(\mathbf{v,w}) = \frac{\mathbf{v\cdot w}}{|\mathbf{v}|\cdot|\mathbf{w}|} = \frac{\displaystyle\sum_{i=1}^{N} v_i w_i}{\displaystyle \sqrt{\sum_{i=1}^{N} v_i^2}\sqrt{\sum_{i=1}^{N} w_i^2}} \)


\section{TF-IDF: Weighing terms in the vector \cite{nlp-1}}\label{TF-IDF: Weighing terms in the vector}

\begin{itemize}
    \item raw frequency is \textbf{NOT} the best measure of association between words.\\
    Raw frequency is very skewed and \textbf{NOT} very discriminative.

    \item \textbf{Paradox}: Words that occur nearby frequently are more important than words that only appear once or twice. Yet words that are too frequent-ubiquitous, like \textit{the} or \textit{good}- are unimportant.

    \item The \textbf{tf-idf weighting} is the product of two terms, each term capturing one of these two intuitions.
\end{itemize}


\subsection{Term Frequency ( $\rcmdXtf_{t,d}$ ) \cite{nlp-1}} \label{Term Frequency}

the frequency of the word/ term $t$ in the document $d$.
\[
    \rcmdXtf_{t,d} = \rcmdXcount(t,d) \hfill \text{(raw count)}
\]
\[
    \rcmdXtf_{t,d} = \begin{cases}
        1 + \log_{10}(\rcmdXcount(t,d)) & \text{ if } \rcmdXcount(t,d) > 0\\
        0 & \text{ otherwise}
    \end{cases} \hfill \text{(Squashed raw frequency)}
\]

\subsection{Document Frequency ( $\rcmdXdf_t$ ) \cite{nlp-1}}\label{Document Frequency}

\begin{itemize}
    \item The second factor in tf-idf is used to give a higher weight to words that occur only in a few documents.

    \item Terms that are limited to a few documents are useful for discriminating those documents from the rest of the collection; terms that occur frequently across entire collection aren't as helpful.

    \item The document frequency $\rcmdXdf_t$ of a word/ term $t$ is the number of documents it occurs in.

    
\end{itemize}

\subsection{Collection frequency \cite{nlp-1}}\label{Collection frequency}
total number of times the word appears in the whole collection in any document.

\subsection{inverse document frequency/ idf term weight ( $\rcmdXidf_t$ )}\label{inverse document frequency/ idf term weight}
\[
    \rcmdXidf_t = \displaystyle\frac{N}{\rcmdXdf_t} \hfill \text{(raw frequancies)}
\]
\[
    \rcmdXidf_t = \log_{10} \left(\displaystyle\frac{N}{\rcmdXdf_t} \right) \hfill \text{(squashed)}
\]

Where,
\begin{itemize}
    \item $N$ is the total number of documents in the collection
    \item $dft$ is the number of documents in which term t occurs.
\end{itemize}

\vspace{0.2cm}
\textbf{Note}:
\begin{itemize}
    \item The fewer documents in which a term occurs, the higher this weight.

    \item The lowest weight of $1$ is assigned to terms that occur in all the documents.

    
\end{itemize}


\subsection{tf-idf weighted value ( $w_{t,d}$ ) \cite{nlp-1}}\label{tf-idf weighted value}
\[
    w_{t,d} = \rcmdXtf_{t,d} \times\rcmdXidf_t
\]


\section{Document similarity using tf-idf}
\begin{itemize}
    \item The tf-idf model of meaning is often used for document functions like deciding if two documents are similar.

    \item We represent a document by taking the vectors of all the words in the document, and computing the \textbf{centroid} of all those vectors.
    
    \item The centroid is the \textit{multidimensional version of the mean}; the centroid of a set of vectors is a single vector that has the minimum sum of squared distances to each of the vectors in the set.

    \item Given $k$ word vectors $w_1,w_2,...,w_k$, the centroid document vector $d$ is:
    \[
        \displaystyle d = \frac{1}{k}\displaystyle\sum_{i=1}^{k} w_i
    \]

    

\end{itemize}









































































































































