\chapter{Probability}\label{chapter: probability}

\section{Bayes' Rule}\label{Bayes' Rule}

\[
    P(x|y) = \dfrac{P(x)P(y|x)}{P(y)}
    \hfill \text{\cite{dnn-deep-learning-ian}}
\]
\[
    P(y) = \dsum_x P(y|x)P(x)
    \hfill \text{\cite{dnn-deep-learning-ian}}
\]

\section{Information Theory}

\begin{enumerate}[itemsep=0.2cm]
    \item We would like to quantify information in a way that formalizes this intuition.\\
    Specifically,
    \begin{enumerate}
        \item Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever.

        \item Less likely events should have higher information content.

        \item Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once.
    \end{enumerate}

\subsection{self-information ($I(x)$)} \label{probability: self-information}
    
    In order to satisfy all three of these properties, we define the \textbf{self-information} of an event x$=x$ to be
    \[
        I(x) 
        = -\log_e(P(x))
        \hfill \text{(units = nats)}
        \hfill \text{\cite{dnn-deep-learning-ian}}
    \]

    \item One \textbf{nat} is the amount of information gained by observing an event of probability $\dfrac{1}{e}$

    \item If base = 2, unit is called \textbf{bits} or \textbf{shannons}.

    \item When x is continuous, we use the same definition of information by analogy, but some of the properties from the discrete case are lost.\\
    \textbf{For example}, an event with unit density still has zero information, despite not being an event that is guaranteed to occur.

    \item Self-information deals only with a single outcome.

\subsection{Shannon entropy/ differential entropy ($H(x)$)} \label{probability: Shannon entropy/ differential entropy}

    \item We can quantify the amount of uncertainty in an entire probability distribution using the \textbf{Shannon entropy}
    \[
        H(x) = H(P)
        = \mathbb{E}_{x\sim P}[I(x)]
        = -\mathbb{E}_{x\sim P}[\log(P(x))]
        \hfill \text{\cite{dnn-deep-learning-ian}}
    \]

    \item  the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution.\\
    It gives a lower bound on the number of bits (if the logarithm is base 2, otherwise the units are different) needed on average to encode symbols drawn from a distribution P.\\
    Distributions that are nearly deterministic (where the outcome is nearly certain) have low entropy; distributions that are closer to uniform have high entropy.\\ 
    When x is \textit{continuous}, the Shannon entropy is known as the \textbf{differential entropy}.

\subsection{Kullback-Leibler (KL) divergence ($D_{KL}(P||Q)$)}\label{probability: Kullback-Leibler (KL) divergence}

    \item If we have two separate probability distributions $P(x)$ and $Q(x)$ over the same random variable $x$, we can measure how different these two distributions are using the \textbf{Kullback-Leibler (KL) divergence}:
    \[
        D_{KL}(P||Q)
        = \mathbb{E}_{x\sim P} \dSquareBrac{\log\dParenBrac{\dfrac{P(x)}{Q(x)}}}
        = \mathbb{E}_{x\sim P} \dSquareBrac{\log(P(x)) - \log(Q(x))}
        \hfill \text{\cite{dnn-deep-learning-ian}}
    \]

    \item In the case of discrete variables, it is the extra amount of information (measured in bits if we use the base 2 logarithm, but in machine learning we usually use nats and the natural logarithm) needed to send a message containing symbols drawn from probability distribution $P$, when we use a code that was designed to minimize the length of messages drawn from probability distribution $Q$.

    \item it is non-negative.\\
    The KL divergence is $0$ if and only if $P$ and $Q$ are the same distribution in the case of discrete variables, or equal “almost everywhere” in the case of continuous variables.\\ 
    Because the KL divergence is non-negative and measures the difference between two distributions, it is often conceptualized as measuring some sort of distance between these distributions.

    \item it is not a true distance measure because it is not symmetric: $D_{KL}(P||Q) \neq D_{KL}(Q||P)$ for some $P$ and $Q$.

    \item A quantity that is closely related to the KL divergence is the cross-entropy $H(P,Q) = H(P ) + D_{KL}(P||Q)$, which is similar to the KL divergence but lacking the term on the left:
    \[
        H(P,Q)
        = -\mathbb{E}_{x\sim P} \dSquareBrac{\log(Q(x))}
        \hfill \text{\cite{dnn-deep-learning-ian}}
    \]
    Minimizing the cross-entropy with respect to Q is equivalent to minimizing the KL divergence, because $Q$ does not participate in the omitted term.
    
\end{enumerate}


\section{Structured Probabilistic Models/ Graphical Models \cite{dnn-deep-learning-ian}} \label{Structured Probabilistic Models/ Graphical Models}

\begin{enumerate}
    \item Using a single function to describe the entire joint probability distribution can be very inefficient (both computationally and statistically).

    \item Instead of using a single function to represent a probability distribution, we can split a probability distribution into many factors that we multiply together.

    \item These factorizations can greatly reduce the number of parameters needed to describe the distribution.\\
    Each factor uses a number of parameters that is exponential in the number of variables in the factor.\\
    This means that we can greatly reduce the cost of representing a distribution if we are able to find a factorization into distributions over fewer variables.\\
    We can describe these kinds of factorizations using graphs. \\
    Here we use the word “graph” in the sense of graph theory: a set of vertices that may be connected to each other with edges.

    \item \textbf{For example}: suppose we have three random variables: a, b and c.\\
    Suppose that a influences the value of b and b influences the value of c, but that a and c are independent given b.\\
    We can represent the probability distribution over all three variables as a product of probability distributions over two variables:
    \[
        p(a,b,c) = p(a) p(b|a) p(c| b)
    \]

    \item There are two main kinds of structured probabilistic models: \textbf{directed} and \textbf{undirected}.\\
    Both kinds of graphical models use a graph $\mathcal{G}$ in which each node in the graph corresponds to a random variable, and an edge connecting two random variables means that the probability distribution is able to represent direct interactions between those two random variables.\\
    These graphical representations of factorizations are a language for describing probability distributions.\\
    They are not mutually exclusive families of probability distributions.\\
    Being directed or undirected is not a property of a probability distribution; it is a property of a particular \textbf{description} of a probability distribution, but any probability distribution may be described in both ways.
\end{enumerate}

\subsection{Directed Models \cite{dnn-deep-learning-ian}}

\begin{enumerate}
    \item Directed models use graphs with directed edges, and they represent factorizations into conditional probability distributions, as in the example above.

    \item Specifically, a directed model contains one factor for every random variable $x_i$ in the distribution, and that factor consists of the conditional distribution over $x_i$ given the parents of $x_i$, denoted $P_{a\mathcal{G}}(x_i)$:
    \[
        p(x)
        = \dprod_i p(x_i | P_{a\mathcal{G}}(x_i))
    \]
\end{enumerate}


\subsection{Undirected Models \cite{dnn-deep-learning-ian}}

\begin{enumerate}
    \item Undirected models use graphs with undirected edges, and they represent factorizations into a set of functions; unlike in the directed case, these functions are usually not probability distributions of any kind. 
    
    \item Any set of nodes that are all connected to each other in $\mathcal{G}$ is called a clique. Each clique $\mathcal{C}^{(i)}$ in an undirected model is associated with a factor $\phi^{(i)} (\mathcal{C}^{(i)} )$. 
    
    \item These factors are just functions, not probability distributions. 
    
    \item The output of each factor must be non-negative, but there is no constraint that the factor must sum or integrate to $1$ like a probability distribution.

    \item The probability of a configuration of random variables is proportional to the product of all of these factors—assignments that result in larger factor values are more likely.\\
    Of course, there is no guarantee that this product will sum to $1$.\\
    We therefore divide by a normalizing constant $Z$, defined to be the sum or integral over all states of the product of the $\phi$ functions, in order to obtain a normalized probability distribution:
    \[
        p(x)
        = \dfrac{1}{Z} \dprod_i \phi^{(i)} \dParenBrac{\mathcal{C}^{(i)}}
    \]

    \item 
\end{enumerate}






















































