\chapter{Statistics: Sampling Plans \cite{ism-1}}

\section*{Intro \cite{ism-1}}

\begin{alternateColorTable}
\renewcommand{\arraystretch}{1.3}
\begin{longtable}{l l}
    $S_k = \dCurlyBrac{i_1,i_2,\cdots,i_n}$ & Sample \\

    $i_h \in \dCurlyBrac{1, 2, 3,\cdots, N}$ & index of unit (wrt population) \\

    $\Omega = \dCurlyBrac{1, 2, 3, \cdots, N}$ & units of population \\

    $\pi_k$ & Probability of choosing sample $S_k$ from population\\

    $p_k$ & Probability of choosing unit $i_k$ from population\\
\end{longtable}
\renewcommand{\arraystretch}{1}
\end{alternateColorTable}

A formal or mathematical definition for collecting a random sample of size $n$ from a population of units indicated by $\Omega = \dCurlyBrac{1, 2, 3, \cdots, N}$, $N \geq n$, can be described as follows:

\begin{enumerate}
    \item $S_k = \dCurlyBrac{i_1,i_2,\cdots,i_n}$  where $i_h \in \dCurlyBrac{1, 2, 3,\cdots, N}$ and all indices unique ($i_h \neq i_l$ when $h \neq l$)

    \item Let $S_1, S_2, \cdots, S_K$ be subsets of the population $\Omega$, $S_k \subset \Omega$, $k = 1, 2,\cdots, K$, such that each subset $S_k$ has $n$ \textbf{unique units} from $\Omega$ and the union of all units from $S_1, S_2,\cdots, S_K$ forms the whole population $\Omega$, i.e., ${\displaystyle \Omega = \bigcup_{k=1}^{K} S_k}$
    
    \item each subset $S_k$ is attached a probability $\pi_k$ such that $\pi_k > 0$, for all $k = 1, 2,\cdots, K$, and ${\displaystyle \sum_{k=1}^{K} \pi_k = 1}$

    \item A random sample of size $n$ is obtained by drawing just one number from $1, 2, 3,\cdots, K$ using the probabilities $\pi_1, \pi_2, \pi_3,\cdots,\pi_K$

    \item The set of samples $S_1, S_2,\cdots, S_K$ with their probabilities $pi_1, pi_2, pi_3,\cdots ,pi_K$ is referred to as a \textbf{sampling plan}.

    \item Subsets $S_1, S_2, \cdots, S_K$ can be assumed to be unique, $S_k \neq S_l$ when $k \neq l$, since otherwise we can create a unique set by adding the probabilities for the subsets that are equal.

    \item This does not mean that there is no overlap in units from different subsets, i.e., we do \textbf{NOT} require $S_k \cap S_l = \varnothing$.

    \item sets $S_1, S_2,\cdots, S_K$ and the probabilities $\pi_1, \pi_2, \pi_3,\cdots,\pi_K$ result in a set of probabilities $p_1, p_2, p_3,\cdots, p_N$ for units $1, 2, 3,\cdots, N$ in the population $\Omega$, with $p_i > 0$

    \item With every sample $S_k$ we have observed a vector of observations $\hat{x}_k^\top = (x_{i_1},\cdots,x_{i_n})$ where  $^\top$ indicates transpose

\end{enumerate}


\section{Descriptive statistic ($\hat{\theta}_k = T(x_k)$) \cite{ism-1}}\label{Descriptive statistic}

\begin{enumerate}
    \item it is used as an estimate for the population parameter $\theta$, with $T$ a function applied to the observed data

    \item In many cases the function $T$ is identical to the calculation $\theta$ at the population level, but alternative functions may be used depending on the sampling plan.

    \item The function $T$ is referred to as the estimator.

    \item Expected population parameter: $\mathbb{E}(T) = \dsum_{k=1}^{K} \hat{\theta}_k\pi_k$
\end{enumerate}


\section{
Bias (${
    bias = ( \sum_{k=1}^{K} \hat{\theta}_k\pi_k )
     - \theta = \mathbb{E}(T) - \theta
}$) 
\cite{ism-1}}\label{sampling plans: Bias}

\begin{enumerate}
    \item the bias is the \textbf{difference} between the weighted average of the sample estimate $\hat{\theta}_k$’s and the true population parameter $\theta$.

    \item The bias of an estimator is the difference between this estimator’s expected value and the true population value.

    \item A small bias of an estimator under a sampling plan does not guarantee that individual sample results $\hat{\theta}_k$ are actually close to the population parameter $\theta$; it just states that they are close on average, if we were to sample over and over again.

    \item if the bias is small, $\mathbb{E}(T)$ is close to the parameter value $\theta$.

    
\end{enumerate}

\subsection{Unbiased estimator} \label{Unbiased estimator}

If the bias of an estimator is \textbf{zero}, this means that, if we repeatedly take samples using our sampling plan and repeatedly compute our statistic of interest, the average over all of those statistics is equal to the true population parameter.

\section{Mean Squared Error (MSE) \cite{ism-1}}
\[
    \hfill
    MSE = \dsum_{k=1}^{K}
    (\hat{\theta}_k - \theta)^2 \pi_k
    \hfill
    MSE = SE^2 + (\mathbb{E}(T) - \theta)^2
    \hfill
\]

\begin{enumerate}
    \item To capture the variability in the sample results  $\hat{\theta}_k$ ($k = 1\cdots K$) with respect to the true value $\theta$, we use the so-called mean squared error (MSE).

    \item The MSE measures the weighted average squared distance of the sample results $\hat{\theta}_k$ ($k = 1\cdots K$) from the population parameter $\theta$.

    \item The weights are again determined by the sampling probabilities. 

    \item Often, the smaller the MSE the better the sampling plan.

    \item If the MSE is small, the variability of the $\hat{\theta}_k$’s around $\theta$ is small, while if the MSE is large, the variability around $\theta$ is large.

    \item if the sampling plan is unbiased and thus $\mathbb{E}(T) = \theta$, the RMSE and the SE are identical.

\end{enumerate}


\section{Root Mean Squared Error (RMSE) \cite{ism-1}}
\[
    \hfill
    RMSE = \sqrt{MSE}
    \hfill
    RMSE = \sqrt{SE^2 + (\mathbb{E}(T) - \theta)}
    \hfill
\]

\begin{enumerate}
    \item Often, the smaller the RMSE the better the sampling plan.

    \item RMSE is \textbf{NEVER} smaller than the SE
\end{enumerate}


\section{Standard Error (SE)}

\[
    \hfill
    SE = \sqrt{
        \dsum_{k=1}^{K}
        (\hat{\theta}_k - \mathbb{E}(T))^2\pi_k
    }
    \hspace{1cm}
    SE = \sqrt{RMSE^2 - (\mathbb{E}(T) - \theta)^2}
    = \sqrt{MSE - (\mathbb{E}(T) - \theta)^2}
    \hfill
\]

\begin{enumerate}
    \item It represents the variability of the sampling plan with respect to the expected population parameter $E(T)$ instead of using the true population parameter $\theta$.

    \item Note that the standard error of an estimator is used as a measure to represent our uncertainty regarding an estimate.

    \item If the SE is small, the variability of the $\hat{\theta}_k$’s around $E(T)$ is small.

    \item if the sampling plan is unbiased and thus $E(T) = \theta$, the RMSE and the SE are identical.

\end{enumerate}

\section{Estimation of the Population Proportion ($\eta$) \cite{ism-1}}\label{Estimation of the Population Proportion}

\begin{enumerate}
    \item Population proportions are very much similar to population means

    \item The population values $z_1, z_2,\cdots,z_N$ are now represented by binary values, i.e., $z_i \in \dCurlyBrac{0, 1}$, and the population proportion $\eta$ is obtained by the fraction of units with the value $1$.
\end{enumerate}

\begin{alternateColorTable}
\begin{table}[H]
    \begin{tabular}{|p{3cm}|p{6cm}|p{6cm}|}
        \hline
        \tableHeaderRow
        & \textbf{Population} & \textbf{Sample} \\
        \hline
        
        \textbf{Mean/ Average} & $
            \eta = \dfrac{1}{N} \dsum_{i=1}^N z_i
        $ & $
            \hat{\eta}_k = \dfrac{1}{n} 
            \dsum_{i\in S_k} z_i
        $\\[2ex]
        \hline

        \textbf{Variance} & 
        \begin{minipage}{\linewidth}
            \vspace{0.1cm}
            \[
                \sigma^2 = \dfrac{1}{N}
                \dsum_{i=1}^{N} (z_i - \eta)^2
                = \dfrac{1}{N}
                \dsum_{i=1}^{N} z_i^2 - \eta^2
            \]
            \[
                \sigma^2 = \dfrac{1}{N}
                \dsum_{i=1}^{N} (x_i - \mu)^2
            \]
            \vspace{0.1cm}
        \end{minipage} & 
        \begin{minipage}{\linewidth}
            \vspace{0.1cm}
            \begin{enumerate}
                \item \textbf{Unbiased}
                \[
                    s_k^2 = \dfrac{1}{n-1}
                    \dsum_{i\in S_k}
                    (z_i - \hat{\eta}_k)^2
                    = \dfrac{n\hat{\eta}_k(1-\hat{\eta}_k)}{n-1}
                \]

                \item \textbf{Biased}
                \[
                    s_k^2 = \dfrac{1}{n}
                    \dsum_{i\in S_k}
                    (x_i - \hat{x}_k)^2
                \]
            \end{enumerate}
            \vspace{0.1cm}
        \end{minipage} \\
        \hline
    \end{tabular}
\end{table}
\end{alternateColorTable}

\section{Estimation of the Population Variance \cite{ism-1}} \label{Estimation of the Population Variance}

\begin{alternateColorTable}
\begin{longtable}{|p{2cm}|p{12cm}|}
    \hline

    \textbf{Sample Variance} & \vspace{0.01cm} $
        s_k^2 = \dfrac{1}{n-1}
        \dsum_{i\in S_k} (x_i \bar{x}_k)^2
    $ \vspace{0.1cm} \\
    \hline

    \textbf{Sample Estimator} & \vspace{0.01cm} $
        \dfrac{N-1}{Nn} \dsum_{i\in S_k}
        (x_i \bar{x}_k)^2
        = \dfrac{(N-1)s_k^2}{N}
    $ \vspace{0.1cm}\\
    \hline

    \textbf{Expected Value} & \vspace{0.01cm} $
        \dfrac{(n-1)\sigma^2}{n}
    $ \vspace{0.1cm} \\
    \hline

    \textbf{Bias} & \vspace{0.01cm} $
        -\sigma^2/n
    $ \vspace{0.1cm} \\
    \hline

    \textbf{Degrees of Freedom (DOF)} & $n-1$\\
    \hline

    \textbf{MSE} & \vspace{0.1cm} $
        MSE\dParenBrac{ \dfrac{Ns_k^2}{N-1} }
        = \sigma^4 \left[ 
            \dfrac{
                (N-n)(N-1)(Nn-N-n-1)
            }{
                N(N-2)(N-3)n(n-1)
            }\gamma_2
            + \dfrac{2(N-n)}{(N-2)(n-1)}
        \right]
    $ \vspace{0.1cm} \\
    \hline

    \textbf{Population excess kurtosis} & $
        \gamma_2 = \dfrac{1}{N\sigma^4}
        \dsum_{i=1}^{N} (x_i-\mu)^4 - 3
    $\\
    \hline
\end{longtable}
\end{alternateColorTable}

\begin{enumerate}
    \item MSE of the estimator $\dfrac{(N-1)s_k^2}{N}$ is close to $\dfrac{2\sigma^4}{n-1}$

    \item when the population size is large, the MSE is approximately $\sigma^4\dParenBrac{ \dfrac{\gamma_2}{n} + \dfrac{2}{n-1} }$
\end{enumerate}


\section{Estimation of the MSE \cite{ism-1}} \label{Estimation of the MSE}

\begin{longtable}{|p{3cm}|p{11cm}|}
    \hline

    \textbf{estimator of the squared variance $\sigma^4$} & 
    \[
        \dfrac{(N-1)^2 s_k^4}{N^2}
    \] \\
    \hline

    \textbf{estimator of the precision of the variance estimator} & \[
        \dfrac{(N-1)s_k^2}{N}
    \]\\
    \hline

    \textbf{estimator of the MSE} & \[
        \dfrac{(N-1)^2 s_k^4}{N^2}
        \left[ 
            \dfrac{
                (N-n)(N-1)(Nn-N-n-1)
            }{
                N(N-2)(N-3)n(n-1)
            }g_2
            + \dfrac{2(N-n)}{(N-2)(n-1)}
        \right]
    \]\\
    \hline

    \textbf{in case we deal with large populations} & \[
        s_k^4\left( 
            \dfrac{g_2}{n} + \dfrac{2}{n-1} 
        \right)
    \]\\
    \hline
\end{longtable}

\section{Methods of Estimation \cite{ism-1}} \label{Methods of Estimation}

\begin{enumerate}
    \item estimators obtained using either MME or MLE are themselves sample statics ($T_n$) and we can study their distribution functions
\end{enumerate}

\subsection{Method of Moments Estimation (MME)} \label{Method of Moments Estimation (MME)}

\begin{table}[H]
    \centering
    \begin{tabular}{l l}
        $f_\theta$ & population density \\

        $\theta$ & set of parameters ($\theta = (\theta_1, \theta_2,\cdots,\theta_m)^\top$)\\

        $f_\theta(k)$ & probability that $X$ is equal to $k$\\
    \end{tabular}
\end{table}

\begin{enumerate}
    \item central moments:
    \begin{enumerate}
        \item Continuous: 
        \[
            \mu _r ( f_\theta  ) 
            = E(X - \mu ( f_\theta  ))^r 
            = \int_\mathbb{R} (x - \mu ( f_\theta  ))^r f_\theta  (x) dx
        \]

        \item Discrete: 
        \[
            \mu _r ( f_\theta  ) 
            = E(X - \mu ( f_\theta  ))^r 
            = \sum_{k=0}^{\theta} (k - \mu ( f_\theta  ))^r f_\theta  (k)
        \]
    \end{enumerate}

    
\end{enumerate}









































