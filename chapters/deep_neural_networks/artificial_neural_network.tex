\chapter{Artificial Neural Network (ANN)}

\section{Perceptron \cite{wiki-perceptron}}\label{perceptron}

\begin{figure}[H]
    \centering
    \includegraphics[height=3cm]{Pictures/deep_neural_networks/perceptron.jpg}
    \caption{Perceptron}
\end{figure}

In machine learning, the perceptron (or McCullochâ€“Pitts neuron) is an algorithm for supervised learning of binary classifiers. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.


\section{Artificial neuron \cite{wiki-Artificial_neuron}}\label{Artificial neuron}
\begin{table}[H]
    \begin{minipage}{0.45\textwidth}
        \begin{figure}[H]
            \centering
            \includegraphics[height=4cm]{Pictures/deep_neural_networks/bio_neuron.jpg}
            \caption{ANN: Biological Neuron}
        \end{figure}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \begin{figure}[H]
            \centering
            \includegraphics[height=4cm]{Pictures/deep_neural_networks/Artificial_neuron_structure.jpg}
            \caption{ANN: Artificial Neuron}
        \end{figure}
    \end{minipage}
\end{table}


An artificial neuron is a mathematical function conceived as a model of biological neurons in a neural network. Artificial neurons are the elementary units of artificial neural networks. The artificial neuron is a function that receives one or more inputs, applies weights to these inputs, and sums them to produce an output.


Usually, each input is separately weighted, and the sum is often added to a term known as a bias (loosely corresponding to the threshold potential), before being passed through a non-linear function known as an \textbf{activation function} or \textbf{transfer function}.

\[
     y_{k}=\varphi \left(\sum _{j=0}^{m}w_{kj}x_{j}\right)
\]


SEE: \fullref{Perceptron vs Artificial Neuron}



\section{Activation Function \cite{wiki-Artificial_neuron, wiki-activation-fn}}
The activation function of a node in an artificial neural network is a function that calculates the output of the node based on its individual inputs and their weights. Nontrivial problems can be solved using only a few nodes if the activation function is nonlinear


\begin{enumerate}
    \item  The transfer functions usually have a \textbf{sigmoid} shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions.
    \item They are also often monotonically increasing, continuous, differentiable and bounded. 
    \item  Non-monotonic, unbounded and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU-like activation functions on many tasks have also been recently explored
\end{enumerate}

See \ref{chapter: Activation Functions} for more details.


\section{Connectionism model \cite{wiki-Connectionism}}\label{Connectionism model}
Connectionism (coined by Edward Thorndike in the 1931) is the name of an approach to the study of human mental processes and cognition that utilizes mathematical models known as connectionist networks or artificial neural networks. Connectionism has had many 'waves' since its beginnings.

\section{Artificial Neural Network (ANN) \cite{wiki-ann}}\label{ann}
\begin{figure}[H]
    \centering
    \includegraphics[height=4cm]{Pictures/deep_neural_networks/ann.png}
    \caption{Artificial Neural Network}
\end{figure}
A neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains. An ANN consists of connected units or nodes called \textbf{artificial neurons}, which loosely model the neurons in a brain. These are connected by edges, which model the synapses in a brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The "signal" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the \textbf{activation function}. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.

SEE: \fullref{connectionism vs ann}

\begin{enumerate}
    \item ANN is subset of Connectionism
\end{enumerate}


\section{Perceptron Learning Algorithm \cite{medium-perceptron-learning-algorithm}}\label{Perceptron Learning Algorithm}

\begin{enumerate}
    \item Goal is to find the w vector that can perfectly classify positive inputs and negative inputs in the data.
\end{enumerate}

\begin{algorithm}[H]
    \caption{Perceptron Learning Algorithm}
    $P \gets$ inputs with label 1\;
    $N \gets$ inputs with label 0\;
    Initialize $\textbf{w}$ randomly\;
    \While{!convergence}{
        Pick random $x \in P \cup N$\;
        \If{$x \in P$ and $w \cdot x < 0$}{
            $w = w + x$\;
        }
        \If{$x \in N$ and $w \cdot x \ge 0$}{
            $w = w - x$\;
        }
    }
\end{algorithm}

Examples:\\
\begin{enumerate}
    \item \url{https://www.geeksforgeeks.org/implementation-of-perceptron-algorithm-for-or-logic-gate-with-2-bit-binary-input/}
    \item \url{https://www.geeksforgeeks.org/implementation-of-perceptron-algorithm-for-and-logic-gate-with-2-bit-binary-input/}
    \item \url{https://www.geeksforgeeks.org/implementation-of-perceptron-algorithm-for-not-logic-gate/}
    \item \url{https://turcomat.org/index.php/turkbilmat/article/view/7786}
\end{enumerate}



\section{Multilayer Perceptron (MLP) \cite{wiki-Multilayer_perceptron}}\label{Multilayer_perceptron}
A multilayer perceptron (MLP) is a name for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear activation function, organized in at least three layers, notable for being able to distinguish data that is not linearly separable. \\
Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the "vanilla" neural networks.\\
MLPs grew out of an effort to improve single-layer perceptrons, which could only distinguish linearly separable data. A perceptron traditionally used a Heaviside step function as its nonlinear activation function. However, the backpropagation algorithm requires that modern MLPs use continuous activation functions such as sigmoid or ReLU.

SEE: \fullref{mlp vs ann}

\begin{enumerate}
    \item MLP is a specific type of ANN, meaning all MLPs are ANNs, but not all ANNs are MLPs.
\end{enumerate}


\section{Width \& Depth of ANN/ MLP \cite{arxiv-2010.15327}}




























































































