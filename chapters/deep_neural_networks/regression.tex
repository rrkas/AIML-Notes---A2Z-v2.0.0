\chapter{Regression}

\section*{Notations}

\begin{alternateColorTable}
\renewcommand{\arraystretch}{1.3}
\begin{table}[H]
    \begin{tabular}{l l}
        $\mathbf{x}$ & input vector (known) \\

        $X$ & input matrix/ design matrix (known) \\
        
        $y$ & actual output (known) \\

        $w$ & weights (unknown) \\

        $b$ & bias (unknown) \\

        $\hat{y}$ & predicted output/ predictor (generated) \\

        $\hat{w}$ & current weights \\

        $\hat{b}$ & current bias \\

        $w^\ast$ & optimal weights \\

        $b^\ast$ & optimal bias \\
    \end{tabular}
\end{table}
\renewcommand{\arraystretch}{1}
\end{alternateColorTable}

\section{Linear Regression \cite{dnn-1}} \label{Linear Regression}

\subsection{Model/ predictor \cite{dnn-1}}
\[
    \hfill
    \hat{y} = w_1x_1 + \cdots + w_dx_d + b
    = w^\top x + b
    \hfill
    (
        \hat{y} \in \mathbb{R},
        x \in \mathbb{R}^d ,
        w \in \mathbb{R}^d,
        b \in \mathbb{R}
    )
\]
    

\noindent
For multi-dimensional input $X$:
\[
    \hfill
    \hat{y} = Xw + b
    \hfill
    (
        \hat{y} \in \mathbb{R}^n,
        X \in \mathbb{R}^{n\times d}, 
        w \in \mathbb{R}^d,
        b \in \mathbb{R}^n
    )
\]

\subsection{Loss Function \cite{dnn-1}}
SEE: \fullref{Squared Error}

\[
    {\displaystyle L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2}
\]

When training the model, we want to find parameters $(w^\ast, b^\ast)$ that minimize the total loss
across all training examples:
\[
    \displaystyle
    \hfill
        w^\ast, b^\ast = \arg\max_{w,b} L(w,b)
    \hfill
\]

\subsection{Analytic Solution \cite{dnn-1}}


\[
    w^\ast = (X^\top X)^{-1}X^\top y
\]

\begin{enumerate}
    \item we can subsume the bias $b$ into the parameter $w$ by appending a column to the design matrix consisting of \textbf{all ones}.

    \item $w^\ast$ will only be unique when the matrix $X^\top X$ is invertible, i.e., when the columns of the design matrix are linearly independent
\end{enumerate}


\subsection{Predictions/ inference \cite{dnn-1}}
\[
    \hat{y} = \hat{w}^\top x + \hat{b}
\]

\subsection{The Normal Distribution and Squared Loss \cite{dnn-1}}

\[
    y = \mathbf{w}^\top \mathbf{x} + b + \epsilon 
    \textrm{ where } 
    \epsilon \sim \mathcal{N}(0, \sigma^2).
\]

likelihood of seeing a particular $y$ for a given $x$ via:
\[
    \displaystyle
    P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right)
\]

As such, the likelihood factorizes. According to the principle of maximum likelihood, the best values of parameters $w$ and $b$ are those that maximize the likelihood of the entire dataset:
\[
    \displaystyle
    P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)} \mid \mathbf{x}^{(i)})
\]

without changing anything, we can minimize the negative log-likelihood, which we can express as follows:
\[
    \displaystyle
    -\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.
\]





































