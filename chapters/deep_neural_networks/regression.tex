\chapter{Regression}

\section*{Notations \cite{dnn-1}}

\begin{table}[H]
    \begin{minipage}[t]{0.49\linewidth}        
        \begin{customTableWrapper}{1.3}
        \begin{table}[H]
            \begin{tabular}{|l l|}
                \hline
        
                $n$ & number of examples \\
        
                \hline
        
                $\mathbf{x}$ & input vector (known) \\
                $X$ & input matrix/ design matrix (known) \\

                \hline
        
                $y$ & actual output (known) \\
                $\hat{y}$ & predicted output/ predictor (generated) \\
        
                \hline
        
            \end{tabular}
        \end{table}
        \end{customTableWrapper}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.49\linewidth}
        \begin{customTableWrapper}{1.3}
        \begin{table}[H]
            \begin{tabular}{|l l|}
                \hline
        
                $w$ & weights (unknown) \\
                $\hat{w}$ & current weights \\
                $w^\ast$ & optimal weights \\
        
                \hline
        
                $b$ & bias (unknown) \\
                $\hat{b}$ & current bias \\
                $b^\ast$ & optimal bias \\
        
                \hline

            \end{tabular}
        \end{table}
        \end{customTableWrapper}
    \end{minipage}
\end{table}


\section{Linear Regression \cite{dnn-1}} \label{Linear Regression}

\begin{enumerate}[itemsep=0.15cm]
    \item we \textbf{assume} that the relationship between features $x$ and target $y$ is approximately linear, i.e., that the conditional mean $\mathbb{E}[Y | X = x]$ can be expressed as a \textbf{weighted sum of the features} $x$.

    \item target value may still deviate from its expected value on account of observation noise. we can impose the assumption that any such noise is \textbf{well-behaved}, following a \textbf{Gaussian distribution}.

\end{enumerate}

\subsection*{Model/ predictor \cite{dnn-1}}
\[
    \hfill
    \hat{y} = w_1x_1 + \cdots + w_dx_d + b
    = w^\top x + b
    \hfill
    (
        \hat{y} \in \mathbb{R},
        x \in \mathbb{R}^d ,
        w \in \mathbb{R}^d,
        b \in \mathbb{R}
    )
\]
    

\noindent
For multi-dimensional input $X$:
\[
    \hfill
    \hat{y} = Xw + b
    \hfill
    (
        \hat{y} \in \mathbb{R}^n,
        X \in \mathbb{R}^{n\times d}, 
        w \in \mathbb{R}^d,
        b \in \mathbb{R}^n
    )
\]
\[
    \hfill
    y^{(i)} = w^\top x^{(i)} + b
    \hfill
    (i \in \dCurlyBrac{1,\cdots,n})
\]

\begin{enumerate}[itemsep=0.2cm]
    \item The \textbf{weights} determine the influence of each feature on our prediction. 
    
    \item The \textbf{bias} determines the value of the estimate when all features are zero.\\
    We still need the bias because it allows us to express all linear functions of our features (versus restricting us to lines that pass through the origin).

    \item It is an \textbf{affine transformation} of input features, which is characterized by a linear transformation of features via weighted sum, combined with a \textbf{translation} via the added bias. 

    \item Our \textbf{goal} is to choose the \textbf{weights} $w$ and the \textbf{bias} $b$ that, on average, make our model’s predictions \textbf{fit} the true prices observed in the data as closely as possible.

\end{enumerate}




\subsection*{Loss Function \cite{dnn-1}}
SEE: \fullref{Squared Error}

\[
    l^{(i)}(w,b) = \frac{1}{2}\left( \hat{y}^{(i)} - y^{(i)}\right)^2
    \hfill
    {\displaystyle L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2}
\]

When training the model, we want to find parameters $(w^\ast, b^\ast)$ that minimize the total loss
across all training examples:
\[
    \displaystyle
    \hfill
        w^\ast, b^\ast = \arg\max_{w,b} L(w,b)
    \hfill
\]


\begin{enumerate}[itemsep=0.2cm]
    \item Loss functions quantify the \textbf{distance} between the real and predicted values of the target.

    \item The loss will usually be a non-negative number where \textbf{smaller} values are \textbf{better} and perfect predictions incur a loss of $0$.

\end{enumerate}





\subsection*{Analytic Solution \cite{dnn-1}}

\[
    \hfill
    \partial\dnorm{y - Xw}^2 = 2X^\top (Xw - y) = 0
    \hspace{0.2cm}
    \Rightarrow
    \hspace{0.2cm}
    X^\top y = X^\top Xw
    \hfill
\]
\[
    \hfill
    w^\ast = (X^\top X)^{-1}X^\top y
    \hfill
\]

\begin{enumerate}
    \item we can subsume the bias $b$ into the parameter $w$ by appending a column to the design matrix consisting of \textbf{all ones}.

    \item $w^\ast$ will only be unique when the matrix $X^\top X$ is invertible, i.e., when the columns of the design matrix are linearly independent

    \item Although analytic solutions allow for nice mathematical analysis, the requirement of an analytic solution is so restrictive that it would exclude almost all exciting aspects of deep learning.

\end{enumerate}




\subsection*{Optimizer \cite{dnn-1}}
SEE: 
\begin{enumerate}
    \item \fullref{Minibatch Stochastic Gradient Descent (Minibatch SGD)}

    \item quadratic losses: \fullref{Squared Error}
\end{enumerate}

\vspace{0.5cm}
\noindent
For \textbf{quadratic losses} and affine transformations, this has a closed-form expansion:\\[1ex]
$\begin{aligned}
    &w \leftarrow 
    w - \dfrac{\eta}{\dabs{B}} \dsum_{i \in B_t} 
        \partial_{w} l^{(i)}(w,b)
    = w - \dfrac{\eta}{\dabs{B}} \dsum_{i \in B_t} 
        x^{(i)} \dParenBrac{w^\top x^{(i)} + b - y^{(i)}} \\
    &b \leftarrow 
    b - \dfrac{\eta}{\dabs{B}} \dsum_{i \in B_t} 
        \partial_{b} l^{(i)}(w,b)
    = b - \dfrac{\eta}{\dabs{B}} \dsum_{i \in B_t} 
        \dParenBrac{w^\top x^{(i)} + b - y^{(i)}} \\
\end{aligned}$

\vspace{0.5cm}
\begin{enumerate}[itemsep=0.2cm]
    \item Since we pick a minibatch $B$ we need to normalize by its size $\dabs{B}$.

    \item Even if function is truly linear and noiseless, these parameters will \textbf{NOT} be the exact minimizers of the loss, or even deterministic. 
    
    \item Although the algorithm converges slowly towards the minimizers it typically \textbf{CANNOT} achieve it exactly in a finite number of steps. 
    
    \item The minibatches $B$ used to update the parameters are chosen at \textbf{random}. This \textbf{breaks determinism}.

    \item Linear regression happens to be a learning problem with a \textbf{global minimum} (whenever $X$ is \textbf{full rank}, or equivalently, whenever $X^\top X$ is \textbf{invertible}).

    \item we typically do not care about finding an exact set of parameters but merely any set of parameters that leads to accurate predictions (and thus low loss).
\end{enumerate}


\subsection*{Predictions/ inference \cite{dnn-1}}
\[
    \hat{y} = \hat{w}^\top x + \hat{b}
\]


\subsection*{The Normal Distribution and Squared Loss \cite{dnn-1}}

\[
    y = \mathbf{w}^\top \mathbf{x} + b + \epsilon 
    \textrm{ where } 
    \epsilon \sim \mathcal{N}(0, \sigma^2).
\]

likelihood of seeing a particular $y$ for a given $x$ via:
\[
    \displaystyle
    P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right)
\]

As such, the likelihood factorizes. According to the principle of maximum likelihood, the best values of parameters $w$ and $b$ are those that maximize the likelihood of the entire dataset:
\[
    \displaystyle
    P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)} \mid \mathbf{x}^{(i)})
\]

without changing anything, we can minimize the negative log-likelihood, which we can express as follows:
\[
    \displaystyle
    -\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.
\]

\subsection*{Generalization \cite{dnn-1}}

\begin{customTableWrapper}{1.1}
\begin{table}[H]
    \centering
    \begin{tabular}{l l p{6cm}}
        $R_\textrm{emp}$ & training error & statistic calculated on the training dataset \\
        
        $R$ & generalization error & expectation taken with respect to the underlying distribution \\

    \end{tabular}
\end{table}
\end{customTableWrapper}

\[
    \hfill
    R_\textrm{emp}[\mathbf{X}, \mathbf{y}, f] 
    = \frac{1}{n} \sum_{i=1}^n l(\mathbf{x}^{(i)}, y^{(i)}, f(\mathbf{x}^{(i)}))
    \hfill
\]
\[
    \hfill
    R[p, f] 
    = E_{(\mathbf{x}, y) \sim P} [l(\mathbf{x}, y, f(\mathbf{x}))] 
    = \iint l(\mathbf{x}, y, f(\mathbf{x})) p(\mathbf{x}, y) \;d\mathbf{x} dy
    \hfill
\]

\begin{enumerate}[itemsep=0.2cm]
    \item Goal is to truly discovered a general pattern and not simply memorized our data.

    \item In the standard supervised learning setting, we assume that the training data and the test data are drawn independently from identical (i.i.d.) distributions.

    \item we cannot sample an infinite stream of data points. Thus, in practice, we must estimate the generalization error by applying our model to an independent test set constituted of a random selection of examples $\mathbf{X}'$ and labels $\mathbf{y}'$  that were withheld from our training set. This consists of applying the same formula that was used for calculating the empirical training error but to a test set $\mathbf{X}', \mathbf{y}'$.

    
\end{enumerate}















\section{Polynomial Curve Fitting \cite{dnn-1}}\label{Polynomial Curve Fitting}

\begin{enumerate}[itemsep=0.2cm]
    \item given training data consisting of a single feature $x$ and a corresponding real-valued label $y$, we try to find the polynomial of degree $d$
    \[
        \hfill
        \hat{y}= \dsum_{i=0}^d x^i w_i
        \hfill
    \]
    for estimating the label $y$. This is just a linear regression problem where our features are given by the powers of $x$, the model’s weights are given by $w_i$, and the bias is given by $w_0$ since $x^0 = 1$ for all $x$. 

\end{enumerate}















