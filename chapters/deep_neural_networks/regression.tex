\chapter{Regression}

\section*{Notations}

\begin{table}[H]
    \begin{tabular}{l l}
        $\mathbf{x}$ & input vector (known) \\

        $X$ & input matrix/ design matrix (known) \\
        
        $y$ & actual output (known) \\

        $w$ & weights (unknown) \\

        $b$ & bias (unknown) \\

        $\hat{y}$ & predicted output/ predictor (generated) \\

        $\hat{w}$ & current weights \\

        $\hat{b}$ & current bias \\

        $w^\ast$ & optimal weights \\

        $b^\ast$ & optimal bias \\
    \end{tabular}
\end{table}

\section{Linear Regression \cite{dnn-1}} \label{Linear Regression}

\subsection{Model/ predictor}
\[
    \hfill
    \hat{y} = w_1x_1 + \cdots + w_dx_d + b
    = w^\top x + b
    \hfill
    (
        \hat{y} \in \mathbb{R},
        x \in \mathbb{R}^d ,
        w \in \mathbb{R}^d,
        b \in \mathbb{R}
    )
\]
    

\noindent
For multi-dimensional input $X$:
\[
    \hfill
    \hat{y} = Xw + b
    \hfill
    (
        \hat{y} \in \mathbb{R}^n,
        X \in \mathbb{R}^{n\times d}, 
        w \in \mathbb{R}^d,
        b \in \mathbb{R}^n
    )
\]

\subsection{Loss Function}
SEE: \fullref{Squared Error}

\[
    L(w,b) = \dfrac{1}{n}
    \displaystyle\sum_{i=1}^{n} l^{(i)}(w,b)
    = \dfrac{1}{n} \displaystyle\sum_{i=1}^{n}
    \dfrac{1}{2}\left( w^\top x^{(i)} + b - y^{(i)} \right)^2
\]

When training the model, we want to find parameters $(w^\ast, b^\ast)$ that minimize the total loss
across all training examples:
\[
    \displaystyle
    \hfill
        w^\ast, b^\ast = \arg\max_{w,b} L(w,b)
    \hfill
\]

\subsection{Analytic Solution}


\[
    w^\ast = (X^\top X)^{-1}X^\top y
\]

\begin{enumerate}
    \item we can subsume the bias $b$ into the parameter $w$ by appending a column to the design matrix consisting of \textbf{all ones}.

    \item $w^\ast$ will only be unique when the matrix $X^\top X$ is invertible, i.e., when the columns of the design matrix are linearly independent
\end{enumerate}


\subsection{Predictions/ inference}
\[
    \hat{y} = \hat{w}^\top x + \hat{b}
\]















































