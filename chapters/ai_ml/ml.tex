\chapter{Machine Learning (ML)}

\input{chapters/ai_ml/ml-algos/Supervised-Learning}
\input{chapters/ai_ml/ml-algos/Semi-Supervised-Learning}
\input{chapters/ai_ml/ml-algos/Self-Supervised-Learning}
\input{chapters/ai_ml/ml-algos/Unsupervised-Learning}
\input{chapters/ai_ml/ml-algos/reinforcement-learning-rl}


\section{Model Complexity \cite{dnn-1}}
\begin{enumerate}[itemsep=0.2cm]
    \item When we work with more complex models and/or fewer examples, we expect the training error to go down but the generalization gap to grow. \cite{dnn-1}

    \item In general, absent any restriction on our model class, we \textbf{CANNOT} conclude based on fitting the training data alone that our model has discovered any generalizable pattern. \cite{dnn-1}

    \item On the other hand, if our model class was not capable of fitting arbitrary labels, then it must have discovered a pattern. \cite{dnn-1}

    \item According to \textbf{Karl Popper}, a theory that can explain any and all observations is not a scientific theory at all. In short, what we want is a hypothesis that could not explain any observations we might conceivably make and yet nevertheless happens to be compatible with those observations that we in fact make. \cite{dnn-1}

    \item Often, models with more parameters are able to fit a greater number of arbitrarily assigned labels. However, this is not necessarily true. For instance, kernel methods operate in spaces with infinite numbers of parameters, yet their complexity is controlled by other means.

    \item One notion of complexity that often proves useful is the range of values that the parameters can take. A model whose parameters are permitted to take arbitrary values would be more complex.

    \item When a model is capable of fitting arbitrary labels, low training error does not necessarily imply low generalization error. However, it does not necessarily imply high generalization error either!

    \item we want to watch out for cases when our training error and validation error are both substantial but there is a little gap between them. If the model is unable to reduce the training error, that could mean that our model is too simple (i.e., insufficiently expressive) to capture the pattern that we are trying to model. Since the generalization gap $(R_\textrm{emp} - R)$ between our training and generalization errors is small, we have reason to believe that we could get away with a more complex model. This phenomenon is known as \textbf{underfitting}.

    \item When our training error is significantly lower than our validation error, indicating severe \textbf{overfitting}.

    
\end{enumerate}



\section{Model Selection \cite{dnn-1}}\label{Model Selection}

\begin{enumerate}[itemsep=0.2cm]
    \item we select our final model only after evaluating multiple models that differ in various ways (different architectures, training objectives, selected features, data preprocessing, learning rates, etc.)

    \item In principle, we should not touch our test set until after we have chosen all our hyperparameters. Were we to use the test data in the model selection process, there is a risk that we might overfit the test data. Then we would be in serious trouble. If we overfit our training data, there is always the evaluation on test data to keep us honest. But if we overfit the test data, how would we ever know?

    \item The common practice for addressing the problem of \textit{training on the test set} is to split our data three ways, incorporating a \textbf{validation set} in addition to the \textbf{training} and \textbf{test datasets}.

\end{enumerate}


\subsection{Cross-Validation \cite{dnn-1,geeksforgeeks/cross-validation-machine-learning/}}\label{Cross-Validation}

\begin{enumerate}
    \item Cross validation is a technique used in machine learning to evaluate the performance of a model on unseen data. It involves dividing the available data into multiple folds or subsets, using one of these folds as a validation set, and training the model on the remaining folds. This process is repeated multiple times, each time using a different fold as the validation set. Finally, the results from each validation step are averaged to produce a more robust estimate of the model’s performance. \cite{geeksforgeeks/cross-validation-machine-learning/}

    \item The main purpose of cross validation is to prevent overfitting.
\end{enumerate}

\subsubsection{Holdout Validation \cite{geeksforgeeks/cross-validation-machine-learning/}} \label{Cross-Validation: Holdout Validation}

\begin{enumerate}
    \item In Holdout Validation, we perform training on the $50\%$ of the given dataset and rest $50\%$ is used for the testing purpose. 
    
    \item It’s a simple and quick way to evaluate a model. 
    
    \item The major \textbf{drawback} of this method is that we perform training on the $50\%$ of the dataset, it may possible that the remaining $50\%$ of the data contains some important information which we are leaving while training our model i.e. \textbf{higher bias}.

\end{enumerate}


\subsubsection{LOOCV (Leave One Out Cross Validation) \cite{geeksforgeeks/cross-validation-machine-learning/}} \label{Cross-Validation: LOOCV (Leave One Out Cross Validation)}

\begin{enumerate}
    \item In this method, we perform training on the whole dataset but leaves only one data-point of the available dataset and then iterates for each data-point. 
    
    \item In LOOCV, the model is trained on $n-1$ samples and tested on the one omitted sample, repeating this process for each data point in the dataset.

    \item An \textbf{advantage} of using this method is that we make use of all data points and hence it is \textbf{low bias}.

    \item The major \textbf{drawback} of this method is that it leads to \textbf{higher variation} in the testing model as we are testing against one data point. If the data point is an outlier it can lead to higher variation. 
    
    \item Another \textbf{drawback} is it takes a \textbf{lot of execution time} as it iterates over ‘the number of data points’ times.
\end{enumerate}


\subsubsection{Stratified Cross-Validation \cite{geeksforgeeks/cross-validation-machine-learning/}} \label{Cross-Validation: Stratified Cross-Validation}

\begin{enumerate}
    \item It is a technique used in machine learning to ensure that each fold of the cross-validation process maintains the same class distribution as the entire dataset. 
    
    \item This is particularly important when dealing with \textbf{imbalanced datasets}, where certain classes may be underrepresented.

\end{enumerate}

\noindent
\textbf{Steps:}
\begin{enumerate}
    \item The dataset is divided into $k$ folds while maintaining the proportion of classes in each fold.
    
    \item During each iteration, one-fold is used for testing, and the remaining folds are used for training.

    \item The process is repeated $k$ times, with each fold serving as the test set exactly once.

\end{enumerate}



\subsubsection{$K$-Fold Cross Validation \cite{geeksforgeeks/cross-validation-machine-learning/}} \label{Cross-Validation: K-Fold Cross Validation}

\begin{table}[H]
    \begin{minipage}{0.38\linewidth}
        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth, height=5cm, keepaspectratio]{Pictures/ai-ml/grid_search_cross_validation.png}
        \end{figure}
    \end{minipage}
    \hfill
    \begin{minipage}{0.58\linewidth}
        \begin{enumerate}
            \item In $K$-Fold Cross Validation, we split the dataset into $k$ number of subsets (known as folds) then we perform training on the all the subsets but leave one $(k-1)$ subset for the evaluation of the trained model. 
            
            \item  We iterate $k$ times with a different subset reserved for testing purpose each time.
        
        \end{enumerate}
    \end{minipage}
\end{table}




\section*{SEE}

\begin{enumerate}
    \item \fullref{Supervised Learning VS Unsupervised Learning VS Reinforcement Learning}
\end{enumerate}








