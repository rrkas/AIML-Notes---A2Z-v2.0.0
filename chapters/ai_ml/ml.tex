\chapter{Machine Learning (ML)}

\section{A Taxonomy of Learning Problems \cite{dnn-1}} \label{A Taxonomy of Learning Problems}

\begin{enumerate}[itemsep=0.3cm]
    \item \textbf{Batch Learning}
    \begin{enumerate}
        \item In batch learning, we have access to training features and labels $\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}$ which we use to train a model $f(x)$. 
        
        \item Later on, we deploy this model to score new data $(x, y)$ drawn from the same distribution.

    \end{enumerate}

    \item \textbf{Online Learning}
    \begin{enumerate}
        \item the data $(x_i, y_i)$ arrives one sample at a time.

        \item assume that we first observe $x_i$ , then we need to come up with an estimate $f(x_i)$ and only once we have done this, we observe $y_i$ and with it, we receive a reward or incur a loss, given our decision.

        \item in online learning, we have the \textit{following cycle} where we are \textbf{continuously improving} our model given new observations:
        \[
            \textrm{model } f_t 
            \longrightarrow 
            \textrm{data }  \mathbf{x}_t 
            \longrightarrow 
            \textrm{estimate } f_t(\mathbf{x}_t) 
            \longrightarrow
            \textrm{observation } y_t 
            \longrightarrow 
            \textrm{loss } l(y_t, f_t(\mathbf{x}_t)) 
            \longrightarrow 
            \textrm{model } f_{t+1}
        \]

    \end{enumerate}

    \item \textbf{Bandits}
    \begin{enumerate}
        \item Bandits are a special case of Online learning.

        \item While in most learning problems we have a continuously parametrized function f where we want to learn its parameters (e.g., a deep network), in a bandit problem we only have a finite number of arms that we can pull, i.e., a finite number of actions that we can take. 

        \item It is not very surprising that for this simpler problem stronger theoretical guarantees in terms of optimality can be obtained. 

        \item This is often (confusingly) treated as if it were a distinct learning setting.

    \end{enumerate}

    \item \textbf{Control}
    \begin{enumerate}
        \item the environment remembers what we did

        \item Not necessarily in an adversarial manner but it will just remember and the response will depend on what happened before.

        \item PID (proportional-integral-derivative) controller algorithms are a popular choice there.\\
        Likewise, a user’s behavior on a news site will depend on what we showed them previously (e.g., they will read most news only once).

        \item Many such algorithms form a model of the environment in which they act so as to make their decisions appear less random. 

        \item For instance, a coffee boiler controller will observe different temperatures depending on whether it was heating the boiler previously.

    \end{enumerate}

    \item \fullref{Reinforcement Learning}
\end{enumerate}


\input{chapters/ai_ml/ml-algos/Supervised-Learning}
\input{chapters/ai_ml/ml-algos/Semi-Supervised-Learning}
\input{chapters/ai_ml/ml-algos/Self-Supervised-Learning}
\input{chapters/ai_ml/ml-algos/Unsupervised-Learning}
\input{chapters/ai_ml/ml-algos/reinforcement-learning-rl}


\section{Model Complexity \cite{dnn-1}}
\begin{enumerate}[itemsep=0.2cm]
    \item When we work with more complex models and/or fewer examples, we expect the training error to go down but the generalization gap to grow. \cite{dnn-1}

    \item In general, absent any restriction on our model class, we \textbf{CANNOT} conclude based on fitting the training data alone that our model has discovered any generalizable pattern. \cite{dnn-1}

    \item On the other hand, if our model class was not capable of fitting arbitrary labels, then it must have discovered a pattern. \cite{dnn-1}

    \item According to \textbf{Karl Popper}, a theory that can explain any and all observations is not a scientific theory at all. In short, what we want is a hypothesis that could not explain any observations we might conceivably make and yet nevertheless happens to be compatible with those observations that we in fact make. \cite{dnn-1}

    \item Often, models with more parameters are able to fit a greater number of arbitrarily assigned labels. However, this is not necessarily true. For instance, kernel methods operate in spaces with infinite numbers of parameters, yet their complexity is controlled by other means.

    \item One notion of complexity that often proves useful is the range of values that the parameters can take. A model whose parameters are permitted to take arbitrary values would be more complex.

    \item When a model is capable of fitting arbitrary labels, low training error does not necessarily imply low generalization error. However, it does not necessarily imply high generalization error either!

    \item we want to watch out for cases when our training error and validation error are both substantial but there is a little gap between them. If the model is unable to reduce the training error, that could mean that our model is too simple (i.e., insufficiently expressive) to capture the pattern that we are trying to model. Since the generalization gap $(R_\textrm{emp} - R)$ between our training and generalization errors is small, we have reason to believe that we could get away with a more complex model. This phenomenon is known as \textbf{underfitting}.

    \item When our training error is significantly lower than our validation error, indicating severe \textbf{overfitting}.

    
\end{enumerate}



\section{Model Selection \cite{dnn-1}}\label{Model Selection}

\begin{enumerate}[itemsep=0.2cm]
    \item we select our final model only after evaluating multiple models that differ in various ways (different architectures, training objectives, selected features, data preprocessing, learning rates, etc.)

    \item In principle, we should not touch our test set until after we have chosen all our hyperparameters. Were we to use the test data in the model selection process, there is a risk that we might overfit the test data. Then we would be in serious trouble. If we overfit our training data, there is always the evaluation on test data to keep us honest. But if we overfit the test data, how would we ever know?

    \item The common practice for addressing the problem of \textit{training on the test set} is to split our data three ways, incorporating a \textbf{validation set} in addition to the \textbf{training} and \textbf{test datasets}.    
\end{enumerate}


\subsection{Cross-Validation \cite{dnn-1,geeksforgeeks/cross-validation-machine-learning/}}\label{Cross-Validation}

\begin{enumerate}
    \item Cross validation is a technique used in machine learning to evaluate the performance of a model on unseen data. It involves dividing the available data into multiple folds or subsets, using one of these folds as a validation set, and training the model on the remaining folds. This process is repeated multiple times, each time using a different fold as the validation set. Finally, the results from each validation step are averaged to produce a more robust estimate of the model’s performance. \cite{geeksforgeeks/cross-validation-machine-learning/}

    \item The main purpose of cross validation is to prevent overfitting.
\end{enumerate}

\subsubsection{Holdout Validation \cite{geeksforgeeks/cross-validation-machine-learning/}} \label{Cross-Validation: Holdout Validation}

\begin{enumerate}
    \item In Holdout Validation, we perform training on the $50\%$ of the given dataset and rest $50\%$ is used for the testing purpose. 
    
    \item It’s a simple and quick way to evaluate a model. 
    
    \item The major \textbf{drawback} of this method is that we perform training on the $50\%$ of the dataset, it may possible that the remaining $50\%$ of the data contains some important information which we are leaving while training our model i.e. \textbf{higher bias}.

\end{enumerate}


\subsubsection{LOOCV (Leave One Out Cross Validation) \cite{geeksforgeeks/cross-validation-machine-learning/}} \label{Cross-Validation: LOOCV (Leave One Out Cross Validation)}

\begin{enumerate}
    \item In this method, we perform training on the whole dataset but leaves only one data-point of the available dataset and then iterates for each data-point. 
    
    \item In LOOCV, the model is trained on $n-1$ samples and tested on the one omitted sample, repeating this process for each data point in the dataset.

    \item An \textbf{advantage} of using this method is that we make use of all data points and hence it is \textbf{low bias}.

    \item The major \textbf{drawback} of this method is that it leads to \textbf{higher variation} in the testing model as we are testing against one data point. If the data point is an outlier it can lead to higher variation. 
    
    \item Another \textbf{drawback} is it takes a \textbf{lot of execution time} as it iterates over ‘the number of data points’ times.
\end{enumerate}


\subsubsection{Stratified Cross-Validation \cite{geeksforgeeks/cross-validation-machine-learning/}} \label{Cross-Validation: Stratified Cross-Validation}

\begin{enumerate}
    \item It is a technique used in machine learning to ensure that each fold of the cross-validation process maintains the same class distribution as the entire dataset. 
    
    \item This is particularly important when dealing with \textbf{imbalanced datasets}, where certain classes may be underrepresented.

\end{enumerate}

\noindent
\textbf{Steps:}
\begin{enumerate}
    \item The dataset is divided into $k$ folds while maintaining the proportion of classes in each fold.
    
    \item During each iteration, one-fold is used for testing, and the remaining folds are used for training.

    \item The process is repeated $k$ times, with each fold serving as the test set exactly once.

\end{enumerate}



\subsubsection{$K$-Fold Cross Validation \cite{geeksforgeeks/cross-validation-machine-learning/}} \label{Cross-Validation: K-Fold Cross Validation}

\begin{table}[H]
    \begin{minipage}{0.38\linewidth}
        \begin{figure}[H]
            \centering
            \includegraphics[width=\linewidth, height=5cm, keepaspectratio]{Pictures/ai-ml/grid_search_cross_validation.png}
        \end{figure}
    \end{minipage}
    \hfill
    \begin{minipage}{0.58\linewidth}
        \begin{enumerate}
            \item In $K$-Fold Cross Validation, we split the dataset into $k$ number of subsets (known as folds) then we perform training on the all the subsets but leave one $(k-1)$ subset for the evaluation of the trained model. 
            
            \item  We iterate $k$ times with a different subset reserved for testing purpose each time.
        
        \end{enumerate}
    \end{minipage}
\end{table}


\section{Fairness, Accountability, and Transparency in Machine Learning} \label{Fairness, Accountability, and Transparency in Machine Learning}

\begin{enumerate}
    \item it is important to remember that when you deploy machine learning systems you are not merely optimizing a predictive model—you are typically providing a tool that will be used to (partially or fully) automate decisions. 
    
    \item These technical systems can impact the lives of individuals who are subject to the resulting decisions. 
    
    \item The leap from considering predictions to making decisions raises not only new technical questions, but also a slew of ethical questions that must be carefully considered.

    \item Overlooking foreseeable risks to the welfare of a subpopulation could cause us to administer inferior care. 
    
    \item Moreover, once we contemplate decision-making systems, we must step back and reconsider how we evaluate our technology. 
    
    \item Among other consequences of this change of scope, we will find that accuracy is seldom the right measure.\\
    When translating predictions into actions, we will often want to take into account the potential cost sensitivity of erring in various ways.

    \item If one way of misclassifying an image could be perceived as a racial sleight of hand, while misclassification to a different category would be harmless, then we might want to adjust our thresholds accordingly, accounting for societal values in designing the decision-making protocol.

    \item \textbf{Example}: If we are deploying a medical diagnostic system, we need to know for which populations it may work and for which it may not.

    \item The various mechanisms by which a model’s predictions become coupled to its training data are unaccounted for in the modeling process. This can lead to what researchers call \textbf{runaway feedback loops}\indexlabel{runaway feedback loops}.

    
\end{enumerate}




\section*{SEE}

\begin{enumerate}
    \item \fullref{Supervised Learning VS Unsupervised Learning VS Reinforcement Learning}
\end{enumerate}








