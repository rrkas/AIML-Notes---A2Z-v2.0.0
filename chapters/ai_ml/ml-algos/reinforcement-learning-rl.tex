\section{Reinforcement Learning \cite{drl-1}} \label{Reinforcement Learning}

\begin{enumerate}
    \item Reinforcement learning is learning what to do - how to map situations to actions - so as to maximize a numerical reward signal.

    \item The learner is \textbf{NOT} told which actions to take, but instead must discover which actions yield the most reward by trying them.

    \item These two characteristics - \textbf{trial-and-error search} and \textbf{delayed reward} - are the two most important distinguishing features of reinforcement learning.

    \item  Although one might be tempted to think of reinforcement learning as a kind of unsupervised learning because it does not rely on examples of correct behavior, \textbf{reinforcement learning is trying to maximize a reward signal instead of trying to find hidden structure}. 
    
    \item Uncovering structure in an agent’s experience can certainly be useful in reinforcement learning, but by itself does not address the reinforcement learning problem of maximizing a reward signal.

    \item  To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be elective in producing reward. 
    
    \item But to discover such actions, it has to try actions that it has not selected before. 
    
    \item The agent has to \textbf{exploit} what it has already experienced in order to obtain reward, but it also has to \textbf{explore} in order to make better action selections in the future. 
    
    \item The dilemma is that neither exploration nor exploitation can be pursued exclusively without failing at the task. 
    
    \item The agent must try a variety of actions and progressively favor those that appear to be best. 

    \item On a \textbf{stochastic task}, each action must be tried many times to gain a reliable estimate of its expected reward.

    \item Another key feature of reinforcement learning is that it explicitly considers the \textbf{whole problem} of a \textbf{goal-directed agent} interacting with an uncertain environment.

    \item All reinforcement learning agents have explicit \textbf{goals}, can sense aspects of their \textbf{environments}, and can choose \textbf{actions} to influence their environments

    \item it is usually assumed from the beginning that the agent has to operate despite significant uncertainty about the environment it faces.

    \item  For learning research to make progress, important subproblems have to be isolated and studied, but they should be subproblems that play clear roles in complete, interactive, goal-seeking agents, even if all the details of the complete agent cannot yet be filled in.

    \item When reinforcement learning involves \textbf{supervised learning} (SEE: \fullref{Supervised Learning}), it does so for specific reasons that determine which capabilities are critical and which are not.
\end{enumerate}

\subsection{Elements of Reinforcement Learning \cite{drl-1}}\label{Elements of Reinforcement Learning}

\subsubsection{Policy ($\pi$) \cite{drl-1}}\label{RL: policy}

\begin{enumerate}
    \item A policy defines the learning agent’s way of behaving at a given time. 

    \item a policy is a \textbf{mapping} from perceived states of the environment to actions to be taken when in those states. 

    \item In some cases the policy may be a simple function or lookup table, whereas in others it may involve extensive computation such as a search process. 

    \item The policy is the core of a reinforcement learning agent in the sense that it alone is \textbf{sufficient} to determine behavior.

    
\end{enumerate}


\subsubsection{Reward \& Reward Signal \cite{drl-1}} \label{rl: reward signal}

\begin{enumerate}
    \item A reward signal defines the goal of a reinforcement learning problem.

    \item On each time step, the environment sends to the reinforcement learning agent a single number called the \textbf{reward}.

    \item The agent’s sole objective is to \textbf{maximize} the \textbf{total reward} it receives over the \textbf{long run}. 

    \item The reward signal thus defines what are the good and bad events for the agent.

    \item They are the \textbf{immediate} and defining features of the problem faced by the agent. 
    
    \item The reward signal is the primary basis for altering the policy; if an action selected by the policy is followed by low reward, then the policy may be changed to select some other action in that situation in the future.

    \item Rewards are basically given \textbf{directly} by the environment.

    
\end{enumerate}

\subsubsection{Value function \cite{drl-1}}\label{rl: Value function}

\begin{enumerate}
    \item  a value function specifies what is good in the long run.

    \item the value of a state is the total amount of reward an agent can expect to accumulate over the \textbf{future}, starting from that state.

    \item values indicate the long-term desirability of states after taking into account the states that are \textbf{likely} to follow and the rewards available in those states.

    \item \textbf{EXAMPLE}: a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards. Or the reverse could be true.

    \item values must be estimated and re-estimated from the sequences of observations an agent makes over its \textbf{entire lifetime}

    
\end{enumerate}

\subsubsection{Model (optional) \cite{drl-1}}\label{rl: model}
\begin{enumerate}
    \item This is something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave.

    \item given a state and action, the model might predict the resultant next state and next reward. 
    
    \item Models are used for planning, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced. 
    
    \item Methods for solving reinforcement learning problems that use models and \textbf{planning} are called \textbf{model-based methods}\indexlabel{rl: model-based methods}, as opposed to simpler \textbf{model-free methods}\indexlabel{rl: model-free methods} that are explicitly trial-and-error learners - viewed as almost the opposite of planning.
\end{enumerate}



\subsection*{Note \cite{drl-1}}
\begin{enumerate}
    \item Rewards are in a sense \textbf{primary}, whereas values, as predictions of rewards, are \textbf{secondary}.

    \item Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward.

    \item it is values with which we are most concerned when making and evaluating decisions. Action choices are made based on value judgments.

    \item We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run.

    
\end{enumerate}


\subsection*{Challenges \cite{drl-1}}
\begin{enumerate}
    \item[] One of the challenges that arise in reinforcement learning, and not in other kinds of learning, is the trade-off between \textbf{exploration} and \textbf{exploitation}.

    
\end{enumerate}



\subsection{Example: Tic-Tac-Toe \cite{drl-1}}

\begin{table}[H]
    \begin{minipage}{0.79\linewidth}
        \textbf{Rules}:
        \begin{enumerate}
            \item \textbf{Two players} take turns playing on a three-by-three board. 
            
            \item One player plays \textbf{X}s and the other \textbf{O}s until one player \textbf{wins} by placing three marks in a row, \textit{horizontally}, \textit{vertically}, or \textit{diagonally}, as the \textbf{X} player has in the game shown to the right. 
            
            \item If the board fills up with neither player getting three in a row, then the game is a \textbf{draw}.
        \end{enumerate}
    \end{minipage}
    \hfill
    \begin{minipage}{0.19\linewidth}
        \begin{table}[H]
            \centering
            \begin{tabular}{c|c|c}
                 X & O & O \\
                 \hline
                 O & X & X \\
                 \hline
                 && X\\
            \end{tabular}
        \end{table}
    \end{minipage}
\end{table}

\noindent
\textbf{Possible Solutions}:
\begin{enumerate}
    \item Classical “\textbf{minimax}” solution from \textit{game theory} is not correct here because it assumes a particular way of playing by the opponent. For example, a minimax player would never reach a game state from which it could lose, even if in fact it always won from that state because of incorrect play by the opponent.

    \item Classical optimization methods for sequential decision problems, such as dynamic programming, can compute an optimal solution for any opponent, but require as input a complete specification of that opponent, including the probabilities with which the opponent makes each move in each board state.
\end{enumerate}


\noindent \textbf{Resources}:
\begin{enumerate}
    \item \url{https://playtictactoe.org/}

    \item \url{https://zackakil.github.io/deep-tic-tac-toe/}

    
\end{enumerate}




\vspace{0.2cm}
\noindent \textbf{DRL-T1} 30/548 \\
\noindent \textbf{CHECK}: \fullref{Deep Reinforcement Learning (DRL)}
