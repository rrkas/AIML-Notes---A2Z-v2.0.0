\chapter{Evaluation Metrics/ Criterion/ Loss Function/ Cost Function}

\section*{Notation}
\begin{customTableWrapper}{1.3}
\begin{table}[H]
    \begin{tabular}{ c l }
        $n$  & Number of records \\ 
        $y$ & actual output (reference) \\ 
        $\hat{y}$ & predicted output (hypothesis) \\ 
    \end{tabular}
\end{table}
\end{customTableWrapper}

SEE: \fullref{Evaluation Metrics VS Criterion}
Essentially, formulas are \textbf{same}.

\begin{table}[h]
    \begin{tabular}{l l}
        \textbf{Loss Function/ Cost Function/ Criterion} & Used in \textbf{training} phase \\
        
        \textbf{Evaluation Metrics} & Used in \textbf{testing} phase \\
    \end{tabular}
\end{table}


\begin{enumerate}
    \item \textbf{Loss functions} quantify the distance between the real and predicted values of the target. 
    
    \item The loss will usually be a non-negative number where \textbf{smaller values are better} and perfect predictions incur a loss of $0$.

\end{enumerate}

\section{Mean Absolute Difference (MAD)/ Mean Absolute Error (MAE)/ $L_1$ Loss}\label{Mean Absolute Difference (MAD)}\label{Mean Absolute Error (MAE)}\label{L1 Loss}

\[
    MAD = MAE = \displaystyle\dfrac{1}{n} \cdot \dsum_{i=1}^{n} \dabs{ y_i - \hat{y}_i }
\]

\section{Smooth $L_1$ Loss ( $\operatorname{smooth}_{L_1}$ )}
\[
    \displaystyle
    \operatorname{smooth}_{L_1}(x) = \begin{cases}
        0.5x^2 & \text{ if } \dabs{x} < 1 \\
        \dabs{x} - 0.5 & \text{ otherwise}
    \end{cases}
\]


\section{Squared Error \cite{dnn-1}} \label{Squared Error}

\[
    l^{(i)}(w,b) = \dfrac{1}{2}
    \left( \hat{y}^{(i)} - y^{(i)} \right)^2
\]

\begin{enumerate}
    \item $\dfrac{1}{2}$ makes no real difference but proves to be notationally \textbf{convenient}, since it cancels out when we take the derivative of the loss.

    \item Note that large differences between estimates $\hat{y}^{(i)}$ and targets $y^{(i)}$ lead to even larger contributions to the loss, due to the quadratic form of the loss (this can be a double-edge sword. While it encourages the model to avoid large errors it can also lead to excessive sensitivity to anomalous data).
\end{enumerate}

\section{Mean Square Error (MSE)}\label{Mean Square Error (MSE)}
\[
    MSE = \displaystyle\dfrac{1}{n} \cdot \dsum_{i=1}^{n} ( y_i - \hat{y}_i )^2
\]

\section{Root Mean Square Error (RMSE)}\label{Root Mean Square Error (RMSE)}
\[
    RMSE = \sqrt{MSE} = \displaystyle\sqrt{\dfrac{1}{n} \cdot \dsum_{i=1}^{n} ( y_i - \hat{y}_i )^2}
\]


\section{Confusion Matrix}
\subsection{Accuracy}

\subsection{Precision}
\begin{enumerate}
    \item What fraction of the returned results are relevant to the information need? \cite{ir-1}
\end{enumerate}

\subsection{Recall}
\begin{enumerate}
    \item What fraction of the relevant documents in the collection were returned by the system? \cite{ir-1}
\end{enumerate}

\subsection{F-Score}


\section{Mean Average precision (mAP)}\label{Mean Average precision (mAP)}


\section{Information Theory: Entropy \cite{dnn-1,nlp-1}} \label{Information Theory: Entropy}

\begin{enumerate}[itemsep=0.2cm]
    \item The central idea in information theory is to quantify the amount of information contained in data. 
    
    \item This places a limit on our ability to compress data.

    \item For a distribution $P$ its entropy, $H[P]$, is defined as:
    $
        \hfill
        H[P] 
        = \dsum_j - P(j) \log P(j)
        \hfill
    $

    \item One of the fundamental theorems of information theory states that in order to encode data drawn randomly from the distribution $P$, we need at least $H[P]$ “nats” to encode it.

    \item If you wonder what a “nat” is, it is the equivalent of bit but when using a code with base $e$ rather than one with base $2$. \\
    Thus, one \textbf{nat}\indexlabel{nat (information theory)} is $\dfrac{1}{\log(2)} \approx 1.44$ bit.

    \item if we cannot perfectly predict every event, then we might sometimes be \textbf{surprised}.\\
    Our \textbf{surprise is greater} when an event is assigned \textbf{lower probability}.\\
    Entropy is the \textbf{level of surprise} experienced by someone who knows the \textbf{true probability}.

    \item[] From \cite{nlp-1}:

    \item \textbf{Entropy} is a measure of information.

    \item Given a random variable $X$ ranging over whatever we are predicting (words, letters, parts of speech, the set of which we’ll call $\chi$) and with a particular probability function, call it $p(x)$, the entropy of the random variable $X$ is:
    \[
        H(\chi) = - \dsum_{x \in \chi} p(x)\log_2p(x)
    \]

    \item The log can, in principle, be computed in any base. If we use log base 2, the resulting value of entropy will be measured in \textbf{bits}.

    \item One intuitive way to think about entropy is as a lower bound on the number of bits it would take to encode a certain decision or piece of information in the optimal coding scheme.

\end{enumerate}





\section{Log-Likelihood Loss/ cross-entropy loss \cite{dnn-1}} \label{Log-Likelihood Loss/ cross-entropy loss}

\begin{enumerate}[itemsep=0.2cm]
    \item The softmax function gives us a vector $\hat{\mathbf{y}}$, which we can interpret as the (estimated) conditional probabilities of each class, given any input $\mathbf{x}$, such as $\hat{y}_1 = P(y=\textrm{class$_1$} \mid \mathbf{x})$.\\
    In the following we assume that for a dataset with features $\mathbf{X}$ the labels $\mathbf{Y}$ are represented using a one-hot encoding label vector.\\
    We can compare the estimates with reality by checking how probable the actual classes are according to our model, given the features:
    \[
        \hfill
        P(\mathbf{Y} \mid \mathbf{X}) 
        = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
        \hfill
    \]

    \item we take the negative logarithm to obtain the equivalent problem of minimizing the negative log-likelihood:
    \[
        \hfill
        -\log P(\mathbf{Y} \mid \mathbf{X}) 
        = \dsum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
        = \dsum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)})
        \hfill
    \]

    \item where for any pair of label $\mathbf{y}$ and model prediction $\hat{\mathbf{y}}$ over $q$ classes, the loss function $l$ is:
    \[
        \hfill
        l(\mathbf{y}, \hat{\mathbf{y}}) = - \dsum_{j=1}^q y_j \log \hat{y}_j
        \hfill
    \]

    \item Since $\mathbf{y}$ is a one-hot vector of length $q$, the sum over all its coordinates $j$ vanishes for all but one term.\\
    Note that the loss $l(\mathbf{y}, \hat{\mathbf{y}})$ is bounded from below by $0$ whenever $\hat{\mathbf{y}}$ is a probability vector: 
    \begin{enumerate}
        \item no single entry is larger than $1$, hence their negative logarithm cannot be lower than $0$

        \item $l(\mathbf{y}, \hat{\mathbf{y}}) = 0$ only if we predict the actual label with \textbf{certainty}

    \end{enumerate}

    \item $
        l(\mathbf{y}, \hat{\mathbf{y}}) 
        = - \dsum_{j=1}^q y_j \log \dParenBrac{\frac{\exp(o_j)}{\dsum_{k=1}^q \exp(o_k)}}
        = \dsum_{j=1}^q y_j \log \dParenBrac{\dsum_{k=1}^q \exp(o_k)} - \dsum_{j=1}^q y_j o_j
        = \log \dParenBrac{\dsum_{k=1}^q \exp(o_k)} - \dsum_{j=1}^q y_j o_j
    $

    \item $
        \partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) 
        = \frac{\exp(o_j)}{\dsum_{k=1}^q \exp(o_k)} - y_j 
        = \mathrm{softmax}(\mathbf{o})_j - y_j
    $

    \item cross-entropy from $P$ to $Q$, denoted $H(P, Q)$, is the \textbf{expected surprisal} of an observer with subjective probabilities $Q$ upon seeing data that was actually generated according to probabilities $P$.\\
    This is given by $H(P, Q) \stackrel{\textrm{def}}{=} \sum_j - P(j) \log Q(j)$\\
    The lowest possible cross-entropy is achieved when $P=Q$ (cross-entropy = $H(P,P)=H(P)$)


    \item we can think of the cross-entropy classification objective in two ways: 
    \begin{enumerate}
        \item as \textbf{maximizing} the \textbf{likelihood} of the observed data
        
        \item as \textbf{minimizing} our \textbf{surprisal} (and thus the number of bits) required to communicate the labels
    \end{enumerate}

    \item[] from \cite{nlp-1}:

    \item The cross-entropy is useful when we \textbf{don’t know} the actual probability distribution p that generated some data. It allows us to use some $m$, which is a model of $p$ (i.e., an approximation to $p$). 
    
    \item The cross-entropy of $m$ on $p$ is defined by:
    \[
        H(p,m) = \lim_{n \rightarrow \infty} - \dfrac{1}{n}\sum_{W\in L} p(w_{1:n})\log(m(w_{1:n}))
    \]

    \item Following the \textbf{Shannon-McMillan-Breiman theorem}, for a stationary ergodic process:
    \[
        H(p,m) = \lim_{n \rightarrow \infty} - \dfrac{1}{n}\log(m(w_{1:n}))
    \]
    
    \item This means that, as for entropy, we can estimate the cross-entropy of a model $m$ on some distribution $p$ by taking a single sequence that is long enough instead of summing over all possible sequences.

    \item What makes the cross-entropy useful is that the cross-entropy $H(p,m)$ is an upper bound on the entropy $H(p)$. For any model $m$, \(H(p) \leq H(p,m)\).

    \item This means that we can use some simplified model $m$ to help estimate the true entropy of a sequence of symbols drawn according to probability $p$.\\
    The more accurate $m$ is, the closer the cross-entropy $H(p,m)$ will be to the true entropy $H(p)$.\\
    Thus, the difference between $H(p,m)$ and $H(p)$ is a measure of how accurate a model is.\\
    Between two models $m_1$ and $m_2$, the more accurate model will be the one with the lower cross-entropy.\\
    (The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy.)

    \item Cross-entropy is defined in the limit as the length of the observed word sequence goes to infinity.\\
    We will need an approximation to cross-entropy, relying on a (sufficiently long) sequence of fixed length.\\
    This approximation to the cross-entropy of a model $M = P(w_i|w_{i-N+1:i-1})$ on a sequence of words $W$ is:
    \[
        H(W) = -\dfrac{1}{N}\log(P(w_1w_2...w_N))
    \]

    \item cross-entropy loss averaged over all the $n$ tokens of a sequence: $
        \dfrac{1}{n} \dsum_{t=1}^n -\log P(x_t \mid x_{t-1}, \cdots, x_1)
    $ where $P$ is given by a language model and $x_t$ is the actual token observed at time step $t$ from the sequence. \hfill \cite{dnn-1}

    \item This makes the performance on documents of different lengths comparable

\end{enumerate}




\section{Measures of Risk \cite{ism-1}} \label{Measures of Risk}

\begin{customTableWrapper}{1.5}
\begin{table}[H]
    \centering
    \begin{tabular}{l p{7cm}}
        $D$ & Outcome/ result (event of interest) \\
        
        $E$ & Exposure/ risk factor/ explanatory event (event that may affect the outcome)\\
    \end{tabular}
\end{table}
\end{customTableWrapper}

\begin{enumerate}[itemsep=0.2cm]
    \item bivariate binary data $(X_i, Y_i) \in \dCurlyBrac{0, 1} \times \dCurlyBrac{0, 1}$

    \item summary statistics:
    $
        N_{xy}
        = \dsum_{i=1}^n 1_{\dCurlyBrac{x}}(X_i)1_{\dCurlyBrac{y}}(Y_i)
        \hfill
        (x, y) \in \dCurlyBrac{0, 1}\times\dCurlyBrac{0, 1}
    $

    \item with binary data it is more \textbf{convenient} to use values $0$ and $1$ for $X$ and $Y$, instead of using the values $1$ and $2$.

    \item $N_{x\cdot} = N_{x1} + N_{x0}$ 
\end{enumerate}


\subsection{Risk Difference/ excess risk ( $ER$ ) \cite{ism-1}} \label{Risk Difference/ excess risk}

\begin{enumerate}[itemsep=0.2cm]
    \item The risk difference or excess risk is an absolute measure of risk, since it is nothing more than the difference in the conditional probabilities
    $
        \hfill
        ER = Pr(D|E) - Pr(D|E^c)
        \quad \in [-1, 1]
        \hfill
    $

    \item based on an \textbf{additive model}, i.e., $Pr (D|E) = ER + Pr (D|E^c)$

    \item Assume $0 < P(E) < 1$:
    \begin{align*}
        ER  
        &\Leftrightarrow Pr (D|E) = Pr (D|E^c) \\
        &\Leftrightarrow Pr (E^c) Pr (D \cap E) = Pr (E) Pr (D \cap E^c) \\
        &\Leftrightarrow [1 - Pr (E)] Pr (D \cap E) = Pr (E)[Pr(D) - Pr (D \cap E)] \\
        &\Leftrightarrow Pr (D \cap E) = Pr (D) Pr (E)
    \end{align*}

    \item it can be viewed as the excess number of cases ($D$) as a fraction of the population size

    \item If the complete population (of size $N$) were to be \textbf{exposed}, the number of cases would be equal to $N \cdot Pr (D) = N \cdot Pr (D|E)$.

    \item If the complete population were \textbf{unexposed} the number of cases would be equal to $N \cdot Pr (D) = N \cdot Pr (D|E^c)$.

    \item difference in these numbers of cases indicates how the number of cases were to change if a completely exposed population would change to a completely unexposed population.
\end{enumerate}

\begin{customTableWrapper}{1.5}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|p{9cm}|}
        \hline

        $ER > 0$ & there is a greater risk of the outcome when exposed ($E$) than when unexposed ($E^c$) \\
        \hline

        $ER < 0$ & exposure ($E$) is \textbf{protective} for the outcome \\
        \hline

        $ER = 0$ & outcome ($D$) is \textbf{independent} of the exposure ($E$) \\
        \hline

    \end{tabular}
\end{table}
\end{customTableWrapper}

\subsubsection{Estimator of Risk Difference ( $\hat{RD}$ ) \cite{ism-1}}\label{Estimator of Risk Difference}

\[
    \hat{RD} 
    = \dfrac{N_{11}}{N_{11} + N_{10}} - \dfrac{N_{01}}{N_{01} + N_{00}}
    = \dfrac{N_{11}N_{00} - N_{10}N_{01}}{(N_{11} + N_{10})(N_{01} + N_{00})}
\]


\subsection{Relative Risk ( $RR$ ) \cite{ism-1}}\label{Relative Risk}

\begin{enumerate}[itemsep=0.2cm]
    \item The relative risk would compare the two conditional probabilities $Pr (D|E)$ and $Pr (D|E^c)$ by taking the ratio 
    $   
        \hfill
        RR = \dfrac{Pr (D|E)}{Pr (D|E^c)} 
        \hfill
    $

    \item based on a \textbf{multiplicative model}, i.e., $Pr (D|E) = RR \cdot Pr (D|E^c)$

    \item it is common to use the unexposed group as reference group (denominator = outcome $D$)

\end{enumerate}


\begin{customTableWrapper}{1.5}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|p{9cm}|}
        \hline

        $RR > 1$ & exposed group has a higher probability of the outcome ($D$) than the unexposed one \\
        \hline

        $RR = 1$ & outcome and exposure are \textbf{independent} \\
        \hline

        $RR < 1$ & unexposed group has a higher probability of the outcome \\
        \hline

    \end{tabular}
\end{table}
\end{customTableWrapper}

\subsubsection{Estimator of Relative Risk ( $\hat{RR}$ ) \cite{ism-1}}\label{Estimator of Relative Risk}

\[
    \hat{RR}
    = \dfrac{N_{11}(N_{01} + N_{00})}{N_{01}(N_{11} + N_{10})}
\]



\subsection{Odds Ratio ( $OR$ ) \cite{ism-1}} \label{Odds Ratio}

\begin{enumerate}[itemsep=0.2cm]
    \item The odds ratio compares the odds for the exposed group with the odds for the unexposed group.

    \item it is common to use the unexposed group as reference group (denominator = unexposed group $O_{E^c}$)

    \item The odds is a measure of how \textbf{likely} the outcome occurs with respect to not observing this outcome.
    \[
        \hfill
        O = \dfrac{p}{1-p}
        \hfill
        O_E = \dfrac{Pr (D|E)}{1 - Pr (D|E)}
        \hfill
        O_{E^c} = \dfrac{Pr (D|E^c)}{1 - Pr (D|E^c)}
        \hfill
    \]

    \item $
        OR 
        = \dfrac{O_E}{O_{E^c}}
        = \dfrac{Pr (D|E)[1 - Pr (D|E^c)]}{Pr (D|E^c)[1 - Pr (D|E)]}
        = \dfrac{Pr (D^c|E^c)}{Pr (D^c|E)} \times RR
    $

\end{enumerate}

\begin{customTableWrapper}{1.5}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|p{11cm}|}
        \hline

        $OR > 1$ & exposed group has a higher odds than the unexposed group, which implies that the exposed group has a higher probability of outcome $D$ \\
        \hline

        $OR = 1$ & outcome is \textbf{independent} of the exposure \\
        \hline

        $OR < 1$ & unexposed group has a higher probability of the outcome \\
        \hline

    \end{tabular}
\end{table}
\end{customTableWrapper}

\subsubsection{Estimator of Odds Ratio ( $\hat{OR}$ ) \cite{ism-1}}\label{Estimator of Odds Ratio}

\[
    \hat{OR}
    = \dfrac{N_{11}N_{00}}{N_{10}N_{01}}
\]



\subsection{Confidence Interval \cite{ism-1}} \label{Measures of risk: Confidence Interval}

\begin{enumerate}[itemsep=0.2cm]
    \item the coverage of the confidence interval containing the population parameter is close to the intended confidence level $100\%(1 - \alpha)$

    \item 
    $
        \hfill
        \hat{p}_0 = \dfrac{N_{01}}{N_{0\cdot}}
        \hfill
        \hat{p}_1 = \dfrac{N_{11}}{N_{1\cdot}}
        \hfill
    $

    \[
        \hfill
        L_{x}
        = \dfrac{
            2N_{x\cdot} \hat{p}_x + z^2_{1 - \alpha/2} 
            - z_{1 - \alpha/2} \sqrt{4N_{x\cdot} \hat{p}_x (1- \hat{p}_x) + z^2_{1 - \alpha/2}}
        }{
            2(N_{x\cdot} + z^2_{1 - \alpha/2} )
        }
        \hspace{0.5cm}
        U_{x}
        = \dfrac{
            2N_{x\cdot} \hat{p}_x + z^2_{1 - \alpha/2} 
            + z_{1 - \alpha/2} \sqrt{4N_{x\cdot} \hat{p}_x (1- \hat{p}_x) + z^2_{1 - \alpha/2}}
        }{
            2(N_{x\cdot} + z^2_{1 - \alpha/2} )
        }
        \hfill
    \]

    \subsection*{CI for RD/ ER \cite{ism-1}}

    \item For $x \in \dCurlyBrac{0, 1}$ and $z_p$ the $p$th quantile of the \textbf{standard normal distribution function}. The $100\%(1 - \alpha)$ confidence interval on the risk difference is then given by
    \[
        \hfill
        \left(
            \hat{RD} - \sqrt{(\hat{p}_1 - L_1)^2 + (\hat{p}_0 - U_0)^2},
            \hspace{0.5cm}
            \hat{RD} - \sqrt{(\hat{p}_1 - U_1)^2 + (\hat{p}_0 - L_0)^2}
        \right]
        \hfill
    \]

    \subsection*{CI for RR \cite{ism-1}}

    \item For the relative risk, the confidence interval is calculated in the \textbf{logarithmic scale}. The estimated standard error of the logarithmic transformed relative risk is given by
    $
        \hat{SE}_{RR}
        = \sqrt{
            \dfrac{1 - \hat{p}_0}{N_{0\cdot}\hat{p}_0}
            + \dfrac{1 - \hat{p}_1}{N_{1\cdot}\hat{p}_1}
        }
    $

    \item Thus the asymptotic $100\%(1 - \alpha)$ confidence interval on the logarithmic transformation of the relative risk is given by
    $
        (L_{RR}, \hspace{0.2cm} U_{RR}]
        = \left( 
            \log(\hat{RR}) - z_{1 - \alpha/2} \hat{SE}_{RR},
            \hspace{0.2cm}
            \log(\hat{RR}) + z_{1 - \alpha/2} \hat{SE}_{RR}
        \right]
    $

    \item The $100\%(1 - \alpha)$ confidence interval on the relative risk is then calculated by transforming these confidence limits back to the original scale using $(exp(L _{RR}), exp(U_{RR})]$.

    \subsection*{CI for OR \cite{ism-1}}

    \item The confidence interval on the odds ratio is also calculated in the logarithmic scale using the standard error $
        \hat{SE}_{OR}
        = \sqrt{N^{-1}_{00} + N^{-1}_{01} + N^{-1}_{10} + N^{-1}_{11}}
    $

    \item The $100\%(1 - \alpha)$ confidence interval on the odds ratio in the logarithmic scale is given by
    \[
        (L _{O R}, U_{O R}]
        = \left(
            \log(\hat{OR}) - z_{1-\alpha/2} \hat{SE}_{O R}, 
            \hspace{0.2cm}
            \log(\hat{OR}) + z_{1-\alpha/2} \hat{SE}_{O R}
        \right]
    \]

    \item Thus the $100\%(1 - \alpha)$ confidence interval on the odds ratio is then calculated by transforming these confidence limits back to the original scale using $(\exp(L_{O R}), \exp (U_{O R})]$.
\end{enumerate}




\subsection*{Relation in $RR$ \& $OR$ \cite{ism-1}}

\begin{enumerate}[itemsep=0.2cm]
    \item odds ratio is always further away from $1$ than the relative risk, i.e., $1 < RR < OR$ or $OR < RR < 1$

    \item If $RR > 1$, we have that $Pr(D|E) > Pr(D|E^c)$, using its definition. \\
    Since $Pr(D^c|E) = 1 - Pr(D|E)$ and $Pr(D^c|E^c) = 1 - Pr(D|E^c)$, we obtain that $Pr(D^c|E) < Pr(D^c|E^c)$.\\
    Combining this inequality with the relation, we see that $OR > RR$.

\end{enumerate}


\subsection{Empirical Risk and Risk \cite{dnn-1}} \label{Empirical Risk and Risk}

\begin{customTableWrapper}{1.5}
\begin{table}[H]
    \centering
    \begin{tabular}{l p{7cm}}
        $\dCurlyBrac{(x_1, y_1), \cdots,(x_n, y_n)}$ & features and associated labels of training data \\

        $f$ & model \\
    \end{tabular}
\end{table}
\end{customTableWrapper}

\begin{enumerate}[itemsep=0.2cm]
    \item \textbf{Risk} is the expectation of the loss over the \textbf{entire population} of data drawn from their \textbf{true distribution} $p(x, y)$:
    \[
        E_{p(\mathbf{x}, y)} [l(f(\mathbf{x}), y)] = \iint l(f(\mathbf{x}), y) p(\mathbf{x}, y) \;d\mathbf{x}dy
    \]
    
    \item minimize the loss on the training: 
    \[
        \displaystyle\mathop{\mathrm{minimize}}_f \dfrac{1}{n} 
        \dsum_{i=1}^n l(f(\mathbf{x}_i), y_i)
    \]
    where $l$ is the \textbf{loss function} measuring “how bad” the prediction $f(x_i)$ is given the associated label $y_i$. \\
    The \textbf{empirical risk} is an average loss over the training data to \textbf{approximate the risk}.
    
\end{enumerate}
















