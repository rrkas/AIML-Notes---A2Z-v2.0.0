\chapter{Evaluation Metrics/ Criterion/ Loss Function/ Cost Function}

\section*{Notation}
\begin{alternateColorTable}
\renewcommand{\arraystretch}{1.3}
\begin{table}[H]
    \begin{tabular}{ c l }
        $n$  & Number of records \\ 
        $y$ & actual output (reference) \\ 
        $\hat{y}$ & predicted output (hypothesis) \\ 
    \end{tabular}
\end{table}
\renewcommand{\arraystretch}{1}
\end{alternateColorTable}

SEE: \fullref{Evaluation Metrics VS Criterion}
Essentially, formulas are \textbf{same}.

\begin{table}[h]
    \begin{tabular}{l l}
        \textbf{Loss Function/ Cost Function/ Criterion} & Used in \textbf{training} phase \\
        
        \textbf{Evaluation Metrics} & Used in \textbf{testing} phase \\
    \end{tabular}
\end{table}


\begin{enumerate}
    \item \textbf{Loss functions} quantify the distance between the real and predicted values of the target. 
    
    \item The loss will usually be a non-negative number where \textbf{smaller values are better} and perfect predictions incur a loss of $0$.

\end{enumerate}

\section{Mean Absolute Difference (MAD)/ Mean Absolute Error (MAE)/ $L_1$ Loss}\label{Mean Absolute Difference (MAD)}\label{Mean Absolute Error (MAE)}\label{L1 Loss}

\[
    MAD = MAE = \displaystyle\dfrac{1}{n} \cdot \sum_{i=1}^{n} \dabs{ y_i - \hat{y}_i }
\]

\section{Smooth $L_1$ Loss ( $\operatorname{smooth}_{L_1}$ )}
\[
    \displaystyle
    \operatorname{smooth}_{L_1}(x) = \begin{cases}
        0.5x^2 & \text{ if } \dabs{x} < 1 \\
        \dabs{x} - 0.5 & \text{ otherwise}
    \end{cases}
\]


\section{Squared Error \cite{dnn-1}} \label{Squared Error}

\[
    l^{(i)}(w,b) = \dfrac{1}{2}
    \left( \hat{y}^{(i)} - y^{(i)} \right)^2
\]

\begin{enumerate}
    \item $\dfrac{1}{2}$ makes no real difference but proves to be notationally \textbf{convenient}, since it cancels out when we take the derivative of the loss.

    \item Note that large differences between estimates $\hat{y}^{(i)}$ and targets $y^{(i)}$ lead to even larger contributions to the loss, due to the quadratic form of the loss (this can be a double-edge sword. While it encourages the model to avoid large errors it can also lead to excessive sensitivity to anomalous data).
\end{enumerate}

\section{Mean Square Error (MSE)}\label{Mean Square Error (MSE)}
\[
    MSE = \displaystyle\dfrac{1}{n} \cdot \sum_{i=1}^{n} ( y_i - \hat{y}_i )^2
\]

\section{Root Mean Square Error (RMSE)}\label{Root Mean Square Error (RMSE)}
\[
    RMSE = \sqrt{MSE} = \displaystyle\sqrt{\dfrac{1}{n} \cdot \sum_{i=1}^{n} ( y_i - \hat{y}_i )^2}
\]


\section{Confusion Matrix}
\subsection{Accuracy}

\subsection{Precision}
\begin{enumerate}
    \item What fraction of the returned results are relevant to the information need? \cite{ir-1}
\end{enumerate}

\subsection{Recall}
\begin{enumerate}
    \item What fraction of the relevant documents in the collection were returned by the system? \cite{ir-1}
\end{enumerate}

\subsection{F-Score}


\section{Mean Average precision (mAP)}\label{Mean Average precision (mAP)}


















































