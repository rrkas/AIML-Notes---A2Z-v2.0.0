\chapter{Matrix \cite{mfml-1}}\label{chapter: Matrix}

\section*{Intro \cite{mfml-1}}

With $m, n \in \mathbb{N}$ a real-valued $(m, n)$-matrix $\mathbf{A}$ is an $m\cdot n$-tuple of elements $a_{ij}$, $i = 1, \cdots , m$, $j = 1, \cdots , n$, which is ordered according to a rectangular scheme consisting of $m$ rows and $n$ columns:
\[
    \hfill
    \mathbf{A} = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n}\\
        a_{21} & a_{22} & \cdots & a_{2n}\\
        \vdots & \vdots & \ddots & \vdots\\
        a_{m1} & a_{m2} & \cdots & a_{mn}\\
    \end{bmatrix}
    \in \mathbb{R}^{m\times n}
    \hfill
\]

\begin{itemize}
    \item $\mathbb{R}^{m\times n}$ is the set of all real-valued $(m, n)$-matrices

    \item (\textbf{flattening}) $\mathbf{A} \in \mathbb{R}^{m\times n}$ (matrix) can be equivalently represented as $\mathbf{a} \in \mathbb{R}^{mn\times 1}$ (vector) by stacking all $n$ columns of the matrix into a long vector
\end{itemize}


\section{Matrix Addition ( $\mathbf{A + B}$ ) \cite{mfml-1}}\label{Matrix Addition}
The sum of two matrices $\mathbf{A, B} \in \mathbb{R}^{m\times n}$ is defined as the element-wise sum:
\[
    \hfill
    \mathbf{A + B} = \begin{bmatrix}
        a_{11} + b_{11} & a_{12} + b_{12} & \cdots & a_{1n} + b_{1n}\\
        a_{21} + b_{21} & a_{22} + b_{22} & \cdots & a_{2n} + b_{2n}\\
        \vdots & \vdots & \ddots & \vdots\\
        a_{m1} + b_{m1} & a_{m2} + b_{m2} & \cdots & a_{mn} + b_{mn}\\
    \end{bmatrix}
    \in \mathbb{R}^{m\times n}
    \hfill
\]


\section{Matrix Multiplication ( $AB = A@B$ ) \cite{mfml-1}}
For matrices $\mathbf{A} \in \mathbb{R}^{m\times n}$, $\mathbf{B} \in \mathbb{R}^{n\times k}$, the elements $c_{ij}$ of the product $C = AB = A@B \in \mathbb{R}^{m\times k}$ are computed as:
\[
    \displaystyle
    c_{ij} = \sum_{n}^{l=1} a_{il}b_{lj}
    \hfill
    (i=1,\cdots,m)(j=1,\cdots,k)
\]

\begin{itemize}
    \item to compute element $c_{ij}$ we multiply the elements of the $i$th row of $\mathbf{A}$ with the $j$th column of $\mathbf{B}$ and sum them up

    \item Matrices can only be multiplied if their “neighboring” dimensions match

    \item Matrix multiplication is \textbf{NOT} defined as an element-wise operation on matrix elements, i.e., $c_{ij} \neq a_{ij}b_{ij}$

    \item matrix multiplication is \textbf{NOT} commutative, i.e., $\mathbf{AB \neq BA}$
\end{itemize}


\section{Equivalence ( $\mathbf{\tilde{A} = T^{-1}AS}$ ) \cite{mfml-1}}\label{Equivalence}

Two matrices $\mathbf{A, \tilde{A}} \in \mathbb{R}^{m\times n}$ are equivalent if there exist regular matrices $\mathbf{S} \in \mathbb{R}^{n\times n}$ and $\mathbf{T} \in \mathbb{R}^{m\times m}$, such that $\mathbf{\tilde{A} = T^{-1}AS}$.

\begin{itemize}
    \item equivalent matrices are \textbf{NOT} necessarily similar.
\end{itemize}


\section{Similarity/ Similar Matrices ( $\mathbf{\tilde{A} = S^{-1}AS}$ ) \cite{mfml-1}}\label{Similarity/ Similar Matrices}

Two matrices $\mathbf{A, \tilde{A}} \in \mathbb{R}^{m\times n}$ are similar if there exists a regular matrix $\mathbf{S} \in \mathbb{R}^{n\times n}$ with $\mathbf{\tilde{A} = S^{-1}AS}$.

\begin{itemize}
    \item Similar matrices are \textbf{ALWAYS} equivalent.
\end{itemize}


\section{Hadamard Product/ element-wise product ( $A \odot B$ )}\label{matrix: Hadamard Product/ element-wise product}

For matrices $\mathbf{A, B} \in \mathbb{R}^{m\times n}$, the elements $c_{ij}$ of the product $C \in \mathbb{R}^{m\times n}$ are computed as:
\[
    \hfill
    C = A \odot B	
    \Rightarrow	c_{ij} = a_{ij}b_{ij}
    \hfill
\]


\section{Special matrices}

\subsection{Identity Matrix ( $\mathbf{I}_n \in \mathbb{R}^{n\times n}$ ) \cite{mfml-1}}\label{Identity Matrix}
In $\mathbb{R}^{n\times n}$, we define the identity matrix:
\[
    \renewcommand{\arraystretch}{0.6}
    \mathbf{I}_n = \begin{bmatrix}
        1 & 0 & \cdots & 0 & \cdots & 0 \\
        0 & 1 & \cdots & 0 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 0 & \cdots & 1 \\
    \end{bmatrix} \in \mathbb{R}^{n\times n}
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Square Matrix ( $\mathbf{A} \in \mathbb{R}^{n\times n}$ ) \cite{mfml-1}}\label{Square Matrix}
\begin{itemize}
    \item possesses the same number of columns and rows
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Diagonal Matrix ( $\mathbf{D} \in \mathbb{R}^{n\times n}$ ) \cite{mfml-1}}\label{Diagonal Matrix}

A diagonal matrix is a matrix that has value zero on all off-diagonal elements, i.e., they are of the form:
\[
    \renewcommand{\arraystretch}{0.6}
    \mathbf{D} = \begin{bmatrix}
        c_1 & 0 & \cdots & 0 & \cdots & 0 \\
        0 & c_2 & \cdots & 0 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & c_k & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 0 & \cdots & c_n \\
    \end{bmatrix} \in \mathbb{R}^{n\times n}
\]

\begin{itemize}
    \item determinant = $det(\mathbf{D}) = |\mathbf{D}|$ = product of its diagonal entries = $\displaystyle \prod_{k=1}^{n} c_k$

    \item matrix power $\mathbf{D}^k$ is given by each diagonal element raised to the power $k$
    \[
        \displaystyle
        \renewcommand{\arraystretch}{0.6}
        \mathbf{D}^{k} = \begin{bmatrix}
            c_1^k & 0 & \cdots & 0 & \cdots & 0 \\
            0 & {c_2}^k & \cdots & 0 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & {c_j}^k & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & 0 & \cdots & {c_n}^k \\
        \end{bmatrix} \in \mathbb{R}^{n\times n}
    \]

    \item inverse $\mathbf{D}^{-1}$ is the reciprocal of its diagonal elements if \textbf{ALL} of them are nonzero:
    \[
        \displaystyle
        \renewcommand{\arraystretch}{0.6}
        \mathbf{D}^{-1} = \begin{bmatrix}
            {1}/{c_1} & 0 & \cdots & 0 & \cdots & 0 \\
            0 & {1}/{c_2} & \cdots & 0 & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & {1}/{c_k} & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & 0 & \cdots & {1}/{c_n} \\
        \end{bmatrix} \in \mathbb{R}^{n\times n}
    \]  
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Diagonalization/ Diagonalizable Matrix \cite{mfml-1}} \label{Diagonalization/ Diagonalizable Matrix}

A matrix $\mathbf{A} \in \mathbb{R}^{n\times n}$ is \textbf{diagonalizable} if it is \textbf{similar} (SEE: \fullref{Similarity/ Similar Matrices}) to a \textbf{diagonal matrix}, i.e., if there exists an \textbf{invertible matrix} (SEE: \fullref{Regular/ Invertible/ Nonsingular Matrix}) $\mathbf{P} \in \mathbb{R}^{n\times n}$ such that $\mathbf{D = P^{-1}AP}$

\begin{itemize}
    \item if and only if $\lambda_1, \cdots , \lambda_n$ are the eigenvalues of $\mathbf{A}$ and $\mathbf{p}_1, \cdots , \mathbf{p}_n$ are corresponding eigenvectors of $\mathbf{A}$.
\[
    \mathbf{P := [p_1, \cdots , p_n]}
\]
\[
    \mathbf{AP = A[p_1, \cdots , p_n] = [Ap_1, \cdots , Ap_n]}
\]
\[
    \mathbf{PD} = \mathbf{[p_1, \cdots , p_n]}\begin{bmatrix}
        \lambda_1 & \cdots & 0 \\
        \vdots & \ddots & \vdots \\
        0 & \cdots & \lambda_n \\
    \end{bmatrix} = \mathbf{[\lambda_1 p_1, \cdots ,\lambda_n p_n]}
\]
    
\end{itemize}
\begin{itemize}
    \item[] $\Rightarrow \mathbf{Ap}_i = \lambda_i \mathbf{p}_i$

    \item[] $\Rightarrow$ columns of $\mathbf{P}$ must be eigenvectors of $\mathbf{A}$

    \item only non-defective matrices can be diagonalized and that the columns of $\mathbf{P}$ are the $n$ eigenvectors of $\mathbf{A}$

    \item \textbf{Spectral theorem} states that we can find an ONB of eigenvectors of $\mathbb{R}^n$. This makes $\mathbf{P}$ an orthogonal matrix so that $\mathbf{D = P^\top AP}$
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Regular/ Invertible/ Nonsingular Matrix \cite{mfml-1}} \label{Regular/ Invertible/ Nonsingular Matrix}

\begin{itemize}
    \item Inverse exists
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Singular/ Non-invertible Matrix \cite{mfml-1}}\label{Singular/ Non-invertible Matrix}

\begin{itemize}
    \item Inverse doesn’t exists
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Symmetric Matrix ( $\mathbf{A = A^\top}$ ) \cite{mfml-1}}\label{Symmetric Matrix}
A matrix $\mathbf{A} \in \mathbb{R}^{n\times n}$ is symmetric if symmetric matrix $\mathbf{A = A^\top}$.

\begin{itemize}
    \item only $(n, n)$-matrices can be symmetric
\end{itemize}

\textbf{Properties}
\begin{itemize}
    \item sum of symmetric matrices $\mathbf{A, B} \in \mathbb{R}^{n\times n}$ is \textbf{ALWAYS} symmetric

    \item product of symmetric matrices $\mathbf{A, B} \in \mathbb{R}^{n\times n}$ is \textbf{ALWAYS} defined, but generally \textbf{NOT} symmetric

    \item A symmetric matrix $\mathbf{S} \in \mathbb{R}^{n\times n}$ can \textbf{ALWAYS} be diagonalized
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Upper-Triangular Matrix \cite{mfml-1}}\label{Upper-Triangular Matrix}
We call a square matrix $\mathbf{T}$ an upper-triangular matrix if $\mathbf{T}_{ij} = 0$ for $i > j$, i.e., the matrix is zero below its diagonal.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Lower-Triangular Matrix \cite{mfml-1}}\label{Lower-Triangular Matrix}
We call a square matrix $\mathbf{T}$ a lower-triangular matrix if $\mathbf{T}_{ij} = 0$ for $i < j$, i.e., the matrix is zero above its diagonal.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Defective matrix \cite{mfml-1}}\label{Defective matrix}
A square matrix $\mathbf{A} \in \mathbb{R}^{n\times n}$ is defective if it possesses fewer than $n$ linearly independent eigenvectors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Symmetric, Positive Definite (SPD) Matrix/ positive definite Matrix \cite{mfml-1}}\label{Symmetric, Positive Definite (SPD) Matrix/ positive definite Matrix}

Consider an $n$-dimensional vector space $V$. A symmetric matrix $\mathbf{A} \in \mathbb{R}^{n\times n}$ that satisfies:
\[
    \displaystyle
    \hfill
    \mathbf{A^\top = A}
    \hfill
    \forall \mathbf{x} \in V \backslash \{ 0 \} : \mathbf{x^\top A x} > 0
    \hfill
\]

is called symmetric, positive definite, or just positive definite. 

\begin{itemize}
    \item If only $geq$ holds, then $\mathbf{A}$ is called \textbf{symmetric, positive semidefinite}\indexlabel{symmetric, positive semidefinite}.
    
\end{itemize}

\begin{theorem}
    Given a matrix $\mathbf{A} \in \mathbb{R}^{m\times n}$, we can always obtain a symmetric, positive semidefinite matrix $\mathbf{S} \in \mathbb{R}^{n\times n}$ by defining $\mathbf{S := A^\top A}$
\end{theorem}

\begin{itemize}
    \item Symmetry : $S = A^\top A = A^\top (A^\top )^\top  = (A^\top A)^\top  = S^\top$ 

    \item positive semidefiniteness : $x^\top Sx = x^\top A^\top Ax = (x^\top A^\top )(Ax) = (Ax)^\top (Ax) \geq 0$ 

    \item If $rk(A) = n$, then $S := A^\top A$ is symmetric, positive definite.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Orthogonal Matrix ( $A^{-1} = A^\top$ ) \cite{mfml-1}}\label{Orthogonal Matrix}
A square matrix $\mathbf{A} \in \mathbb{R}^{n\times n}$ is an orthogonal matrix if and only if its columns are orthonormal so that 
\[
    \hfill
    AA^\top  = I = A^\top A 	
    \Rightarrow	 A^{-1} = A^\top 
    \hfill
\]

\begin{itemize}
    \item inverse is obtained by simply transposing the matrix

    \item $||Ax||^2 = (Ax)^\top (Ax) = x^\top A^\top Ax = x^\top Ix = x^\top x = ||x||^2$
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Inverse of a matrix ( $\mathbf{A}^{-1}$ ) \cite{mfml-1}}\label{Inverse of a matrix}

Consider a square matrix $\mathbf{A} \in \mathbb{R}^{n\times n}$. Let matrix $\mathbf{B} \in \mathbb{R}^{n\times n}$ have the property that $\mathbf{AB = I_n = BA}$. $\mathbf{B}$ is called the inverse of $\mathbf{A}$ and denoted by $\mathbf{A}^{-1}$.

\begin{itemize}
    \item not every matrix $\mathbf{A}$ possesses an inverse $\mathbf{A}^{-1}$

    \item When the matrix inverse exists, it is unique.
    
    \item we can generally use the \textbf{determinant} to check whether a matrix is invertible.
\end{itemize}

\begin{theorem}
    For any square matrix $\mathbf{A} \in \mathbb{R}^{n\times n}$ it holds that $\mathbf{A}$ is invertible if and only if $det(\mathbf{A}) \neq 0$
\end{theorem}

\subsection{Calculating the Inverse \cite{mfml-1}}

\subsubsection{Using Gaussian Elimination}
To compute the inverse $A^{-1}$ of $A \in R^{n\times n}$, we need to find a matrix $X$ that satisfies $AX=I_n$. Then, $\mathbf{X = A^{-1}}$. We can write this down as a set of simultaneous linear equations $AX=I_n$, where we solve for $\mathbf{X = [x_1,\cdots,x_n]}$.

\[
    \displaystyle
    \hfill
    \mathbf{[A|I_n] \Rightarrow \cdots \Rightarrow [I_n|A^{-1}]}
    \hfill
\]

\subsubsection{Moore-Penrose pseudo-inverse ( $\mathbf{A^{-1} = (A^\top A)^{-1}A^\top }$ ) \cite{mfml-1}} \label{Moore-Penrose pseudo-inverse}

\begin{align*}
    \mathbf{AX} &= \mathbf{I}_n \\
    \mathbf{A^\top AX} &= \mathbf{A^\top} &&&& (\text{ multiplying both sides by } \mathbf{A^\top}) \\
    \mathbf{(A^\top A)^{-1}(A^\top A)X} &= (\mathbf{A^\top A})^{-1}\mathbf{A^\top} &&&& (\text{ multiplying both sides by } (\mathbf{A^\top A)^{-1}}) \\
    \mathbf{X} &= \mathbf{(A^\top A)^{-1}A^\top }
\end{align*}


\begin{itemize}
    \item Works on square and even \textbf{rectangular matrices} (non-square matrix)
\end{itemize}


\section{Transpose ( $A^\top$ ) \cite{mfml-1}}\label{matrix: Transpose}
For $\mathbf{A} \in R^{m\times n}$ the matrix $B \in R^{n\times m}$ with $b_{ij} = a_{ji}$ is called the transpose of $\mathbf{A}$
\[
    \hfill
    \mathbf{B = A}^\top
    \hfill
\]


\section{Properties of matrices}

\subsection{Associativity \cite{mfml-1}}\label{matrix: Associativity}

$\forall A \in R^{m\times n} , B \in R^{n\times p} , C \in R^{p\times q} , \lambda, \psi \in R$:

\begin{itemize}
    \item $(AB)C = A(BC)$
    
    \item $(\lambda\psi)C = \lambda(\Psi C)$

    \item  $\lambda(BC) = (\lambda B)C = B(\lambda C) = (BC)\lambda$
\end{itemize}


\subsection{Distributivity \cite{mfml-1}}\label{matrix: Distributivity}

$\forall A, B \in R^{m\times n} , C, D \in R^{n\times p} , \lambda, \psi \in R$:

\begin{itemize}
    \item $(A + B)C = AC + BC$
    \item $A(C + D) = AC + AD$
    \item $(\lambda + \psi)C = \lambda C + \psi C$
    \item $\lambda (B + C) = \lambda B + \lambda C$
\end{itemize}



\subsection{Multiplication by a Scalar \cite{mfml-1}} \label{matrix: Multiplication by a Scalar}
$\forall A \in  Rm\times n , \lambda  \in  R$

\begin{itemize}

    \item $\lambda A = K \hfill (K_{ij} = \lambda a_{ij})$

    \item $\lambda$ scales each element of $A$
\end{itemize}


\subsection{Multiplication with the identity matrix}
$\forall A \in  R^{m\times n}$:

\begin{itemize}
    \item $I_mA = AI_n = A \hfill (I_m \neq I_n \text{ for } m \neq n)$
\end{itemize}


\subsection{Inverses and transposes}
\begin{enumerate}
    \item $AA^{-1} = I = A^{-1}A$

    \item $(AB)^\top  = B^\top A^\top $

    \item $(AB)^{-1} = B^{-1}A^{-1}$

    \item $(A + B)^\top  = A^\top  + B^\top $

    \item $(A + B)^{-1} \neq A^{-1} + B^{-1}$

    \item $(A^\top )^\top  = A$

    \item $(A^{-1})^{-1} = A$

    \item $(A^{-1})^\top  = (A^\top )^{-1} = A^{-^\top} $

    \item $(\lambda C)^\top  = C^\top \lambda ^\top  = C^\top \lambda  = \lambda C^\top  \hfill (\lambda  = \lambda ^\top , \forall\lambda  \in R)$
\end{enumerate}


\section{Rank of a matrix ( $rk(A)$ )}\label{Rank of a matrix}
The number of linearly independent columns of a matrix $A \in  R^{m\times n}$ equals the number of linearly independent rows and is called the rank of $A$ and is denoted by $rk(A)$.

\vspace{0.2cm}
\noindent\textbf{Properties}
\begin{enumerate}
    \item $rk(A) = rk(A^\top )$, i.e., the column rank equals the row rank

    \item The columns of $A \in  R^{m\times n}$ span a subspace (aka image/ range) $U \subseteq R^m$ with $dim(U) = rk(A)$\\
    Use Gaussian elimination to $A$ to find basis
    
    \item The rows of $A \in  R^{m\times n}$ span a subspace $W \subseteq R^n$ with $-dim(W) = rk(A)$\\
    Use Gaussian elimination to $A^\top$ to find basis
    
    \item For all $A \in  R^{n\times n}$ it holds that $A$ is regular (invertible) if and only if $rk(A) = n$
    
    \item For all $A \in  R^{m\times n}$ and all $b \in  R^m$ it holds that the linear equation system $Ax = b$ can be solved if and only if $rk(A) = rk(A|b)$, where $A|b$ denotes the augmented system
    
    \item For $A \in  R^{m\times n}$ the subspace (aka kernel/ null space) of solutions for $Ax = 0$ possesses dimension $n - rk(A)$
    
    \item A matrix $A \in  R^{m\times n}$ has \textbf{full rank}\indexlabel{matrix: full rank} if its rank equals the largest possible rank for a matrix of the same dimensions. $rk(A) = min(m, n)$
    
    \item A matrix is said to be \textbf{rank deficient}\indexlabel{matrix: rank deficient} if it does not have full rank
\end{enumerate}

\section{Determinant (det(A) or |A|)}\label{matrix: Determinant}
The determinant of a square matrix $A \in R^{n\times n}$ is a function that maps $A$ onto a real number.
\[
    det(A) = \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn} \\
    \end{bmatrix}
\]

\begin{table}[h]
    \begin{tabular}{l l}
        $A \in R^1$ & $det(A) = det(a_{11}) = a_{11}$ \\
        
        $A \in R^2$ & \( det(A) = \begin{vmatrix}
            a_{11} & a_{12}\\
            a_{21} & a_{22}\\
        \end{vmatrix} = a_{11}a_{22} - a_{12}a_{21} \) \\

        $A \in R^3$ & \( 
            det(A) 
            = \begin{vmatrix}
                a_{11} & a_{12} & a_{13} \\
                a_{21} & a_{22} & a_{23} \\
                a_{31} & a_{32} & a_{33} \\
                \end{vmatrix}
            = a_{11}\begin{vmatrix}
                    a_{22} & a_{23}\\
                    a_{32} & a_{33}\\
                \end{vmatrix} 
                - a_{21}\begin{vmatrix}
                    a_{12} & a_{13}\\
                    a_{32} & a_{33}\\
                \end{vmatrix} 
                + a_{31}\begin{vmatrix}
                    a_{12} & a_{13}\\
                    a_{22} & a_{23}\\
                \end{vmatrix}
            \) \\

        (\textbf{Sarrus' rule})\indexlabel{Matrix Determinant: Sarrus' rule} & \( = a_{11}a_{22}a_{33} + a_{21}a_{32}a_{13} + a_{31}a_{12}a_{23} - a_{31}a_{22}a_{13} - a_{11}a_{32}a_{23} - a_{21}a_{12}a_{33} \)

        
    \end{tabular}
\end{table}

\begin{itemize}
    \item For a triangular matrix (upper or lower)  $\mathbf{T} \in R^{n\times n}$, the determinant is the product of the diagonal elements:
    \[
        \displaystyle
        det(T) = \prod_{i=1}^{n} \mathbf{T}_{ii}
    \]
\end{itemize}

\begin{theorem}[Laplace Expansion]\label{matrix determinant: Laplace Expansion}
    Consider a matrix $A \in R^{n\times n}$. Then, for all $j = 1, \cdots , n$:
    \begin{enumerate}
        \item Expansion along column $j$
        \[
            \displaystyle
            det(A) = \sum_{k=1}^{n} (-1)^{k+j} a_{kj} det(A_{k,j})
        \]
    
        \item Expansion along row $j$
        \[
            \displaystyle
            det(A) = \sum_{k=1}^{n} (-1)^{k+j} a_{jk} det(A_{j,k})
        \]
    \end{enumerate}

\noindent $A_{k,j} \in R^{(n-1)\times(n-1)}$ is the submatrix of $A$ that we obtain when deleting row $k$ and column $j$.

\end{theorem}

\begin{theorem}
    A square matrix $A \in R^{n\times n}$ has $det(A) \neq 0$ if and only if $rk(A) = n$. $A$ is invertible if and only if it is \textbf{full rank}.
\end{theorem}

\noindent \textbf{Properties of Determinant}:
\begin{enumerate}
    \item The determinant of a matrix product is the product of the corresponding determinants, $det(AB) = det(A)det(B)$.

    \item Determinants are invariant to transposition, i.e., $det(A) = det(A^\top)$.

    \item If $A$ is regular (invertible), then $\displaystyle det(A^{-1}) = \displaystyle\frac{1}{det(A)}$.

    \item Similar matrices (SEE: \fullref{Similarity/ Similar Matrices}) possess the \textbf{same} determinant. Therefore, for a linear mapping $\Phi : V \to V$ all transformation matrices $A_\Phi$ of $\Phi$ have the same determinant. Thus, the determinant is invariant to the choice of basis of a linear mapping.

    \item Adding a multiple of a column/ row to another one does not change $det(A)$.

    \item Multiplication of a column/row with $\lambda  \in R$ scales $det(A)$ by $\lambda$ . In particular, $det(\lambda A) = \lambda ^n det(A)$.

    \item Swapping two rows/ columns changes the sign of $det(A)$.

    \item \textbf{Gaussian elimination} can be used to compute the determinant of a matrix

    \item The determinant of a matrix $A \in R^{n\times n}$ is the product of its eigenvalues:
    \[
        det(A) = \prod_{i=1}^{n} \lambda_i
    \]
    where $\lambda_i \in C$ are (possibly repeated) eigenvalues of $A$.

\end{enumerate}


\section{Trace ( $tr(A)$ )}\label{matrix: Trace}
The trace of a square matrix $A \in R^{n\times n}$ is defined as the trace is the \textbf{sum} of the diagonal elements of $A$:
\[
    tr(A) = \sum_{i=1}^{n} A_{ii}
\]

\noindent\textbf{Properties}:
\begin{enumerate}
    \item for $A, B \in  R^{n\times n}$,  $tr(A + B) = tr(A) + tr(B)$

    \item for $A \in  R^{n\times n}$ and $\alpha  \in  R$,  $tr(\alpha A) = \alpha\cdot tr(A)$

    \item $tr(I_n) = n$

    \item for $A \in  R^{n\times k}$, $B \in  R^{k\times n}$, $tr(AB) = tr(BA)$

    \item The trace is invariant under \textbf{cyclic permutations}.\\
    for $A \in  R^{a\times k}$, $K \in  R^{k\times l}$ , $L \in  R^{l\times a}$, $tr(AKL) = tr(KLA) = tr(LAK)$

    \item for $x, y \in  R^n$, $tr(xy^\top ) = tr(y^\top x) = y^\top x \in  R$

    \item for $\Phi  : V \to  V$,	$tr(\Phi ) = tr(A_\Phi )$

    \item for basis change $A_\Phi  \to  B_\Phi$, $tr(B) = tr(S^{-1}AS) = tr(ASS^{-1}) = tr(A)$

    \item The trace of a matrix $A \in  R^{n\times n}$ is the sum of its eigenvalues,
    \[
        tr(A) = \sum_{i=1}^{n} \lambda_{i}
    \]
    where $\lambda_i \in  C$ are (possibly repeated) eigenvalues of $A$

\end{enumerate}

\section{Characteristic Polynomial ( $p_A(\lambda)$ )}\label{Characteristic Polynomial}

For $\lambda \in R$ and a square matrix $A \in R^{n\times n}$ 
\begin{align*}
    p_A(\lambda) &:= det(A - \lambda I)\\
        &= c_0\lambda^0 + c_1\lambda^1  + c_2\lambda^ 2 + \cdots  + c_{n-1}\lambda^{n-1} + (-1)^n\lambda^n
\end{align*}

where, $c_0, \cdots  , c_{n-1} \in R$, is the characteristic polynomial of $A$
\[
    \hfill
    c_0 = det(A)
    \hfill
    c_{n-1} = (-1)^{n-1}tr(A)
    \hfill
\]


\section{Eigenvalues ($\lambda$) and Eigenvectors} \label{Eigenvalues and Eigenvectors}

Let $A \in  R^{n\times n}$ be a \textbf{square matrix}. Then $\lambda  \in  R$ is an eigenvalue of $A$ and $x \in  R^n\backslash \{0\}$ is the corresponding eigenvector of $A$ if $Ax = \lambda x$	(aka \textbf{eigenvalue equation}\label{eigenvalue equation})

\begin{enumerate}
    \item $\lambda$  is an eigenvalue of $A \in  R^{n\times n}$

    \item There exists an $x \in  Rn\backslash \{0\}$ with $Ax = \lambda x$, or, $(A - \lambda In)x = 0$ can be solved non-trivially, i.e., $x \neq 0$

    \item \(
        \hfill
        rk(A - \lambda In) < n
        \hfill
        det(A - \lambda In) = 0
        \hfill
    \)
\end{enumerate}

\noindent\textbf{Note:}
\begin{itemize}
    \item Eigen is a German word meaning “characteristic”, “self”, or “own”

    \item it is often a convention that eigenvalues are sorted in \textbf{descending order}

    \item If x is an eigenvector of $A$ associated with eigenvalue $\lambda$, then for any $c \in  R\backslash \{0\}$ it holds that $cx$ is an eigenvector of $A$ with the same eigenvalue since 
    \[
        A(cx) = cAx = c\lambda x = \lambda (cx)
    \]

    \item all vectors that are collinear to $x$ are also eigenvectors of $A$

    \item Geometrically, the eigenvector corresponding to a nonzero eigenvalue points in a direction that is stretched by the linear mapping. The eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction of the stretching is flipped.

    \item A matrix $A$ and its transpose $A^\top$  possess the same eigenvalues, but not necessarily the same eigenvectors.

    \item Similar matrices possess the same eigenvalues

    \item a linear mapping $\Phi$ has eigenvalues that are independent of the choice of basis of its transformation matrix. This makes eigenvalues, together with the determinant and the trace, key characteristic parameters of a linear mapping as they are all invariant under basis change.

    \item Symmetric, positive definite matrices always have positive, real eigenvalues

    \item eigenvectors of a matrix with n distinct eigenvalues form a basis of $R^n$

\end{itemize}

\begin{theorem}
    $\lambda  \in  R$ is an eigenvalue of $A \in  R^{n\times n}$ if and only if $\lambda$  is a root of the characteristic polynomial $p_A(\lambda ) of A$
\end{theorem}

\begin{theorem}[Spectral Theorem]\label{Spectral Theorem}
    If $A \in  R^{n\times n}$ is symmetric, there exists an orthonormal basis of the corresponding vector space $V$ consisting of eigenvectors of $A$, and each eigenvalue is real.
\end{theorem}

\begin{theorem}
    The eigenvectors $x_1, \cdots  , x_n$ of a matrix $A \in  R^{n\times n}$ with $n$ distinct eigenvalues $\lambda_1, \cdots  , \lambda_n$ are linearly independent.
\end{theorem}



\section{Eigenspace ($E_\lambda$)}\label{Eigenspace}
For $A \in  R^{n\times n}$, the set of all eigenvectors of $A$ associated with an eigenvalue $\lambda$  spans a subspace of $R^n$, which is called the eigenspace of $A$ with respect to $\lambda$  and is denoted by $E_\lambda$ 

\begin{itemize}
    \item If $\lambda$  is an eigenvalue of $A \in  R^{n\times n}$, then the corresponding eigenspace $E_\lambda$  is the solution space of the homogeneous system of linear equations $(A-\lambda I)x = 0$

    \item The eigenspace $E_\lambda$  is the null space of $A - \lambda I$ since\\
    $Ax = \lambda x \Leftrightarrow  Ax - \lambda x = 0 \Leftrightarrow  (A - \lambda I)x = 0 \Leftrightarrow  x \in  ker(A - \lambda I)$
\end{itemize}



\section{Eigenspectrum/ spectrum}\label{Eigenspectrum/ spectrum}
The set of all eigenvalues of $A$ is called the eigenspectrum, or just spectrum, of $A$


\section{Multiplicity}\label{Multiplicity}
Let a square matrix $A$ have an eigenvalue $\lambda_i$

\subsection{Algebraic Multiplicity}\label{Algebraic Multiplicity}
The algebraic multiplicity of $\lambda_i$ is the \textbf{number of times} the root appears in the characteristic polynomial.


\subsection{Geometric Multiplicity}\label{Geometric Multiplicity}

Then the geometric multiplicity of $\lambda_i$ is the \textbf{number of linearly independent eigenvectors} associated with $\lambda_i$

\begin{itemize}
    \item It is the dimensionality of the eigenspace spanned by the eigenvectors associated with $\lambda_i$

    \item A specific eigenvalue’s geometric multiplicity must be at least one because every eigenvalue has at least one associated eigenvector

    \item An eigenvalue’s geometric multiplicity \textbf{CANNOT} exceed its algebraic multiplicity, but it may be lower
\end{itemize}


\section{Norms of a Matrix}\label{Norms of a Matrix}

\subsection{Spectral Norm of a Matrix}\label{Spectral Norm of a Matrix}
For $x \in  R^n\backslash \{0\}$, the spectral norm of a matrix $A \in  R^{m\times n}$ is defined as:
\[
    \displaystyle
    ||A||_2 = \max_x \frac{||Ax||_2}{||x||_2}
\]


\begin{theorem}
    The spectral norm of $A$ is its largest singular value $\sigma_1$
\end{theorem}


\subsection{Frobenius norm of a Matrix ($||x||_F$) \cite{wiki/Matrix_norm}}\label{Frobenius norm of a Matrix}

defined as the square root of the sum of the squares of a matrix’s elements:
\[
    \displaystyle
    ||X||_F = \sqrt{
        \sum_{i=1}^{m} \sum_{j=1}^{n} x_{ij}
    }
\]

The Frobenius norm behaves as if it were an l2 norm of a matrix-shaped vector.


\section{Matrix Decompositions}

\subsection{Cholesky Decomposition/ Cholesky Factorization ($A = LL^\top$)}\label{Cholesky Decomposition/ Cholesky Factorization}

A symmetric, positive definite matrix $A$ can be factorized into a product $A = LL^\top$, where $L$ is a lower-triangular matrix with positive diagonal elements:
% \[
    
% \]















